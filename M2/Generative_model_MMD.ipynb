{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "assert sys.version_info >= (3, 5)\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "assert tf.__version__ >= \"2.0\"\n",
    "import numpy as np\n",
    "import time\n",
    "K = keras.backend\n",
    "import pandas as pd\n",
    "import math\n",
    "import os\n",
    "from sklearn.metrics import r2_score\n",
    "from scipy.stats import uniform,randint\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import scipy.stats\n",
    "from scipy.stats import norm\n",
    "from sklearn.metrics import silhouette_score\n",
    "from scipy.cluster.hierarchy import linkage, cophenet\n",
    "\n",
    "\n",
    "tf.random.set_seed(42)\n",
    "np.random.seed(42)\n",
    "random_state=42"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_path = 'save_path'\n",
    "os.chdir(save_path)\n",
    "\n",
    "X_train_omics_unlabelled = pd.read_csv(\"X_train_omics_unlabelled.csv\",index_col=0)\n",
    "X_train_omics_labelled = pd.read_csv(\"X_train_omics_labelled.csv\",index_col=0)\n",
    "X_test_omics= pd.read_csv(\"X_test_omics.csv\",index_col=0)\n",
    "X_valid_omics= pd.read_csv(\"X_valid_omics.csv\",index_col=0)\n",
    "features = np.load(\"feature_selection.npy\",allow_pickle=True)\n",
    "\n",
    "train_set_labelled_y= pd.read_csv(\"train_set_labelled_y.csv\",index_col=0)\n",
    "test_set_labelled_y= pd.read_csv(\"test_set_labelled_y.csv\",index_col=0)\n",
    "valid_set_labelled_y= pd.read_csv(\"valid_set_labelled_y.csv\",index_col=0)\n",
    "\n",
    "X_train_omics_unlabelled = X_train_omics_unlabelled[features]\n",
    "X_train_omics_labelled = X_train_omics_labelled[features]\n",
    "X_test_omics = X_test_omics[features]\n",
    "X_valid_omics = X_valid_omics[features]\n",
    "\n",
    "train_set_labelled_c= pd.read_csv(\"train_set_labelled_c.csv\",index_col=0)\n",
    "train_set_unlabelled_c= pd.read_csv(\"train_set_unlabelled_c.csv\",index_col=0)\n",
    "test_set_labelled_c= pd.read_csv(\"test_set_labelled_c.csv\",index_col=0)\n",
    "valid_set_labelled_c= pd.read_csv(\"valid_set_labelled_c.csv\",index_col=0)\n",
    "\n",
    "#bin y \n",
    "from sklearn.preprocessing import KBinsDiscretizer\n",
    "binner = KBinsDiscretizer(n_bins=10,encode=\"onehot-dense\",strategy=\"uniform\")\n",
    "train_set_labelled_y = binner.fit_transform(train_set_labelled_y)\n",
    "valid_set_labelled_y = binner.transform(valid_set_labelled_y)\n",
    "test_set_labelled_y=binner.transform(test_set_labelled_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_model_path = 'save_model_path'\n",
    "os.chdir(save_model_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_shape = X_train_omics_labelled.shape[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Custom parts # "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Helpful functions ##"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def validation_log_lik_sampling(y_val,x_val,variational_decoder,codings_size,samples=200):\n",
    "    \"\"\"\n",
    "    Samples a value of z for the expectation, and calculates something proportional to loglikelihood.\n",
    "    \n",
    "    The more samples of z, the better the MC approximation to loglik, but the longer it takes to compute.\n",
    "    \n",
    "    This is how we do our evaluation on the validation and also test set. \n",
    "    \n",
    "    We look at the ability to generate x given y i.e. loglik(x|y)\"\"\"\n",
    "    \n",
    "    x_val_len = len(x_val)\n",
    "    expectation = 0\n",
    "    for i in range(samples):\n",
    "        z = np.random.normal(loc=0,scale=1,size=codings_size*x_val_len).reshape(x_val_len,codings_size)\n",
    "        x_pred = variational_decoder([z,y_val])\n",
    "        diff = (x_val-x_pred)**2\n",
    "        pdf = K.sum(diff,axis=-1)\n",
    "        pdf = K.exp(-pdf)\n",
    "        expectation += pdf \n",
    "    expectation = expectation / samples\n",
    "    lik = tf.math.log(expectation)\n",
    "    lik = K.mean(lik)    \n",
    "    return lik\n",
    "\n",
    "\n",
    "def create_batch(x_label, y_label, x_unlabel, batch_s=32):\n",
    "    '''\n",
    "    Creates batches of labelled and unlabelled data. The total number of points in both batches is equal to batch_s.\n",
    "    Thanks to Omer Nivron for help with this.\n",
    "    \n",
    "    '''\n",
    "    proportion_labelled = x_label.shape[0]/(x_label.shape[0] + x_unlabel.shape[0])\n",
    "    \n",
    "    shape_label = x_label.shape[0]\n",
    "    label_per_batch = int(np.ceil(proportion_labelled*batch_s))\n",
    "    batch_idx_la = np.random.choice(list(range(shape_label)), label_per_batch)\n",
    "    batch_x_la = (x_label.iloc[batch_idx_la, :])\n",
    "    batch_y_la = (y_label[batch_idx_la,:])\n",
    "\n",
    "    \n",
    "    shape_unlabel = x_unlabel.shape[0]\n",
    "    unlabel_per_batch = batch_s - label_per_batch\n",
    "    batch_idx_un = np.random.choice(list(range(shape_unlabel)), unlabel_per_batch)\n",
    "    batch_x_un = (x_unlabel.iloc[batch_idx_un, :])\n",
    "    \n",
    "    del batch_idx_la,batch_idx_un\n",
    "            \n",
    "    return batch_x_la, batch_y_la, batch_x_un\n",
    "\n",
    "\n",
    "def progress_bar(iteration, total, size=30):\n",
    "    \"\"\"Progress bar for training\"\"\"\n",
    "    running = iteration < total\n",
    "    c = \">\" if running else \"=\"\n",
    "    p = (size - 1) * iteration // total\n",
    "    fmt = \"{{:-{}d}}/{{}} [{{}}]\".format(len(str(total)))\n",
    "    params = [iteration, total, \"=\" * p + c + \".\" * (size - p - 1)]\n",
    "    return fmt.format(*params)\n",
    "\n",
    "def print_status_bar(iteration, total, loss, metrics=None, size=30):\n",
    "    \"\"\"Status bar for training\"\"\"\n",
    "    metrics = \" - \".join([\"Loss for batch: {:.4f}\".format(loss)])\n",
    "    end = \"\" if iteration < total else \"\\n\"\n",
    "    print(\"\\r{} - {}\".format(progress_bar(iteration, total), metrics), end=end)\n",
    "    \n",
    "def print_status_bar_epoch(iteration, total, training_loss_for_epoch,val_loss, metrics=None, size=30):\n",
    "    \"\"\"Status bar for training (end of epoch)\"\"\"\n",
    "    metrics = \" - \".join(\n",
    "        [\"trainLoss: {:.4f}  Val_loss: {:.4f} \".format(\n",
    "            training_loss_for_epoch,val_loss)]\n",
    "    )\n",
    "    end = \"\" if iteration < total else \"\\n\"\n",
    "    print(\"\\r{} - {}\".format(progress_bar(iteration, total), metrics), end=end)\n",
    "    \n",
    "    \n",
    "def list_average(list_of_loss):\n",
    "    return sum(list_of_loss)/len(list_of_loss)\n",
    "\n",
    "\n",
    "def y_pred_loss(y_in):\n",
    "    \"\"\"Calculates loss and true y distribution given some y data.\n",
    "    \n",
    "    When the model calculates this it does it in batches (unlike this function which can take the whole data in).\n",
    "    \n",
    "    Therefore the model's learned distribution will probably not be as good as what is learnt when using the whole\n",
    "    dataset. But that is one of the things that happens if we use mini-batch gradient descent.\n",
    "    \n",
    "    \"\"\"\n",
    "    y_distribution = (K.sum(y_in,axis=0) / len(y_in))\n",
    "    loss = tf.reduce_mean(keras.losses.categorical_crossentropy(y_in,y_distribution))\n",
    "    return loss,y_distribution \n",
    "\n",
    "\n",
    "def rounded_accuracy(y_true,y_pred):\n",
    "    \"\"\"\n",
    "    Calculates accuracy of classification predictions.\n",
    "    \n",
    "    For the 10D p vector which is y_pred, it sets the highest number to 1 and the rest to 0.\n",
    "    \n",
    "    It then computes accuracy.\n",
    "    \"\"\"\n",
    "    from sklearn.metrics import accuracy_score\n",
    "    b = np.zeros_like(y_pred)\n",
    "    b[np.arange(len(y_pred)),y_pred.argmax(1)] = 1\n",
    "    return accuracy_score(y_true,b)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model ##"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Custom components ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Sampling(keras.layers.Layer):\n",
    "    \"\"\"reparameterization trick\"\"\"\n",
    "    def call(self, inputs):\n",
    "        mean, log_var = inputs\n",
    "        return K.random_normal(tf.shape(log_var)) * K.exp(log_var/2) + mean\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "class y_dist(keras.layers.Layer):\n",
    "    \"\"\"\n",
    "    Custom layer that is used to learn the parameters, p, of the distribution over y.\n",
    "    \n",
    "    Outputs a loss and p. The loss is used for training. The loss is the categorical cross entropy loss between \n",
    "    p and every y sample. The mean of this is then taken to provide a per batch loss. \n",
    "    \n",
    "    Shapes are configured for a 10D y. Change if you want to use different number of categories.\n",
    "    \n",
    "    \"\"\"\n",
    "    def __init__(self,**kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        \n",
    "    def build(self,batch_input_shape):\n",
    "        self.q = self.add_weight(name=\"q\",shape=[1,9],initializer=\"uniform\",trainable=True)\n",
    "        super().build(batch_input_shape)\n",
    "    \n",
    "    def call(self,X):\n",
    "        concatenated = tf.concat([self.q,tf.constant(np.array(0.0).reshape(1,-1),dtype=\"float32\")],axis=-1)\n",
    "        p = K.exp(concatenated)\n",
    "        p = tf.math.divide(p,K.sum(p))\n",
    "        loss = keras.losses.categorical_crossentropy(X,p)\n",
    "        loss = tf.reduce_mean(loss)\n",
    "        return loss,p \n",
    "    \n",
    "    def compute_output_shape(self,batch_input_shape):\n",
    "        return tf.TensorShape(10)\n",
    "    \n",
    "    \n",
    "    \n",
    "class FullModel_MMD(keras.models.Model):\n",
    "    \"\"\"\n",
    "    This is the full model. For MMD. This is used for training purposes.\n",
    "    \n",
    "    It requires an encoder, decoder, classifier and y_distribution model to be already defined (as can be done with \n",
    "    the build_model function).\n",
    "    \n",
    "    It returns the nloglik i.e. the loss. \n",
    "    \n",
    "    This loss can then be used in gradient descent and be minimised wrt parameters (of the four component models).\n",
    "    \n",
    "    At test time, you will call which of the component models you want to use (as opposed to trying to \"call\" this \n",
    "    FullModel which you won't want to do as its purpose is just to calculate the nloglik for training).\n",
    "    \n",
    "    \"\"\"\n",
    "    def __init__(self,N_parameter,beta,variational_encoder,variational_decoder,classifier,y_distribution,\n",
    "                 codings_size,**kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.encoder = variational_encoder\n",
    "        self.decoder = variational_decoder\n",
    "        self.classifier = classifier  \n",
    "        self.y_distribution = y_distribution\n",
    "        self.codings_size = codings_size\n",
    "        self.N = N_parameter\n",
    "        self.beta = beta\n",
    "    def call(self,inputs):\n",
    "        \"\"\"Inputs is a list, as such:\n",
    "            inputs[0] is labelled X \n",
    "            inputs[1] is labelled y \n",
    "            inputs[2] is unlabelled X\"\"\"\n",
    "        \n",
    "        X_labelled = inputs[0]\n",
    "        y_labelled = inputs[1]\n",
    "        X_unlabelled = inputs[2]\n",
    "        \n",
    "        ############### LABELLED CASE #################\n",
    "        \n",
    "        codings_mean,codings_log_var,codings = self.encoder([X_labelled,y_labelled])\n",
    "        y_pred_label = self.classifier(X_labelled)\n",
    "        reconstructions = self.decoder([codings,y_labelled])\n",
    "\n",
    "        #LOSSES#\n",
    "        \n",
    "        recon_loss = labelled_loss_reconstruction_mmd(codings=codings,x=X_labelled,x_decoded_mean=reconstructions,\n",
    "                                                      batch_size=32,codings_size=self.codings_size,beta=self.beta)\n",
    "        cls_loss = labelled_cls_loss(y_labelled,y_pred_label,self.N)\n",
    "        y_dist_loss = self.y_distribution(y_labelled)[0]\n",
    "        labelled_loss = recon_loss + cls_loss + y_dist_loss\n",
    "\n",
    "        ############### UNLABELLED CASE #################\n",
    "       \n",
    "        y_pred_unlabel = self.classifier(X_unlabelled)\n",
    "        codings_mean,codings_log_var,codings = self.encoder([X_unlabelled,y_pred_unlabel])\n",
    "        reconstructions_un = self.decoder([codings,y_pred_unlabel])\n",
    "        \n",
    "        #LOSSES#\n",
    "        \n",
    "        unlabelled_recon_loss = unlabelled_loss_reconstruction_mmd(codings=codings,y_pred=y_pred_unlabel,x=X_unlabelled,\n",
    "                                x_decoded_mean=reconstructions_un,beta=self.beta,\n",
    "                                    codings_size=self.codings_size,batch_size=32)\n",
    "\n",
    "        y_dist_loss = self.y_distribution(y_pred_unlabel)[0]\n",
    "        unlabelled_loss = unlabelled_recon_loss + y_dist_loss\n",
    "        \n",
    "        ############### ALL LOSSES #######################\n",
    "        \n",
    "        loss = labelled_loss + unlabelled_loss\n",
    "        return loss    \n",
    "\n",
    "    \n",
    "\n",
    "def build_model_mmd(n_hidden=1, n_neurons=723,input_shape=input_shape,beta=1,n_hidden_classifier=1,\n",
    "              n_neurons_classifier=300,N=30,codings_size=50):\n",
    "    \n",
    "    \"\"\"\n",
    "    Builds deep generative model.\n",
    "    \n",
    "    Parameters specify the architecture. Architecture is such that encoder and decoder have same number of nodes and hidden\n",
    "    layers. Done for simplicity. Classifier has its own architecture.\n",
    "    \n",
    "    Returns encoder,decoder,y_distribution, classifier and overall model. These can be used downstream.\n",
    "    \n",
    "    e.g. variational_encoder,variational_decoder,classifier,y_distribution,model = build_model_mmd(n_hidden=1, n_neurons=723,input_shape=input_shape,beta=1,n_hidden_classifier=1,\n",
    "              n_neurons_classifier=300,N=30,codings_size=50)\n",
    "    \"\"\"\n",
    "       \n",
    "    ########## ENCODER ###############\n",
    "    \n",
    "    x_in = keras.layers.Input(shape=[input_shape])\n",
    "    y_in = keras.layers.Input(shape=[10])\n",
    "    z = keras.layers.concatenate([x_in,y_in])\n",
    "    for layer in range(n_hidden):\n",
    "        z = keras.layers.Dense(n_neurons,activation=\"elu\",kernel_initializer=\"he_normal\")(z)\n",
    "        z = keras.layers.Dropout(0.3)(z)\n",
    "\n",
    "    codings_mean = keras.layers.Dense(codings_size)(z)\n",
    "    codings_log_var = keras.layers.Dense(codings_size)(z)\n",
    "    codings = Sampling()([codings_mean, codings_log_var])\n",
    "    variational_encoder = keras.models.Model(\n",
    "        inputs=[x_in,y_in], outputs=[codings_mean, codings_log_var, codings])\n",
    "    \n",
    "    \n",
    "    ########## DECODER ###############\n",
    "\n",
    "    latent = keras.layers.Input(shape=[codings_size])\n",
    "    l_merged = keras.layers.concatenate([latent,y_in])\n",
    "    x = l_merged\n",
    "    for layer in range(n_hidden):\n",
    "        x = keras.layers.Dense(n_neurons, activation=\"elu\",kernel_initializer=\"he_normal\")(x)\n",
    "        x = keras.layers.Dropout(0.3)(x)\n",
    "    x_out = keras.layers.Dense(input_shape,activation=\"sigmoid\")(x) \n",
    "    variational_decoder = keras.models.Model(inputs=[latent,y_in], outputs=[x_out])\n",
    "    \n",
    "    \n",
    "    ########### CLASSIFIER ############\n",
    "    \n",
    "    y_classifier = x_in\n",
    "    for layer in range(n_hidden_classifier):\n",
    "        y_classifier = keras.layers.Dense(n_neurons_classifier, activation=\"elu\",kernel_initializer=\"he_normal\")(y_classifier)\n",
    "        y_classifier = keras.layers.Dropout(rate=0.3)(y_classifier)\n",
    "    y_pred = keras.layers.Dense(10,activation=\"softmax\")(y_classifier) \n",
    "    classifier = keras.models.Model(inputs=[x_in], outputs=[y_pred])\n",
    "    \n",
    "    \n",
    "    ############ Y DISTRIBUTION #############\n",
    "    \n",
    "    loss,p = y_dist()(y_in)\n",
    "    y_distribution = keras.models.Model(inputs=[y_in],outputs=[loss,p])\n",
    "    \n",
    "    \n",
    "    ########## FULL MODEL #############\n",
    "    \n",
    "    model = FullModel_MMD(N_parameter=N,beta=beta,variational_encoder=variational_encoder,\n",
    "                  variational_decoder=variational_decoder,classifier=classifier,y_distribution=y_distribution,\n",
    "                     codings_size=codings_size)\n",
    "    \n",
    "    return variational_encoder,variational_decoder,classifier,y_distribution,model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loss functions ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def custom_mse(x,x_decoded_mean):\n",
    "    \"\"\"returns column of squared errors. Length of column is number of samples.\"\"\"\n",
    "    diff = (x-x_decoded_mean)**2\n",
    "    return K.sum(diff,axis=-1) /2 \n",
    "\n",
    "def compute_kernel(x, y):\n",
    "    x_size = tf.shape(x)[0]\n",
    "    y_size = tf.shape(y)[0]\n",
    "    dim = tf.shape(x)[1]\n",
    "    tiled_x = tf.tile(tf.reshape(x, tf.stack([x_size, 1, dim])), tf.stack([1, y_size, 1]))\n",
    "    tiled_y = tf.tile(tf.reshape(y, tf.stack([1, y_size, dim])), tf.stack([x_size, 1, 1]))\n",
    "    return tf.exp(-tf.reduce_mean(tf.square(tiled_x - tiled_y), axis=2) / tf.cast(dim, tf.float32))\n",
    "\n",
    "def compute_mmd(x, y, sigma_sqr=1.0):\n",
    "    x_kernel = compute_kernel(x, x)\n",
    "    y_kernel = compute_kernel(y, y)\n",
    "    xy_kernel = compute_kernel(x, y)\n",
    "    return tf.reduce_mean(x_kernel) + tf.reduce_mean(y_kernel) - 2 * tf.reduce_mean(xy_kernel)\n",
    "    #read this for calculations: https://scikit-learn.org/stable/modules/generated/sklearn.gaussian_process.kernels.RBF.html\n",
    "    #https://stats.stackexchange.com/questions/239008/rbf-kernel-algorithm-python\n",
    "    #https://blogs.rstudio.com/ai/posts/2018-10-22-mmd-vae/\n",
    "    #https://ermongroup.github.io/blog/a-tutorial-on-mmd-variational-autoencoders/\n",
    "\n",
    "def labelled_loss_reconstruction_mmd(codings,x,x_decoded_mean,batch_size=32,codings_size=50,beta=1):\n",
    "    recon_loss = custom_mse(x,x_decoded_mean)\n",
    "        # Compare the generated z with true samples from a standard Gaussian, and compute their MMD distance\n",
    "    true_samples = tf.random.normal(tf.stack([batch_size, codings_size]))\n",
    "    loss_mmd = compute_mmd(true_samples, codings)\n",
    "    return K.mean(recon_loss) + beta*loss_mmd\n",
    "\n",
    "def unlabelled_loss_reconstruction_mmd(codings,y_pred,x,x_decoded_mean,beta=1,codings_size=50,batch_size=32):\n",
    "    recon_loss = custom_mse(x,x_decoded_mean)\n",
    "    true_samples = tf.random.normal(tf.stack([batch_size, codings_size]))\n",
    "    loss_mmd = compute_mmd(true_samples, codings)    \n",
    "    entropy = keras.losses.categorical_crossentropy(y_pred,y_pred)\n",
    "    loss = K.mean(recon_loss) + beta*loss_mmd\n",
    "    #need to check below. We are summing over y, but we are assuming that the loss term is independent of y\n",
    "    #which is not the case. Should update this for better model https://github.com/bjlkeng/sandbox/issues/3\n",
    "    return K.mean(K.sum(y_pred*loss,axis=-1)) - K.mean(entropy) #note the sign    \n",
    "\n",
    "def dummy_loss(y,ypred):\n",
    "    \"\"\"This is a dummy loss that returns a value of zero. It is here as keras requires a loss term for each output.\n",
    "        The regression_loss_for_labelled_y gives the loss which depends on the log var and mean, so we don't need another\n",
    "        loss. But keras wants us to give separate losses for each. To keep keras happy, we use the dummy loss as a placeholder.\"\"\"\n",
    "    return 0.0\n",
    "\n",
    "def labelled_cls_loss(y, y_pred,N=383):\n",
    "    alpha = 0.1*N\n",
    "    cat_xent_loss = keras.losses.categorical_crossentropy(y, y_pred)\n",
    "    return alpha*K.mean(cat_xent_loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training functions ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "@tf.function\n",
    "def train_step(inputs):\n",
    "    \"\"\"Decorated train_step function which applies a gradient update to the parameters\"\"\"\n",
    "    with tf.GradientTape() as tape:\n",
    "        loss = model(inputs,training=True)\n",
    "        loss = tf.add_n([loss] + model.losses)\n",
    "    gradients = tape.gradient(loss, model.trainable_variables)\n",
    "    optimizer.apply_gradients(zip(gradients, model.trainable_variables))\n",
    "    return loss\n",
    "\n",
    "def fit_model(X_train_la, y_train_la, X_train_un,epochs,X_valid_la, y_valid_la,\n",
    "              patience,variational_encoder,variational_decoder,\n",
    "             classifier,y_distribution,model,Sampling=Sampling,y_dist=y_dist,batch_size=32,learning_rate=0.001,codings_size=50,\n",
    "             valid_set=True):\n",
    "\n",
    "    \"\"\"\n",
    "    Fits a single model. Gets the validation loss too if valid set exists. \n",
    "    And includes a version of early stopping, given by the patience.\n",
    "    Progress bars are shown too.\n",
    "    Number of epochs are specified by the parameter epochs.\n",
    "    \n",
    "    Need to pass in all the custom components. Maybe could put them in a dictionary for cleanliness.\n",
    "    \n",
    "    Valid set is True or False depending if you have one. If you don't, the model at the end of training is saved.\n",
    "    You must still pass in dummy valid sets even if valid_set=False.\n",
    "    \n",
    "    Returns list of training loss, and the minimum validation loss. It also saves the best encoder, decoder and\n",
    "    regressor so they can be used. \n",
    "    \n",
    "    e.g. usage fit_model(X_train_omics_labelled, train_set_labelled_y, X_train_omics_unlabelled,50,X_valid_omics, valid_set_labelled_y,\n",
    "              10,variational_encoder=variational_encoder,variational_decoder=variational_decoder,\n",
    "             classifier=classifier,y_distribution=y_distribution,model=model,\n",
    "          Sampling=Sampling,y_dist=y_dist,batch_size=32,learning_rate=0.001,codings_size=50,valid_set=True)\n",
    "    \"\"\"\n",
    "    if valid_set is True:\n",
    "    \n",
    "        start = time.time()\n",
    "        history = []\n",
    "        K.clear_session()\n",
    "\n",
    "        @tf.function\n",
    "        def train_step(inputs):\n",
    "            with tf.GradientTape() as tape:\n",
    "                loss = model(inputs,training=True)\n",
    "                loss = tf.add_n([loss] + model.losses)\n",
    "            gradients = tape.gradient(loss, model.trainable_variables)\n",
    "            optimizer.apply_gradients(zip(gradients, model.trainable_variables))\n",
    "            return loss\n",
    "\n",
    "        validation_loss = []\n",
    "        optimizer=keras.optimizers.Adam(learning_rate=learning_rate)\n",
    "        batch_loss = []\n",
    "        batches_per_epoch = int(np.floor((X_train_la.shape[0] + X_train_un.shape[0])/batch_size))\n",
    "\n",
    "        for epoch in range(epochs):\n",
    "\n",
    "                print(\"Epoch {}/{}\".format(epoch,epochs))\n",
    "\n",
    "                for i in range(batches_per_epoch):\n",
    "\n",
    "                    batch_x_la, batch_y_la, batch_x_un= create_batch(\n",
    "                        X_train_la, y_train_la, X_train_un,batch_size)\n",
    "\n",
    "                    inputs = [batch_x_la.to_numpy(),batch_y_la,batch_x_un.to_numpy()]\n",
    "                    loss = train_step(inputs)\n",
    "                    batch_loss.append(loss)\n",
    "                    average_batch_loss = list_average(batch_loss)\n",
    "                    print_status_bar(i*batch_size,X_train_la.shape[0] + X_train_un.shape[0],average_batch_loss)\n",
    "\n",
    "                training_loss_for_epoch = list_average(batch_loss)\n",
    "                batch_loss = []\n",
    "                history.append(training_loss_for_epoch)\n",
    "                val_loss = -validation_log_lik_sampling(y_valid_la,X_valid_la.to_numpy(),variational_decoder=variational_decoder,codings_size=codings_size)\n",
    "\n",
    "                validation_loss.append(val_loss)\n",
    "                print_status_bar_epoch(X_train_la.shape[0] + X_train_un.shape[0]\n",
    "                                 ,(X_train_la.shape[0] + X_train_un.shape[0]),training_loss_for_epoch,val_loss )\n",
    "\n",
    "                #callback for early stopping\n",
    "                if epoch <= patience - 1:\n",
    "\n",
    "                    if epoch == 0:\n",
    "\n",
    "                        variational_encoder.save(\"variational_encoder.h5\")\n",
    "                        variational_decoder.save(\"variational_decoder.h5\")\n",
    "                        classifier.save(\"classifier.h5\")\n",
    "                        y_distribution.save(\"y_distribution.h5\")\n",
    "\n",
    "                    else:\n",
    "                        if all(val_loss<i for i in validation_loss[:-1]) is True:\n",
    "                            variational_encoder.save(\"variational_encoder.h5\")\n",
    "                            variational_decoder.save(\"variational_decoder.h5\")\n",
    "                            classifier.save(\"classifier.h5\")\n",
    "                            y_distribution.save(\"y_distribution.h5\")\n",
    "                #this statement means at least a model is saved. Because if the best model was before epoch > patience-1,\n",
    "                #then the statement below won't save any model, which is undesirable as we need to load a model. \n",
    "\n",
    "                if epoch > patience - 1:\n",
    "\n",
    "                    latest_val_loss = validation_loss[-patience:]\n",
    "                    if all(val_loss<i for i in latest_val_loss[:-2]) is True:\n",
    "                        variational_encoder.save(\"variational_encoder.h5\")\n",
    "                        variational_decoder.save(\"variational_decoder.h5\")\n",
    "                        classifier.save(\"classifier.h5\")\n",
    "                        y_distribution.save(\"y_distribution.h5\")\n",
    "                    if all(i>latest_val_loss[0] for i in latest_val_loss[1:]) is True:\n",
    "                        break     \n",
    "\n",
    "        #load best model#\n",
    "        variational_encoder = keras.models.load_model(\"variational_encoder.h5\", custom_objects={\n",
    "           \"Sampling\": Sampling\n",
    "        })\n",
    "        variational_decoder = keras.models.load_model(\"variational_decoder.h5\")\n",
    "        classifier = keras.models.load_model(\"classifier.h5\")     \n",
    "        y_distribution = keras.models.load_model(\"y_distribution.h5\", custom_objects={\n",
    "           \"y_dist\": y_dist\n",
    "        })    \n",
    "\n",
    "        done = time.time()\n",
    "        elapsed = done-start\n",
    "        print(\"Elapsed/s: \",elapsed)\n",
    "        print(\"Final training loss: \",training_loss_for_epoch)\n",
    "        print(\"best val loss: \", min(validation_loss))\n",
    "        \n",
    "        return history, min(validation_loss)\n",
    "\n",
    "        \n",
    "    else:\n",
    "        \n",
    "        start = time.time()\n",
    "        history = []\n",
    "        K.clear_session()\n",
    "\n",
    "        @tf.function\n",
    "        def train_step(inputs):\n",
    "            with tf.GradientTape() as tape:\n",
    "                loss = model(inputs,training=True)\n",
    "                loss = tf.add_n([loss] + model.losses)\n",
    "            gradients = tape.gradient(loss, model.trainable_variables)\n",
    "            optimizer.apply_gradients(zip(gradients, model.trainable_variables))\n",
    "            return loss\n",
    "\n",
    "        optimizer=keras.optimizers.Adam(learning_rate=learning_rate)\n",
    "        batch_loss = []\n",
    "        batches_per_epoch = int(np.floor((X_train_la.shape[0] + X_train_un.shape[0])/batch_size))        \n",
    "        val_loss = 0\n",
    "\n",
    "        for epoch in range(epochs):\n",
    "\n",
    "                print(\"Epoch {}/{}\".format(epoch,epochs))\n",
    "                for i in range(batches_per_epoch):\n",
    "\n",
    "                    batch_x_la, batch_y_la, batch_x_un= create_batch(\n",
    "                        X_train_la, y_train_la, X_train_un,batch_size)\n",
    "\n",
    "                    inputs = [batch_x_la.to_numpy(),batch_y_la,batch_x_un.to_numpy()]\n",
    "                    loss = train_step(inputs)\n",
    "                    batch_loss.append(loss)\n",
    "                    average_batch_loss = list_average(batch_loss)\n",
    "                    print_status_bar(i*batch_size,X_train_la.shape[0] + X_train_un.shape[0],average_batch_loss)\n",
    "\n",
    "                training_loss_for_epoch = list_average(batch_loss)\n",
    "                batch_loss = []\n",
    "                history.append(training_loss_for_epoch)\n",
    "                print_status_bar_epoch(X_train_la.shape[0] + X_train_un.shape[0]\n",
    "                                 ,(X_train_la.shape[0] + X_train_un.shape[0]),training_loss_for_epoch,val_loss )\n",
    "        \n",
    "\n",
    "        variational_encoder.save(\"variational_encoder.h5\")\n",
    "        variational_decoder.save(\"variational_decoder.h5\")\n",
    "        classifier.save(\"classifier.h5\")\n",
    "        y_distribution.save(\"y_distribution.h5\")\n",
    "        \n",
    "        #load best model#\n",
    "        variational_encoder = keras.models.load_model(\"variational_encoder.h5\", custom_objects={\n",
    "           \"Sampling\": Sampling\n",
    "        })\n",
    "        variational_decoder = keras.models.load_model(\"variational_decoder.h5\")\n",
    "        classifier = keras.models.load_model(\"classifier.h5\")     \n",
    "        y_distribution = keras.models.load_model(\"y_distribution.h5\", custom_objects={\n",
    "           \"y_dist\": y_dist\n",
    "        })    \n",
    "\n",
    "        done = time.time()\n",
    "        elapsed = done-start\n",
    "        print(\"Elapsed/s: \",elapsed)\n",
    "        print(\"Final training loss: \",training_loss_for_epoch)\n",
    "        \n",
    "    \n",
    "        return history\n",
    "\n",
    "\n",
    "def fit_model_search(X_train_la, y_train_la, X_train_un,epochs,X_valid_la, y_valid_la,\n",
    "              patience,variational_encoder,variational_decoder,\n",
    "             classifier,y_distribution,model,Sampling=Sampling,y_dist=y_dist,batch_size=32,learning_rate=0.001,\n",
    "                    codings_size=50):\n",
    "\n",
    "    \"\"\"\n",
    "    Use for hyperparameter search. \n",
    "    \n",
    "    Fits the model. Gets the validation loss too. And includes a version of early stopping, given by the patience.\n",
    "    Progress bars are shown too.\n",
    "    Number of epochs are specified by the parameter epochs.\n",
    "    \n",
    "    Need to pass in all the custom components. Maybe could put them in a dictionary for cleanliness.\n",
    "    \n",
    "    Returns list of training loss, and the minimum validation loss. It also saves the best encoder, decoder and\n",
    "    regressor so they can be used. \n",
    "    \n",
    "    \"\"\"\n",
    "    \n",
    "    start = time.time()\n",
    "    history = []   \n",
    "       \n",
    "    @tf.function\n",
    "    def train_step(inputs):\n",
    "        with tf.GradientTape() as tape:\n",
    "            loss = model(inputs,training=True)\n",
    "            loss = tf.add_n([loss] + model.losses)\n",
    "        gradients = tape.gradient(loss, model.trainable_variables)\n",
    "        optimizer.apply_gradients(zip(gradients, model.trainable_variables))\n",
    "        return loss\n",
    "    \n",
    "    validation_loss = []\n",
    "    optimizer=keras.optimizers.Adam(learning_rate=learning_rate)\n",
    "    batch_loss = []    \n",
    "    batches_per_epoch = int(np.floor((X_train_la.shape[0] + X_train_un.shape[0])/batch_size))\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "            \n",
    "            print(\"Epoch {}/{}\".format(epoch,epochs))\n",
    "            \n",
    "            for i in range(batches_per_epoch):\n",
    "                \n",
    "                batch_x_la, batch_y_la, batch_x_un= create_batch(\n",
    "                    X_train_la, y_train_la, X_train_un,batch_size)\n",
    "\n",
    "                inputs = [batch_x_la.to_numpy(),batch_y_la,batch_x_un.to_numpy()]\n",
    "                loss = train_step(inputs)\n",
    "                batch_loss.append(loss)                \n",
    "                average_batch_loss = list_average(batch_loss)                \n",
    "                print_status_bar(i*batch_size,X_train_la.shape[0] + X_train_un.shape[0],average_batch_loss)\n",
    "            \n",
    "            training_loss_for_epoch = list_average(batch_loss)\n",
    "            batch_loss = []                \n",
    "            history.append(training_loss_for_epoch)            \n",
    "            val_loss = -validation_log_lik_sampling(y_valid_la,X_valid_la.to_numpy(),variational_decoder=variational_decoder,codings_size=codings_size)\n",
    "            \n",
    "            validation_loss.append(val_loss)            \n",
    "            print_status_bar_epoch(X_train_la.shape[0] + X_train_un.shape[0]\n",
    "                             ,(X_train_la.shape[0] + X_train_un.shape[0]),training_loss_for_epoch,val_loss )\n",
    "            \n",
    "            #callback for early stopping\n",
    "            \n",
    "            if epoch <= patience - 1:\n",
    "                \n",
    "                if epoch == 0:\n",
    "                \n",
    "                    variational_encoder.save(\"variational_encoder_intermediate.h5\")\n",
    "                    variational_decoder.save(\"variational_decoder_intermediate.h5\")\n",
    "                    classifier.save(\"classifier_intermediate.h5\")\n",
    "                    y_distribution.save(\"y_distribution_intermediate.h5\")\n",
    "                    \n",
    "                else:\n",
    "                    if all(val_loss<i for i in validation_loss[:-1]) is True:\n",
    "                        variational_encoder.save(\"variational_encoder_intermediate.h5\")\n",
    "                        variational_decoder.save(\"variational_decoder_intermediate.h5\")\n",
    "                        classifier.save(\"classifier_intermediate.h5\")\n",
    "                        y_distribution.save(\"y_distribution_intermediate.h5\")\n",
    "            #this statement means at least a model is saved. Because if the best model was before epoch > patience-1,\n",
    "            #then the statement below won't save any model, which is undesirable as we need to load a model. \n",
    "            \n",
    "            if epoch > patience - 1:\n",
    "                                \n",
    "                latest_val_loss = validation_loss[-patience:]\n",
    "                if all(val_loss<i for i in latest_val_loss[:-1]) is True:\n",
    "                    variational_encoder.save(\"variational_encoder_intermediate.h5\")\n",
    "                    variational_decoder.save(\"variational_decoder_intermediate.h5\")\n",
    "                    classifier.save(\"classifier_intermediate.h5\")\n",
    "                    y_distribution.save(\"y_distribution_intermediate.h5\")\n",
    "                if all(i>latest_val_loss[0] for i in latest_val_loss[1:]) is True:\n",
    "                    break     \n",
    "    \n",
    "    #load best model#\n",
    "    variational_encoder = keras.models.load_model(\"variational_encoder_intermediate.h5\", custom_objects={\n",
    "       \"Sampling\": Sampling\n",
    "    })\n",
    "    variational_decoder = keras.models.load_model(\"variational_decoder_intermediate.h5\")\n",
    "    classifier = keras.models.load_model(\"classifier_intermediate.h5\")     \n",
    "    y_distribution = keras.models.load_model(\"y_distribution_intermediate.h5\", custom_objects={\n",
    "       \"y_dist\": y_dist\n",
    "    })    \n",
    "                \n",
    "    done = time.time()\n",
    "    elapsed = done-start\n",
    "    print(\"Elapsed/s: \",elapsed)\n",
    "    print(\"Final training loss: \",training_loss_for_epoch)\n",
    "    print(\"best val loss: \", min(validation_loss))\n",
    "    \n",
    "    return history, min(validation_loss)\n",
    "\n",
    "def hyperparameter_search_mmd(param_distribs,epochs,patience,n_iter,X_train_la=X_train_omics_labelled, \n",
    "                          y_train_la=train_set_labelled_y, X_train_un=X_train_omics_unlabelled,\n",
    "                          X_valid_la=X_valid_omics, y_valid_la=valid_set_labelled_y):\n",
    "    \n",
    "    \"\"\"\n",
    "    Performs hyperparameter, random search. Assesses performance by determining the score on the validation set. \n",
    "    \n",
    "    Saves best models (encoder, decoder and regressor) and returns these. These can then be used downstream.\n",
    "    \n",
    "    Also returns dictionary of the search results.\n",
    "    \n",
    "    Param_distribs of the form: \n",
    "            param_distribs = {\n",
    "            \"n_hidden\": [1],\n",
    "            \"n_hidden_classifier\": [1],\n",
    "            \"beta\": [1],\n",
    "            \"n_neurons\": randint.rvs(50,1000-49,size=20,random_state=random_state).tolist(),\n",
    "           \"n_neurons_classifier\": randint.rvs(49,1000-49,size=20,random_state=random_state).tolist(),\n",
    "            \"codings_size\": randint.rvs(50,290-50,size=30,random_state=random_state).tolist(),\n",
    "            \"N\" :randint.rvs().tolist(),\n",
    "            \"learning_rate\" : ....\n",
    "            #\"codings_size\": [50]}\n",
    "            \n",
    "    There must be a value for every parameter. If you know the value you want to use, set it in the param_distribs\n",
    "    dictionary.\n",
    "    \n",
    "    Patience must be less than the number of epochs.\n",
    "    \n",
    "    e.g. result,variational_encoder,variational_decoder,classifier,y_distribution =\n",
    "            hyperparameter_search_mmd(param_distribs,500,10,n_iter=10)\n",
    "\n",
    "            \n",
    "    \"\"\"\n",
    "\n",
    "    np.random.seed(42) #needs to be here so that everything that follows is consistent\n",
    "\n",
    "    min_val_loss = []\n",
    "    master = {}\n",
    "\n",
    "    for i in range(n_iter): \n",
    "        K.clear_session()\n",
    "        master[i] = {}\n",
    "        master[i][\"parameters\"] = {}\n",
    "        \n",
    "        N= np.random.choice(param_distribs[\"N\"])\n",
    "        learning_rate= np.random.choice(param_distribs[\"learning_rate\"])\n",
    "        beta= np.random.choice(param_distribs[\"beta\"])\n",
    "        n_neurons =np.random.choice(param_distribs[\"n_neurons\"]) \n",
    "        n_neurons_classifier =np.random.choice(param_distribs[\"n_neurons_classifier\"]) \n",
    "        n_hidden  =np.random.choice(param_distribs[\"n_hidden\"]) \n",
    "        n_hidden_classifier  =np.random.choice(param_distribs[\"n_hidden_classifier\"]) \n",
    "        codings_size =np.random.choice(param_distribs[\"codings_size\"]) \n",
    "       \n",
    "        master[i][\"parameters\"][\"N\"] = N\n",
    "        master[i][\"parameters\"][\"learning_rate\"] = learning_rate\n",
    "        master[i][\"parameters\"][\"beta\"] = beta\n",
    "        master[i][\"parameters\"][\"n_neurons\"] = n_neurons\n",
    "        master[i][\"parameters\"][\"n_neurons_classifier\"] = n_neurons_classifier\n",
    "        master[i][\"parameters\"][\"n_hidden\"] = n_hidden\n",
    "        master[i][\"parameters\"][\"n_hidden_classifier\"] = n_hidden_classifier\n",
    "        master[i][\"parameters\"][\"codings_size\"] = codings_size\n",
    "\n",
    "        \n",
    "        variational_encoder,variational_decoder,classifier,y_distribution,model = build_model_mmd(n_hidden=n_hidden,       \n",
    "                                       n_neurons=n_neurons,beta=beta,n_hidden_classifier=n_hidden_classifier,\n",
    "                                        n_neurons_classifier=n_neurons_classifier,N=N,codings_size=codings_size)\n",
    "        \n",
    "                \n",
    "        history,val_loss = fit_model_search(X_train_la=X_train_la, y_train_la=y_train_la, \n",
    "                                 X_train_un=X_train_un, epochs=epochs,X_valid_la=X_valid_la, \n",
    "                                 y_valid_la=y_valid_la,patience=patience,variational_encoder=variational_encoder,\n",
    "                                variational_decoder=variational_decoder, classifier=classifier,\n",
    "                                y_distribution=y_distribution,model=model,Sampling=Sampling,y_dist=y_dist,\n",
    "                                            batch_size=32,learning_rate=learning_rate,codings_size=codings_size)        \n",
    "\n",
    "        master[i][\"val_loss\"] = val_loss\n",
    "        min_val_loss.append(val_loss)\n",
    "\n",
    "        #If val loss is lowest, save this model. \n",
    "        if val_loss <=  min(min_val_loss):\n",
    "            os.rename(\"variational_encoder_intermediate.h5\",\"variational_encoder.h5\")\n",
    "            os.rename(\"variational_decoder_intermediate.h5\",\"variational_decoder.h5\")\n",
    "            os.rename(\"classifier_intermediate.h5\",\"classifier.h5\")\n",
    "            os.rename(\"y_distribution_intermediate.h5\",\"y_distribution.h5\")\n",
    "\n",
    "        print(master)\n",
    "            \n",
    "    #load best model#\n",
    "    variational_encoder = keras.models.load_model(\"variational_encoder.h5\", custom_objects={\n",
    "       \"Sampling\": Sampling\n",
    "    })\n",
    "    variational_decoder = keras.models.load_model(\"variational_decoder.h5\")\n",
    "    classifier = keras.models.load_model(\"classifier.h5\")     \n",
    "    y_distribution = keras.models.load_model(\"y_distribution.h5\", custom_objects={\n",
    "       \"y_dist\": y_dist\n",
    "    })    \n",
    "\n",
    "    result = sorted(master.items(), key=lambda item: item[1][\"val_loss\"])\n",
    "    return result,variational_encoder,variational_decoder,classifier,y_distribution\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hyperparameter Search #"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "param_distribs = {\n",
    "            \"n_hidden\": [1,2],\n",
    "            \"n_hidden_classifier\": [1,2],\n",
    "            \"beta\": [1,10,15],\n",
    "    #\"n_neurons\": [300,500],\n",
    "   # \"n_neurons_classifier\": [50,100,150],\n",
    "            \"n_neurons\": randint.rvs(50,600-49,size=20,random_state=random_state).tolist(),\n",
    "           \"n_neurons_classifier\": randint.rvs(20,120-20,size=20,random_state=random_state).tolist(),\n",
    "            \"codings_size\": randint.rvs(20,290-20,size=30,random_state=random_state).tolist(),\n",
    "   # \"codings_size\": [20,50,70],\n",
    "            \"N\" :[0.1,1,10,50,100],\n",
    "            \"learning_rate\" : [0.001,0.0005],\n",
    "            #\"codings_size\": [120,60]\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0/100\n",
      "WARNING:tensorflow:Layer full_model_mmd is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because it's dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "2240/2278 [============================>.] - Loss for batch: 23.9656WARNING:tensorflow:Layer concatenate_1 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because it's dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "2278/2278 [==============================] - trainLoss: 23.9656  Val_loss: 23.0469 \n",
      "Epoch 1/100\n",
      "2278/2278 [==============================] - trainLoss: 18.1230  Val_loss: 21.5686 \n",
      "Epoch 2/100\n",
      "2278/2278 [==============================] - trainLoss: 16.6773  Val_loss: 20.8074 \n",
      "Epoch 3/100\n",
      "2278/2278 [==============================] - trainLoss: 16.1283  Val_loss: 20.0069 \n",
      "Epoch 4/100\n",
      "2278/2278 [==============================] - trainLoss: 15.5245  Val_loss: 19.8287 \n",
      "Epoch 5/100\n",
      "2278/2278 [==============================] - trainLoss: 14.7595  Val_loss: 19.8052 \n",
      "Epoch 6/100\n",
      "2278/2278 [==============================] - trainLoss: 14.7509  Val_loss: 19.7037 \n",
      "Epoch 7/100\n",
      "2278/2278 [==============================] - trainLoss: 14.8035  Val_loss: 19.6662 \n",
      "Epoch 8/100\n",
      "2278/2278 [==============================] - trainLoss: 14.3702  Val_loss: 19.7812 \n",
      "Epoch 9/100\n",
      "2278/2278 [==============================] - trainLoss: 14.1623  Val_loss: 19.6348 \n",
      "Epoch 10/100\n",
      "2278/2278 [==============================] - trainLoss: 14.1120  Val_loss: 19.6711 \n",
      "Epoch 11/100\n",
      "2278/2278 [==============================] - trainLoss: 13.9729  Val_loss: 19.4342 \n",
      "Epoch 12/100\n",
      "2278/2278 [==============================] - trainLoss: 13.9754  Val_loss: 19.4938 \n",
      "Epoch 13/100\n",
      "2278/2278 [==============================] - trainLoss: 13.7400  Val_loss: 19.3275 \n",
      "Epoch 14/100\n",
      "2278/2278 [==============================] - trainLoss: 13.6005  Val_loss: 19.1792 \n",
      "Epoch 15/100\n",
      "2278/2278 [==============================] - trainLoss: 13.2125  Val_loss: 19.0821 \n",
      "Epoch 16/100\n",
      "2278/2278 [==============================] - trainLoss: 13.1217  Val_loss: 19.2004 \n",
      "Epoch 17/100\n",
      "2278/2278 [==============================] - trainLoss: 13.3118  Val_loss: 19.0243 \n",
      "Epoch 18/100\n",
      "2278/2278 [==============================] - trainLoss: 13.0099  Val_loss: 18.9754 \n",
      "Epoch 19/100\n",
      "2278/2278 [==============================] - trainLoss: 12.8060  Val_loss: 18.9350 \n",
      "Epoch 20/100\n",
      "2278/2278 [==============================] - trainLoss: 12.8371  Val_loss: 18.7489 \n",
      "Epoch 21/100\n",
      "2278/2278 [==============================] - trainLoss: 12.8552  Val_loss: 18.7819 \n",
      "Epoch 22/100\n",
      "2278/2278 [==============================] - trainLoss: 12.6646  Val_loss: 18.7855 \n",
      "Epoch 23/100\n",
      "2278/2278 [==============================] - trainLoss: 12.6281  Val_loss: 18.9043 \n",
      "Epoch 24/100\n",
      "2278/2278 [==============================] - trainLoss: 12.2832  Val_loss: 19.0207 \n",
      "Epoch 25/100\n",
      "2278/2278 [==============================] - trainLoss: 12.6424  Val_loss: 19.0121 \n",
      "Epoch 26/100\n",
      "2278/2278 [==============================] - trainLoss: 12.6023  Val_loss: 18.8072 \n",
      "Epoch 27/100\n",
      "2278/2278 [==============================] - trainLoss: 12.5156  Val_loss: 18.7044 \n",
      "Epoch 28/100\n",
      "2278/2278 [==============================] - trainLoss: 12.0882  Val_loss: 18.7786 \n",
      "Epoch 29/100\n",
      "2278/2278 [==============================] - trainLoss: 12.0948  Val_loss: 18.7587 \n",
      "Epoch 30/100\n",
      "2278/2278 [==============================] - trainLoss: 12.1051  Val_loss: 18.7001 \n",
      "Epoch 31/100\n",
      "2278/2278 [==============================] - trainLoss: 11.8595  Val_loss: 18.6988 \n",
      "Epoch 32/100\n",
      "2278/2278 [==============================] - trainLoss: 11.5328  Val_loss: 18.5218 \n",
      "Epoch 33/100\n",
      "2278/2278 [==============================] - trainLoss: 11.3274  Val_loss: 18.6139 \n",
      "Epoch 34/100\n",
      "2278/2278 [==============================] - trainLoss: 12.0095  Val_loss: 18.5030 \n",
      "Epoch 35/100\n",
      "2278/2278 [==============================] - trainLoss: 11.6581  Val_loss: 18.6696 \n",
      "Epoch 36/100\n",
      "2278/2278 [==============================] - trainLoss: 11.8480  Val_loss: 18.5677 \n",
      "Epoch 37/100\n",
      "2278/2278 [==============================] - trainLoss: 11.9911  Val_loss: 18.6218 \n",
      "Epoch 38/100\n",
      "2278/2278 [==============================] - trainLoss: 11.5422  Val_loss: 18.5665 \n",
      "Epoch 39/100\n",
      "2278/2278 [==============================] - trainLoss: 11.2108  Val_loss: 18.7585 \n",
      "Epoch 40/100\n",
      "2278/2278 [==============================] - trainLoss: 11.3626  Val_loss: 18.5952 \n",
      "Epoch 41/100\n",
      "2278/2278 [==============================] - trainLoss: 11.3134  Val_loss: 18.5546 \n",
      "Epoch 42/100\n",
      "2278/2278 [==============================] - trainLoss: 11.0806  Val_loss: 18.4843 \n",
      "Epoch 43/100\n",
      "2278/2278 [==============================] - trainLoss: 11.0751  Val_loss: 18.5940 \n",
      "Epoch 44/100\n",
      "2278/2278 [==============================] - trainLoss: 10.8129  Val_loss: 18.5237 \n",
      "Epoch 45/100\n",
      "2278/2278 [==============================] - trainLoss: 11.1525  Val_loss: 18.4563 \n",
      "Epoch 46/100\n",
      "2278/2278 [==============================] - trainLoss: 11.1227  Val_loss: 18.6325 \n",
      "Epoch 47/100\n",
      "2278/2278 [==============================] - trainLoss: 11.2250  Val_loss: 18.5271 \n",
      "Epoch 48/100\n",
      "2278/2278 [==============================] - trainLoss: 10.8247  Val_loss: 18.4172 \n",
      "Epoch 49/100\n",
      "2278/2278 [==============================] - trainLoss: 10.7385  Val_loss: 18.4976 \n",
      "Epoch 50/100\n",
      "2278/2278 [==============================] - trainLoss: 10.8261  Val_loss: 18.4205 \n",
      "Epoch 51/100\n",
      "2278/2278 [==============================] - trainLoss: 10.6993  Val_loss: 18.5361 \n",
      "Epoch 52/100\n",
      "2278/2278 [==============================] - trainLoss: 10.2396  Val_loss: 18.4391 \n",
      "Epoch 53/100\n",
      "2278/2278 [==============================] - trainLoss: 10.2891  Val_loss: 18.4613 \n",
      "Epoch 54/100\n",
      "2278/2278 [==============================] - trainLoss: 10.8079  Val_loss: 18.4473 \n",
      "Epoch 55/100\n",
      "2278/2278 [==============================] - trainLoss: 10.2851  Val_loss: 18.4404 \n",
      "Epoch 56/100\n",
      "2278/2278 [==============================] - trainLoss: 10.3261  Val_loss: 18.3723 \n",
      "Epoch 57/100\n",
      "2278/2278 [==============================] - trainLoss: 10.0993  Val_loss: 18.5538 \n",
      "Epoch 58/100\n",
      "2278/2278 [==============================] - trainLoss: 10.4108  Val_loss: 18.4674 \n",
      "Epoch 59/100\n",
      "2278/2278 [==============================] - trainLoss: 10.3045  Val_loss: 18.4948 \n",
      "Epoch 60/100\n",
      "2278/2278 [==============================] - trainLoss: 9.8065  Val_loss: 18.4199 \n",
      "Epoch 61/100\n",
      "2278/2278 [==============================] - trainLoss: 9.7184  Val_loss: 18.4144 \n",
      "Epoch 62/100\n",
      "2278/2278 [==============================] - trainLoss: 9.7286  Val_loss: 18.3926 \n",
      "Epoch 63/100\n",
      "2278/2278 [==============================] - trainLoss: 10.0682  Val_loss: 18.4962 \n",
      "Epoch 64/100\n",
      "2278/2278 [==============================] - trainLoss: 9.8445  Val_loss: 18.3234 \n",
      "Epoch 65/100\n",
      "2278/2278 [==============================] - trainLoss: 9.8873  Val_loss: 18.5186 \n",
      "Epoch 66/100\n",
      "2278/2278 [==============================] - trainLoss: 9.7625  Val_loss: 18.4924 \n",
      "Epoch 67/100\n",
      "2278/2278 [==============================] - trainLoss: 9.8806  Val_loss: 18.5785 \n",
      "Epoch 68/100\n",
      "2278/2278 [==============================] - trainLoss: 9.3821  Val_loss: 18.5986 \n",
      "Epoch 69/100\n",
      "2278/2278 [==============================] - trainLoss: 9.4339  Val_loss: 18.5184 \n",
      "Epoch 70/100\n",
      "2278/2278 [==============================] - trainLoss: 9.7708  Val_loss: 18.6328 \n",
      "Epoch 71/100\n",
      "2278/2278 [==============================] - trainLoss: 9.7939  Val_loss: 18.4785 \n",
      "Epoch 72/100\n",
      "2278/2278 [==============================] - trainLoss: 9.5241  Val_loss: 18.5610 \n",
      "Epoch 73/100\n",
      "2278/2278 [==============================] - trainLoss: 9.3541  Val_loss: 18.4307 \n",
      "WARNING:tensorflow:No training configuration found in save file: the model was *not* compiled. Compile it manually.\n",
      "WARNING:tensorflow:No training configuration found in save file: the model was *not* compiled. Compile it manually.\n",
      "WARNING:tensorflow:No training configuration found in save file: the model was *not* compiled. Compile it manually.\n",
      "WARNING:tensorflow:No training configuration found in save file: the model was *not* compiled. Compile it manually.\n",
      "Elapsed/s:  80.42491030693054\n",
      "Final training loss:  tf.Tensor(9.354059, shape=(), dtype=float32)\n",
      "best val loss:  tf.Tensor(18.3234, shape=(), dtype=float32)\n",
      "{0: {'parameters': {'n_neurons_classifier': 43, 'n_hidden_classifier': 1, 'learning_rate': 0.001, 'codings_size': 208, 'beta': 15, 'N': 50.0, 'n_neurons': 516, 'n_hidden': 1}, 'val_loss': <tf.Tensor: shape=(), dtype=float32, numpy=18.3234>}}\n",
      "Epoch 0/100\n",
      "WARNING:tensorflow:Layer full_model_mmd is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because it's dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "2240/2278 [============================>.] - Loss for batch: 28.1659WARNING:tensorflow:Layer concatenate_1 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because it's dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "2278/2278 [==============================] - trainLoss: 28.1659  Val_loss: 23.2600 \n",
      "Epoch 1/100\n",
      "2278/2278 [==============================] - trainLoss: 20.2922  Val_loss: 22.7627 \n",
      "Epoch 2/100\n",
      "2278/2278 [==============================] - trainLoss: 18.7383  Val_loss: 22.3084 \n",
      "Epoch 3/100\n",
      "2278/2278 [==============================] - trainLoss: 17.7942  Val_loss: 21.7344 \n",
      "Epoch 4/100\n",
      "2278/2278 [==============================] - trainLoss: 17.0252  Val_loss: 21.3016 \n",
      "Epoch 5/100\n",
      "2278/2278 [==============================] - trainLoss: 17.0399  Val_loss: 21.1751 \n",
      "Epoch 6/100\n",
      "2278/2278 [==============================] - trainLoss: 16.5601  Val_loss: 20.9590 \n",
      "Epoch 7/100\n",
      "2278/2278 [==============================] - trainLoss: 16.2736  Val_loss: 20.4429 \n",
      "Epoch 8/100\n",
      "2278/2278 [==============================] - trainLoss: 15.6276  Val_loss: 20.3783 \n",
      "Epoch 9/100\n",
      "2278/2278 [==============================] - trainLoss: 15.6308  Val_loss: 20.1117 \n",
      "Epoch 10/100\n",
      "2278/2278 [==============================] - trainLoss: 15.3338  Val_loss: 19.8967 \n",
      "Epoch 11/100\n",
      "2278/2278 [==============================] - trainLoss: 15.2052  Val_loss: 19.4190 \n",
      "Epoch 12/100\n",
      "2278/2278 [==============================] - trainLoss: 15.2596  Val_loss: 19.6824 \n",
      "Epoch 13/100\n",
      "2278/2278 [==============================] - trainLoss: 15.0305  Val_loss: 19.4133 \n",
      "Epoch 14/100\n",
      "2278/2278 [==============================] - trainLoss: 14.5019  Val_loss: 19.5901 \n",
      "Epoch 15/100\n",
      "2278/2278 [==============================] - trainLoss: 14.6169  Val_loss: 19.2717 \n",
      "Epoch 16/100\n",
      "2278/2278 [==============================] - trainLoss: 14.6963  Val_loss: 19.0652 \n",
      "Epoch 17/100\n",
      "2278/2278 [==============================] - trainLoss: 14.6712  Val_loss: 19.0482 \n",
      "Epoch 18/100\n",
      "2278/2278 [==============================] - trainLoss: 14.2930  Val_loss: 19.0072 \n",
      "Epoch 19/100\n",
      "2278/2278 [==============================] - trainLoss: 14.0461  Val_loss: 18.9568 \n",
      "Epoch 20/100\n",
      "2278/2278 [==============================] - trainLoss: 13.8802  Val_loss: 18.8799 \n",
      "Epoch 21/100\n",
      "2278/2278 [==============================] - trainLoss: 13.8371  Val_loss: 18.6658 \n",
      "Epoch 22/100\n",
      "2278/2278 [==============================] - trainLoss: 13.8542  Val_loss: 18.7251 \n",
      "Epoch 23/100\n",
      "2278/2278 [==============================] - trainLoss: 13.5339  Val_loss: 18.5523 \n",
      "Epoch 24/100\n",
      "2278/2278 [==============================] - trainLoss: 13.6736  Val_loss: 18.3188 \n",
      "Epoch 25/100\n",
      "2278/2278 [==============================] - trainLoss: 13.6189  Val_loss: 18.5326 \n",
      "Epoch 26/100\n",
      "2278/2278 [==============================] - trainLoss: 13.5568  Val_loss: 18.3909 \n",
      "Epoch 27/100\n",
      "2278/2278 [==============================] - trainLoss: 13.4316  Val_loss: 18.1423 \n",
      "Epoch 28/100\n",
      "2278/2278 [==============================] - trainLoss: 13.2708  Val_loss: 18.3296 \n",
      "Epoch 29/100\n",
      "2278/2278 [==============================] - trainLoss: 13.3807  Val_loss: 18.1375 \n",
      "Epoch 30/100\n",
      "2278/2278 [==============================] - trainLoss: 13.2686  Val_loss: 17.9907 \n",
      "Epoch 31/100\n",
      "2278/2278 [==============================] - trainLoss: 12.7465  Val_loss: 18.0238 \n",
      "Epoch 32/100\n",
      "2278/2278 [==============================] - trainLoss: 13.1642  Val_loss: 18.0627 \n",
      "Epoch 33/100\n",
      "2278/2278 [==============================] - trainLoss: 12.9652  Val_loss: 17.8335 \n",
      "Epoch 34/100\n",
      "2278/2278 [==============================] - trainLoss: 12.9705  Val_loss: 17.8652 \n",
      "Epoch 35/100\n",
      "2278/2278 [==============================] - trainLoss: 12.7966  Val_loss: 17.8058 \n",
      "Epoch 36/100\n",
      "2278/2278 [==============================] - trainLoss: 12.7929  Val_loss: 17.7236 \n",
      "Epoch 37/100\n",
      "2278/2278 [==============================] - trainLoss: 12.9067  Val_loss: 17.8011 \n",
      "Epoch 38/100\n",
      "2278/2278 [==============================] - trainLoss: 12.8206  Val_loss: 17.6608 \n",
      "Epoch 39/100\n",
      "2278/2278 [==============================] - trainLoss: 12.5282  Val_loss: 17.7407 \n",
      "Epoch 40/100\n",
      "2278/2278 [==============================] - trainLoss: 12.3798  Val_loss: 17.7099 \n",
      "Epoch 41/100\n",
      "2278/2278 [==============================] - trainLoss: 12.7536  Val_loss: 17.6426 \n",
      "Epoch 42/100\n",
      "2278/2278 [==============================] - trainLoss: 12.5886  Val_loss: 17.5481 \n",
      "Epoch 43/100\n",
      "2278/2278 [==============================] - trainLoss: 12.3301  Val_loss: 17.5323 \n",
      "Epoch 44/100\n",
      "2278/2278 [==============================] - trainLoss: 12.1195  Val_loss: 17.5964 \n",
      "Epoch 45/100\n",
      "2278/2278 [==============================] - trainLoss: 12.1814  Val_loss: 17.4737 \n",
      "Epoch 46/100\n",
      "2278/2278 [==============================] - trainLoss: 12.1590  Val_loss: 17.6034 \n",
      "Epoch 47/100\n",
      "2278/2278 [==============================] - trainLoss: 11.9286  Val_loss: 17.6402 \n",
      "Epoch 48/100\n",
      "2278/2278 [==============================] - trainLoss: 11.9134  Val_loss: 17.5074 \n",
      "Epoch 49/100\n",
      "2278/2278 [==============================] - trainLoss: 12.0561  Val_loss: 17.5934 \n",
      "Epoch 50/100\n",
      "2278/2278 [==============================] - trainLoss: 11.9697  Val_loss: 17.3781 \n",
      "Epoch 51/100\n",
      "2278/2278 [==============================] - trainLoss: 11.3985  Val_loss: 17.3885 \n",
      "Epoch 52/100\n",
      "2278/2278 [==============================] - trainLoss: 11.8878  Val_loss: 17.5228 \n",
      "Epoch 53/100\n",
      "2278/2278 [==============================] - trainLoss: 11.7828  Val_loss: 17.2682 \n",
      "Epoch 54/100\n",
      "2278/2278 [==============================] - trainLoss: 11.3448  Val_loss: 17.2929 \n",
      "Epoch 55/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2278/2278 [==============================] - trainLoss: 11.7348  Val_loss: 17.3752 \n",
      "Epoch 56/100\n",
      "2278/2278 [==============================] - trainLoss: 11.6883  Val_loss: 17.3669 \n",
      "Epoch 57/100\n",
      "2278/2278 [==============================] - trainLoss: 11.4818  Val_loss: 17.2385 \n",
      "Epoch 58/100\n",
      "2278/2278 [==============================] - trainLoss: 11.4335  Val_loss: 17.2641 \n",
      "Epoch 59/100\n",
      "2278/2278 [==============================] - trainLoss: 11.2829  Val_loss: 17.4330 \n",
      "Epoch 60/100\n",
      "2278/2278 [==============================] - trainLoss: 11.1547  Val_loss: 17.3959 \n",
      "Epoch 61/100\n",
      "2278/2278 [==============================] - trainLoss: 11.5772  Val_loss: 17.1809 \n",
      "Epoch 62/100\n",
      "2278/2278 [==============================] - trainLoss: 10.8586  Val_loss: 17.3636 \n",
      "Epoch 63/100\n",
      "2278/2278 [==============================] - trainLoss: 10.9029  Val_loss: 17.2034 \n",
      "Epoch 64/100\n",
      "2278/2278 [==============================] - trainLoss: 10.5163  Val_loss: 17.2283 \n",
      "Epoch 65/100\n",
      "2278/2278 [==============================] - trainLoss: 10.8144  Val_loss: 17.2178 \n",
      "Epoch 66/100\n",
      "2278/2278 [==============================] - trainLoss: 10.7321  Val_loss: 17.1189 \n",
      "Epoch 67/100\n",
      "2278/2278 [==============================] - trainLoss: 11.1590  Val_loss: 17.1462 \n",
      "Epoch 68/100\n",
      "2278/2278 [==============================] - trainLoss: 11.3132  Val_loss: 17.2334 \n",
      "Epoch 69/100\n",
      "2278/2278 [==============================] - trainLoss: 10.7097  Val_loss: 17.1287 \n",
      "Epoch 70/100\n",
      "2278/2278 [==============================] - trainLoss: 10.5545  Val_loss: 17.1880 \n",
      "Epoch 71/100\n",
      "2278/2278 [==============================] - trainLoss: 10.6939  Val_loss: 17.0805 \n",
      "Epoch 72/100\n",
      "2278/2278 [==============================] - trainLoss: 10.8953  Val_loss: 17.1182 \n",
      "Epoch 73/100\n",
      "2278/2278 [==============================] - trainLoss: 10.9979  Val_loss: 17.1427 \n",
      "Epoch 74/100\n",
      "2278/2278 [==============================] - trainLoss: 10.6958  Val_loss: 17.0274 \n",
      "Epoch 75/100\n",
      "2278/2278 [==============================] - trainLoss: 10.7229  Val_loss: 17.1169 \n",
      "Epoch 76/100\n",
      "2278/2278 [==============================] - trainLoss: 10.4531  Val_loss: 17.0060 \n",
      "Epoch 77/100\n",
      "2278/2278 [==============================] - trainLoss: 10.4623  Val_loss: 16.9991 \n",
      "Epoch 78/100\n",
      "2278/2278 [==============================] - trainLoss: 10.3204  Val_loss: 16.9486 \n",
      "Epoch 79/100\n",
      "2278/2278 [==============================] - trainLoss: 10.4362  Val_loss: 16.9823 \n",
      "Epoch 80/100\n",
      "2278/2278 [==============================] - trainLoss: 10.3686  Val_loss: 16.9497 \n",
      "Epoch 81/100\n",
      "2278/2278 [==============================] - trainLoss: 10.4183  Val_loss: 16.9607 \n",
      "Epoch 82/100\n",
      "2278/2278 [==============================] - trainLoss: 10.3699  Val_loss: 16.9370 \n",
      "Epoch 83/100\n",
      "2278/2278 [==============================] - trainLoss: 10.1886  Val_loss: 16.9862 \n",
      "Epoch 84/100\n",
      "2278/2278 [==============================] - trainLoss: 10.0905  Val_loss: 17.0230 \n",
      "Epoch 85/100\n",
      "2278/2278 [==============================] - trainLoss: 10.0392  Val_loss: 16.9364 \n",
      "Epoch 86/100\n",
      "2278/2278 [==============================] - trainLoss: 10.0454  Val_loss: 16.8939 \n",
      "Epoch 87/100\n",
      "2278/2278 [==============================] - trainLoss: 9.6994  Val_loss: 16.8411 \n",
      "Epoch 88/100\n",
      "2278/2278 [==============================] - trainLoss: 10.2725  Val_loss: 16.8018 \n",
      "Epoch 89/100\n",
      "2278/2278 [==============================] - trainLoss: 9.8153  Val_loss: 16.8759 \n",
      "Epoch 90/100\n",
      "2278/2278 [==============================] - trainLoss: 9.8829  Val_loss: 16.8153 \n",
      "Epoch 91/100\n",
      "2278/2278 [==============================] - trainLoss: 9.8015  Val_loss: 16.8349 \n",
      "Epoch 92/100\n",
      "2278/2278 [==============================] - trainLoss: 9.5848  Val_loss: 16.8437 \n",
      "Epoch 93/100\n",
      "2278/2278 [==============================] - trainLoss: 10.0243  Val_loss: 16.8356 \n",
      "Epoch 94/100\n",
      "2278/2278 [==============================] - trainLoss: 9.8940  Val_loss: 16.6967 \n",
      "Epoch 95/100\n",
      "2278/2278 [==============================] - trainLoss: 9.8615  Val_loss: 16.8242 \n",
      "Epoch 96/100\n",
      "2278/2278 [==============================] - trainLoss: 9.8290  Val_loss: 16.7627 \n",
      "Epoch 97/100\n",
      "2278/2278 [==============================] - trainLoss: 9.7123  Val_loss: 16.7020 \n",
      "Epoch 98/100\n",
      "2278/2278 [==============================] - trainLoss: 9.6460  Val_loss: 16.7399 \n",
      "Epoch 99/100\n",
      "2278/2278 [==============================] - trainLoss: 9.7206  Val_loss: 16.8439 \n",
      "WARNING:tensorflow:No training configuration found in save file: the model was *not* compiled. Compile it manually.\n",
      "WARNING:tensorflow:No training configuration found in save file: the model was *not* compiled. Compile it manually.\n",
      "WARNING:tensorflow:No training configuration found in save file: the model was *not* compiled. Compile it manually.\n",
      "WARNING:tensorflow:No training configuration found in save file: the model was *not* compiled. Compile it manually.\n",
      "Elapsed/s:  105.71690797805786\n",
      "Final training loss:  tf.Tensor(9.720639, shape=(), dtype=float32)\n",
      "best val loss:  tf.Tensor(16.69674, shape=(), dtype=float32)\n",
      "{0: {'parameters': {'n_neurons_classifier': 43, 'n_hidden_classifier': 1, 'learning_rate': 0.001, 'codings_size': 208, 'beta': 15, 'N': 50.0, 'n_neurons': 516, 'n_hidden': 1}, 'val_loss': <tf.Tensor: shape=(), dtype=float32, numpy=18.3234>}, 1: {'parameters': {'n_neurons_classifier': 49, 'n_hidden_classifier': 2, 'learning_rate': 0.0005, 'codings_size': 149, 'beta': 1, 'N': 50.0, 'n_neurons': 238, 'n_hidden': 1}, 'val_loss': <tf.Tensor: shape=(), dtype=float32, numpy=16.69674>}}\n",
      "Epoch 0/100\n",
      "WARNING:tensorflow:Layer full_model_mmd is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because it's dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "2240/2278 [============================>.] - Loss for batch: 43.9853WARNING:tensorflow:Layer concatenate_1 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because it's dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "2278/2278 [==============================] - trainLoss: 43.9853  Val_loss: 20.7719 \n",
      "Epoch 1/100\n",
      "2278/2278 [==============================] - trainLoss: 32.8801  Val_loss: 19.0584 \n",
      "Epoch 2/100\n",
      "2278/2278 [==============================] - trainLoss: 30.7784  Val_loss: 18.0370 \n",
      "Epoch 3/100\n",
      "2278/2278 [==============================] - trainLoss: 29.2034  Val_loss: 17.2696 \n",
      "Epoch 4/100\n",
      "2278/2278 [==============================] - trainLoss: 29.1162  Val_loss: 16.6886 \n",
      "Epoch 5/100\n",
      "2278/2278 [==============================] - trainLoss: 28.1823  Val_loss: 16.2054 \n",
      "Epoch 6/100\n",
      "2278/2278 [==============================] - trainLoss: 27.4499  Val_loss: 15.6912 \n",
      "Epoch 7/100\n",
      "2278/2278 [==============================] - trainLoss: 26.7604  Val_loss: 15.3824 \n",
      "Epoch 8/100\n",
      "2278/2278 [==============================] - trainLoss: 26.4406  Val_loss: 15.0224 \n",
      "Epoch 9/100\n",
      "2278/2278 [==============================] - trainLoss: 25.7802  Val_loss: 14.8161 \n",
      "Epoch 10/100\n",
      "2278/2278 [==============================] - trainLoss: 25.2557  Val_loss: 14.4817 \n",
      "Epoch 11/100\n",
      "2278/2278 [==============================] - trainLoss: 25.2003  Val_loss: 14.4450 \n",
      "Epoch 12/100\n",
      "2278/2278 [==============================] - trainLoss: 24.7058  Val_loss: 14.2876 \n",
      "Epoch 13/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2278/2278 [==============================] - trainLoss: 24.4498  Val_loss: 14.1477 \n",
      "Epoch 14/100\n",
      "2278/2278 [==============================] - trainLoss: 24.0356  Val_loss: 14.1836 \n",
      "Epoch 15/100\n",
      "2278/2278 [==============================] - trainLoss: 23.9755  Val_loss: 14.0185 \n",
      "Epoch 16/100\n",
      "2278/2278 [==============================] - trainLoss: 23.7275  Val_loss: 13.9967 \n",
      "Epoch 17/100\n",
      "2278/2278 [==============================] - trainLoss: 23.6378  Val_loss: 13.9510 \n",
      "Epoch 18/100\n",
      "2278/2278 [==============================] - trainLoss: 23.9478  Val_loss: 13.8938 \n",
      "Epoch 19/100\n",
      "2278/2278 [==============================] - trainLoss: 23.0221  Val_loss: 13.7989 \n",
      "Epoch 20/100\n",
      "2278/2278 [==============================] - trainLoss: 23.1342  Val_loss: 13.7370 \n",
      "Epoch 21/100\n",
      "2278/2278 [==============================] - trainLoss: 23.2819  Val_loss: 13.8340 \n",
      "Epoch 22/100\n",
      "2278/2278 [==============================] - trainLoss: 22.4438  Val_loss: 13.7946 \n",
      "Epoch 23/100\n",
      "2278/2278 [==============================] - trainLoss: 22.4141  Val_loss: 13.7127 \n",
      "Epoch 24/100\n",
      "2278/2278 [==============================] - trainLoss: 22.0095  Val_loss: 13.7476 \n",
      "Epoch 25/100\n",
      "2278/2278 [==============================] - trainLoss: 21.7977  Val_loss: 13.8007 \n",
      "Epoch 26/100\n",
      "2278/2278 [==============================] - trainLoss: 22.2695  Val_loss: 13.7678 \n",
      "Epoch 27/100\n",
      "2278/2278 [==============================] - trainLoss: 21.9643  Val_loss: 13.7599 \n",
      "Epoch 28/100\n",
      "2278/2278 [==============================] - trainLoss: 21.7730  Val_loss: 13.8593 \n",
      "Epoch 29/100\n",
      "2278/2278 [==============================] - trainLoss: 21.1633  Val_loss: 13.8067 \n",
      "Epoch 30/100\n",
      "2278/2278 [==============================] - trainLoss: 21.3579  Val_loss: 13.7772 \n",
      "Epoch 31/100\n",
      "2278/2278 [==============================] - trainLoss: 21.5144  Val_loss: 13.6778 \n",
      "Epoch 32/100\n",
      "2278/2278 [==============================] - trainLoss: 21.1325  Val_loss: 13.7605 \n",
      "Epoch 33/100\n",
      "2278/2278 [==============================] - trainLoss: 21.2003  Val_loss: 13.8074 \n",
      "Epoch 34/100\n",
      "2278/2278 [==============================] - trainLoss: 21.3917  Val_loss: 13.7678 \n",
      "Epoch 35/100\n",
      "2278/2278 [==============================] - trainLoss: 20.1607  Val_loss: 13.8298 \n",
      "Epoch 36/100\n",
      "2278/2278 [==============================] - trainLoss: 20.9636  Val_loss: 13.8142 \n",
      "Epoch 37/100\n",
      "2278/2278 [==============================] - trainLoss: 20.9974  Val_loss: 13.7884 \n",
      "Epoch 38/100\n",
      "2278/2278 [==============================] - trainLoss: 19.9668  Val_loss: 13.7446 \n",
      "Epoch 39/100\n",
      "2278/2278 [==============================] - trainLoss: 20.1756  Val_loss: 13.6687 \n",
      "Epoch 40/100\n",
      "2278/2278 [==============================] - trainLoss: 19.5880  Val_loss: 13.7017 \n",
      "Epoch 41/100\n",
      "2278/2278 [==============================] - trainLoss: 19.9003  Val_loss: 13.6782 \n",
      "Epoch 42/100\n",
      "2278/2278 [==============================] - trainLoss: 20.0726  Val_loss: 13.6747 \n",
      "Epoch 43/100\n",
      "2278/2278 [==============================] - trainLoss: 18.8284  Val_loss: 13.7270 \n",
      "Epoch 44/100\n",
      "2278/2278 [==============================] - trainLoss: 20.3594  Val_loss: 13.7619 \n",
      "Epoch 45/100\n",
      "2278/2278 [==============================] - trainLoss: 18.8108  Val_loss: 13.6879 \n",
      "Epoch 46/100\n",
      "2278/2278 [==============================] - trainLoss: 19.3290  Val_loss: 13.7482 \n",
      "Epoch 47/100\n",
      "2278/2278 [==============================] - trainLoss: 19.0349  Val_loss: 13.7911 \n",
      "Epoch 48/100\n",
      "2278/2278 [==============================] - trainLoss: 19.1278  Val_loss: 13.7826 \n",
      "WARNING:tensorflow:No training configuration found in save file: the model was *not* compiled. Compile it manually.\n",
      "WARNING:tensorflow:No training configuration found in save file: the model was *not* compiled. Compile it manually.\n",
      "WARNING:tensorflow:No training configuration found in save file: the model was *not* compiled. Compile it manually.\n",
      "WARNING:tensorflow:No training configuration found in save file: the model was *not* compiled. Compile it manually.\n",
      "Elapsed/s:  57.837257862091064\n",
      "Final training loss:  tf.Tensor(19.127752, shape=(), dtype=float32)\n",
      "best val loss:  tf.Tensor(13.668679, shape=(), dtype=float32)\n",
      "{0: {'parameters': {'n_neurons_classifier': 43, 'n_hidden_classifier': 1, 'learning_rate': 0.001, 'codings_size': 208, 'beta': 15, 'N': 50.0, 'n_neurons': 516, 'n_hidden': 1}, 'val_loss': <tf.Tensor: shape=(), dtype=float32, numpy=18.3234>}, 1: {'parameters': {'n_neurons_classifier': 49, 'n_hidden_classifier': 2, 'learning_rate': 0.0005, 'codings_size': 149, 'beta': 1, 'N': 50.0, 'n_neurons': 238, 'n_hidden': 1}, 'val_loss': <tf.Tensor: shape=(), dtype=float32, numpy=16.69674>}, 2: {'parameters': {'n_neurons_classifier': 94, 'n_hidden_classifier': 1, 'learning_rate': 0.0005, 'codings_size': 107, 'beta': 15, 'N': 100.0, 'n_neurons': 516, 'n_hidden': 2}, 'val_loss': <tf.Tensor: shape=(), dtype=float32, numpy=13.668679>}}\n",
      "Epoch 0/100\n",
      "WARNING:tensorflow:Layer full_model_mmd is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because it's dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "2240/2278 [============================>.] - Loss for batch: 19.1995WARNING:tensorflow:Layer concatenate_1 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because it's dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "2278/2278 [==============================] - trainLoss: 19.1995  Val_loss: 19.5188 \n",
      "Epoch 1/100\n",
      "2278/2278 [==============================] - trainLoss: 11.2607  Val_loss: 18.5815 \n",
      "Epoch 2/100\n",
      "2278/2278 [==============================] - trainLoss: 10.1156  Val_loss: 17.9279 \n",
      "Epoch 3/100\n",
      "2278/2278 [==============================] - trainLoss: 9.2615  Val_loss: 17.3333 \n",
      "Epoch 4/100\n",
      "2278/2278 [==============================] - trainLoss: 8.5941  Val_loss: 16.9146 \n",
      "Epoch 5/100\n",
      "2278/2278 [==============================] - trainLoss: 8.0663  Val_loss: 16.7112 \n",
      "Epoch 6/100\n",
      "2278/2278 [==============================] - trainLoss: 7.8590  Val_loss: 15.8192 \n",
      "Epoch 7/100\n",
      "2278/2278 [==============================] - trainLoss: 7.5829  Val_loss: 15.5532 \n",
      "Epoch 8/100\n",
      "2278/2278 [==============================] - trainLoss: 7.4008  Val_loss: 15.1656 \n",
      "Epoch 9/100\n",
      "2278/2278 [==============================] - trainLoss: 7.2445  Val_loss: 14.9655 \n",
      "Epoch 10/100\n",
      "2278/2278 [==============================] - trainLoss: 7.1477  Val_loss: 14.4516 \n",
      "Epoch 11/100\n",
      "2278/2278 [==============================] - trainLoss: 6.9941  Val_loss: 14.2623 \n",
      "Epoch 12/100\n",
      "2278/2278 [==============================] - trainLoss: 6.9002  Val_loss: 14.0428 \n",
      "Epoch 13/100\n",
      "2278/2278 [==============================] - trainLoss: 6.9139  Val_loss: 13.9044 \n",
      "Epoch 14/100\n",
      "2278/2278 [==============================] - trainLoss: 6.7471  Val_loss: 13.7301 \n",
      "Epoch 15/100\n",
      "2278/2278 [==============================] - trainLoss: 6.7041  Val_loss: 13.6613 \n",
      "Epoch 16/100\n",
      "2278/2278 [==============================] - trainLoss: 6.7138  Val_loss: 13.4650 \n",
      "Epoch 17/100\n",
      "2278/2278 [==============================] - trainLoss: 6.5311  Val_loss: 13.4602 \n",
      "Epoch 18/100\n",
      "2278/2278 [==============================] - trainLoss: 6.5963  Val_loss: 13.2666 \n",
      "Epoch 19/100\n",
      "2278/2278 [==============================] - trainLoss: 6.5093  Val_loss: 13.2071 \n",
      "Epoch 20/100\n",
      "2278/2278 [==============================] - trainLoss: 6.3777  Val_loss: 12.9702 \n",
      "Epoch 21/100\n",
      "2278/2278 [==============================] - trainLoss: 6.4276  Val_loss: 12.8741 \n",
      "Epoch 22/100\n",
      "2278/2278 [==============================] - trainLoss: 6.3137  Val_loss: 12.7679 \n",
      "Epoch 23/100\n",
      "2278/2278 [==============================] - trainLoss: 6.3921  Val_loss: 12.7720 \n",
      "Epoch 24/100\n",
      "2278/2278 [==============================] - trainLoss: 6.2851  Val_loss: 12.8394 \n",
      "Epoch 25/100\n",
      "2278/2278 [==============================] - trainLoss: 6.2165  Val_loss: 12.8881 \n",
      "Epoch 26/100\n",
      "2278/2278 [==============================] - trainLoss: 6.2342  Val_loss: 12.7238 \n",
      "Epoch 27/100\n",
      "2278/2278 [==============================] - trainLoss: 6.2550  Val_loss: 12.6880 \n",
      "Epoch 28/100\n",
      "2278/2278 [==============================] - trainLoss: 6.1921  Val_loss: 12.6475 \n",
      "Epoch 29/100\n",
      "2278/2278 [==============================] - trainLoss: 6.2572  Val_loss: 12.5018 \n",
      "Epoch 30/100\n",
      "2278/2278 [==============================] - trainLoss: 6.0948  Val_loss: 12.5424 \n",
      "Epoch 31/100\n",
      "2278/2278 [==============================] - trainLoss: 6.1161  Val_loss: 12.5329 \n",
      "Epoch 32/100\n",
      "2278/2278 [==============================] - trainLoss: 6.1320  Val_loss: 12.5761 \n",
      "Epoch 33/100\n",
      "2278/2278 [==============================] - trainLoss: 6.0995  Val_loss: 12.5850 \n",
      "Epoch 34/100\n",
      "2278/2278 [==============================] - trainLoss: 6.0469  Val_loss: 12.5528 \n",
      "Epoch 35/100\n",
      "2278/2278 [==============================] - trainLoss: 6.0438  Val_loss: 12.5430 \n",
      "Epoch 36/100\n",
      "2278/2278 [==============================] - trainLoss: 6.0196  Val_loss: 12.4554 \n",
      "Epoch 37/100\n",
      "2278/2278 [==============================] - trainLoss: 6.0317  Val_loss: 12.4942 \n",
      "Epoch 38/100\n",
      "2278/2278 [==============================] - trainLoss: 6.0283  Val_loss: 12.5651 \n",
      "Epoch 39/100\n",
      "2278/2278 [==============================] - trainLoss: 6.0105  Val_loss: 12.4538 \n",
      "Epoch 40/100\n",
      "2278/2278 [==============================] - trainLoss: 6.0074  Val_loss: 12.4111 \n",
      "Epoch 41/100\n",
      "2278/2278 [==============================] - trainLoss: 5.9164  Val_loss: 12.4578 \n",
      "Epoch 42/100\n",
      "2278/2278 [==============================] - trainLoss: 5.9129  Val_loss: 12.5366 \n",
      "Epoch 43/100\n",
      "2278/2278 [==============================] - trainLoss: 6.0425  Val_loss: 12.4985 \n",
      "Epoch 44/100\n",
      "2278/2278 [==============================] - trainLoss: 6.0273  Val_loss: 12.4351 \n",
      "Epoch 45/100\n",
      "2278/2278 [==============================] - trainLoss: 5.9065  Val_loss: 12.4367 \n",
      "Epoch 46/100\n",
      "2278/2278 [==============================] - trainLoss: 5.8467  Val_loss: 12.4697 \n",
      "Epoch 47/100\n",
      "2278/2278 [==============================] - trainLoss: 5.8416  Val_loss: 12.4977 \n",
      "Epoch 48/100\n",
      "2278/2278 [==============================] - trainLoss: 5.9065  Val_loss: 12.4539 \n",
      "Epoch 49/100\n",
      "2278/2278 [==============================] - trainLoss: 5.7991  Val_loss: 12.4508 \n",
      "WARNING:tensorflow:No training configuration found in save file: the model was *not* compiled. Compile it manually.\n",
      "WARNING:tensorflow:No training configuration found in save file: the model was *not* compiled. Compile it manually.\n",
      "WARNING:tensorflow:No training configuration found in save file: the model was *not* compiled. Compile it manually.\n",
      "WARNING:tensorflow:No training configuration found in save file: the model was *not* compiled. Compile it manually.\n",
      "Elapsed/s:  61.74222183227539\n",
      "Final training loss:  tf.Tensor(5.7990756, shape=(), dtype=float32)\n",
      "best val loss:  tf.Tensor(12.411133, shape=(), dtype=float32)\n",
      "{0: {'parameters': {'n_neurons_classifier': 43, 'n_hidden_classifier': 1, 'learning_rate': 0.001, 'codings_size': 208, 'beta': 15, 'N': 50.0, 'n_neurons': 516, 'n_hidden': 1}, 'val_loss': <tf.Tensor: shape=(), dtype=float32, numpy=18.3234>}, 1: {'parameters': {'n_neurons_classifier': 49, 'n_hidden_classifier': 2, 'learning_rate': 0.0005, 'codings_size': 149, 'beta': 1, 'N': 50.0, 'n_neurons': 238, 'n_hidden': 1}, 'val_loss': <tf.Tensor: shape=(), dtype=float32, numpy=16.69674>}, 2: {'parameters': {'n_neurons_classifier': 94, 'n_hidden_classifier': 1, 'learning_rate': 0.0005, 'codings_size': 107, 'beta': 15, 'N': 100.0, 'n_neurons': 516, 'n_hidden': 2}, 'val_loss': <tf.Tensor: shape=(), dtype=float32, numpy=13.668679>}, 3: {'parameters': {'n_neurons_classifier': 52, 'n_hidden_classifier': 1, 'learning_rate': 0.001, 'codings_size': 208, 'beta': 10, 'N': 10.0, 'n_neurons': 320, 'n_hidden': 2}, 'val_loss': <tf.Tensor: shape=(), dtype=float32, numpy=12.411133>}}\n",
      "Epoch 0/100\n",
      "WARNING:tensorflow:Layer full_model_mmd is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because it's dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "2240/2278 [============================>.] - Loss for batch: 22.0078WARNING:tensorflow:Layer concatenate_1 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because it's dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "2278/2278 [==============================] - trainLoss: 22.0078  Val_loss: 17.8077 \n",
      "Epoch 1/100\n",
      "2278/2278 [==============================] - trainLoss: 11.3367  Val_loss: 16.6416 \n",
      "Epoch 2/100\n",
      "2278/2278 [==============================] - trainLoss: 9.2107  Val_loss: 15.9399 \n",
      "Epoch 3/100\n",
      "2278/2278 [==============================] - trainLoss: 8.3362  Val_loss: 15.6260 \n",
      "Epoch 4/100\n",
      "2278/2278 [==============================] - trainLoss: 7.7796  Val_loss: 15.2590 \n",
      "Epoch 5/100\n",
      "2278/2278 [==============================] - trainLoss: 7.4940  Val_loss: 15.0296 \n",
      "Epoch 6/100\n",
      "2278/2278 [==============================] - trainLoss: 7.0884  Val_loss: 14.7776 \n",
      "Epoch 7/100\n",
      "2278/2278 [==============================] - trainLoss: 6.9392  Val_loss: 14.5611 \n",
      "Epoch 8/100\n",
      "2278/2278 [==============================] - trainLoss: 6.7686  Val_loss: 14.4156 \n",
      "Epoch 9/100\n",
      "2278/2278 [==============================] - trainLoss: 6.7027  Val_loss: 14.2256 \n",
      "Epoch 10/100\n",
      "2278/2278 [==============================] - trainLoss: 6.4001  Val_loss: 14.0699 \n",
      "Epoch 11/100\n",
      "2278/2278 [==============================] - trainLoss: 6.2380  Val_loss: 13.9544 \n",
      "Epoch 12/100\n",
      "2278/2278 [==============================] - trainLoss: 6.1468  Val_loss: 13.8711 \n",
      "Epoch 13/100\n",
      "2278/2278 [==============================] - trainLoss: 5.9688  Val_loss: 13.6698 \n",
      "Epoch 14/100\n",
      "2278/2278 [==============================] - trainLoss: 5.8395  Val_loss: 13.5316 \n",
      "Epoch 15/100\n",
      "2278/2278 [==============================] - trainLoss: 5.8182  Val_loss: 13.5014 \n",
      "Epoch 16/100\n",
      "2278/2278 [==============================] - trainLoss: 5.7539  Val_loss: 13.3824 \n",
      "Epoch 17/100\n",
      "2278/2278 [==============================] - trainLoss: 5.5864  Val_loss: 13.3640 \n",
      "Epoch 18/100\n",
      "2278/2278 [==============================] - trainLoss: 5.6266  Val_loss: 13.2215 \n",
      "Epoch 19/100\n",
      "2278/2278 [==============================] - trainLoss: 5.4946  Val_loss: 13.1689 \n",
      "Epoch 20/100\n",
      "2278/2278 [==============================] - trainLoss: 5.4869  Val_loss: 13.1553 \n",
      "Epoch 21/100\n",
      "2278/2278 [==============================] - trainLoss: 5.5024  Val_loss: 13.0349 \n",
      "Epoch 22/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2278/2278 [==============================] - trainLoss: 5.3715  Val_loss: 13.0178 \n",
      "Epoch 23/100\n",
      "2278/2278 [==============================] - trainLoss: 5.3640  Val_loss: 12.9642 \n",
      "Epoch 24/100\n",
      "2278/2278 [==============================] - trainLoss: 5.3334  Val_loss: 12.8915 \n",
      "Epoch 25/100\n",
      "2278/2278 [==============================] - trainLoss: 5.2390  Val_loss: 12.8335 \n",
      "Epoch 26/100\n",
      "2278/2278 [==============================] - trainLoss: 5.2385  Val_loss: 12.8316 \n",
      "Epoch 27/100\n",
      "2278/2278 [==============================] - trainLoss: 5.2125  Val_loss: 12.8149 \n",
      "Epoch 28/100\n",
      "2278/2278 [==============================] - trainLoss: 5.2458  Val_loss: 12.7366 \n",
      "Epoch 29/100\n",
      "2278/2278 [==============================] - trainLoss: 5.1195  Val_loss: 12.6950 \n",
      "Epoch 30/100\n",
      "2278/2278 [==============================] - trainLoss: 5.1645  Val_loss: 12.7457 \n",
      "Epoch 31/100\n",
      "2278/2278 [==============================] - trainLoss: 5.0599  Val_loss: 12.6815 \n",
      "Epoch 32/100\n",
      "2278/2278 [==============================] - trainLoss: 5.0769  Val_loss: 12.6632 \n",
      "Epoch 33/100\n",
      "2278/2278 [==============================] - trainLoss: 5.0544  Val_loss: 12.6552 \n",
      "Epoch 34/100\n",
      "2278/2278 [==============================] - trainLoss: 5.1602  Val_loss: 12.4909 \n",
      "Epoch 35/100\n",
      "2278/2278 [==============================] - trainLoss: 5.0996  Val_loss: 12.5274 \n",
      "Epoch 36/100\n",
      "2278/2278 [==============================] - trainLoss: 5.0213  Val_loss: 12.5199 \n",
      "Epoch 37/100\n",
      "2278/2278 [==============================] - trainLoss: 4.9384  Val_loss: 12.4741 \n",
      "Epoch 38/100\n",
      "2278/2278 [==============================] - trainLoss: 5.0249  Val_loss: 12.4427 \n",
      "Epoch 39/100\n",
      "2278/2278 [==============================] - trainLoss: 5.0090  Val_loss: 12.4564 \n",
      "Epoch 40/100\n",
      "2278/2278 [==============================] - trainLoss: 4.9236  Val_loss: 12.4160 \n",
      "Epoch 41/100\n",
      "2278/2278 [==============================] - trainLoss: 4.9930  Val_loss: 12.4357 \n",
      "Epoch 42/100\n",
      "2278/2278 [==============================] - trainLoss: 4.9189  Val_loss: 12.4390 \n",
      "Epoch 43/100\n",
      "2278/2278 [==============================] - trainLoss: 4.8870  Val_loss: 12.4760 \n",
      "Epoch 44/100\n",
      "2278/2278 [==============================] - trainLoss: 4.9794  Val_loss: 12.4068 \n",
      "Epoch 45/100\n",
      "2278/2278 [==============================] - trainLoss: 4.9659  Val_loss: 12.4284 \n",
      "Epoch 46/100\n",
      "2278/2278 [==============================] - trainLoss: 4.9037  Val_loss: 12.4679 \n",
      "Epoch 47/100\n",
      "2278/2278 [==============================] - trainLoss: 4.9402  Val_loss: 12.4192 \n",
      "Epoch 48/100\n",
      "2278/2278 [==============================] - trainLoss: 4.9370  Val_loss: 12.4318 \n",
      "Epoch 49/100\n",
      "2278/2278 [==============================] - trainLoss: 4.8360  Val_loss: 12.3729 \n",
      "Epoch 50/100\n",
      "2278/2278 [==============================] - trainLoss: 4.8811  Val_loss: 12.4467 \n",
      "Epoch 51/100\n",
      "2278/2278 [==============================] - trainLoss: 4.8845  Val_loss: 12.4025 \n",
      "Epoch 52/100\n",
      "2278/2278 [==============================] - trainLoss: 4.8279  Val_loss: 12.3571 \n",
      "Epoch 53/100\n",
      "2278/2278 [==============================] - trainLoss: 4.8046  Val_loss: 12.4202 \n",
      "Epoch 54/100\n",
      "2278/2278 [==============================] - trainLoss: 4.8285  Val_loss: 12.3422 \n",
      "Epoch 55/100\n",
      "2278/2278 [==============================] - trainLoss: 4.8386  Val_loss: 12.3355 \n",
      "Epoch 56/100\n",
      "2278/2278 [==============================] - trainLoss: 4.8966  Val_loss: 12.3508 \n",
      "Epoch 57/100\n",
      "2278/2278 [==============================] - trainLoss: 4.8630  Val_loss: 12.4218 \n",
      "Epoch 58/100\n",
      "2278/2278 [==============================] - trainLoss: 4.8323  Val_loss: 12.2766 \n",
      "Epoch 59/100\n",
      "2278/2278 [==============================] - trainLoss: 4.8445  Val_loss: 12.2868 \n",
      "Epoch 60/100\n",
      "2278/2278 [==============================] - trainLoss: 4.8281  Val_loss: 12.3255 \n",
      "Epoch 61/100\n",
      "2278/2278 [==============================] - trainLoss: 4.8299  Val_loss: 12.3458 \n",
      "Epoch 62/100\n",
      "2278/2278 [==============================] - trainLoss: 4.7823  Val_loss: 12.3691 \n",
      "Epoch 63/100\n",
      "2278/2278 [==============================] - trainLoss: 4.7993  Val_loss: 12.3149 \n",
      "Epoch 64/100\n",
      "2278/2278 [==============================] - trainLoss: 4.8240  Val_loss: 12.3188 \n",
      "Epoch 65/100\n",
      "2278/2278 [==============================] - trainLoss: 4.7864  Val_loss: 12.3531 \n",
      "Epoch 66/100\n",
      "2278/2278 [==============================] - trainLoss: 4.7502  Val_loss: 12.2340 \n",
      "Epoch 67/100\n",
      "2278/2278 [==============================] - trainLoss: 4.7844  Val_loss: 12.2842 \n",
      "Epoch 68/100\n",
      "2278/2278 [==============================] - trainLoss: 4.7970  Val_loss: 12.2947 \n",
      "Epoch 69/100\n",
      "2278/2278 [==============================] - trainLoss: 4.7106  Val_loss: 12.3260 \n",
      "Epoch 70/100\n",
      "2278/2278 [==============================] - trainLoss: 4.8030  Val_loss: 12.2987 \n",
      "Epoch 71/100\n",
      "2278/2278 [==============================] - trainLoss: 4.7539  Val_loss: 12.3226 \n",
      "Epoch 72/100\n",
      "2278/2278 [==============================] - trainLoss: 4.7503  Val_loss: 12.2788 \n",
      "Epoch 73/100\n",
      "2278/2278 [==============================] - trainLoss: 4.7352  Val_loss: 12.3023 \n",
      "Epoch 74/100\n",
      "2278/2278 [==============================] - trainLoss: 4.7229  Val_loss: 12.2597 \n",
      "Epoch 75/100\n",
      "2278/2278 [==============================] - trainLoss: 4.6907  Val_loss: 12.2346 \n",
      "WARNING:tensorflow:No training configuration found in save file: the model was *not* compiled. Compile it manually.\n",
      "WARNING:tensorflow:No training configuration found in save file: the model was *not* compiled. Compile it manually.\n",
      "WARNING:tensorflow:No training configuration found in save file: the model was *not* compiled. Compile it manually.\n",
      "WARNING:tensorflow:No training configuration found in save file: the model was *not* compiled. Compile it manually.\n",
      "Elapsed/s:  87.90529346466064\n",
      "Final training loss:  tf.Tensor(4.6907015, shape=(), dtype=float32)\n",
      "best val loss:  tf.Tensor(12.23399, shape=(), dtype=float32)\n",
      "{0: {'parameters': {'n_neurons_classifier': 43, 'n_hidden_classifier': 1, 'learning_rate': 0.001, 'codings_size': 208, 'beta': 15, 'N': 50.0, 'n_neurons': 516, 'n_hidden': 1}, 'val_loss': <tf.Tensor: shape=(), dtype=float32, numpy=18.3234>}, 1: {'parameters': {'n_neurons_classifier': 49, 'n_hidden_classifier': 2, 'learning_rate': 0.0005, 'codings_size': 149, 'beta': 1, 'N': 50.0, 'n_neurons': 238, 'n_hidden': 1}, 'val_loss': <tf.Tensor: shape=(), dtype=float32, numpy=16.69674>}, 2: {'parameters': {'n_neurons_classifier': 94, 'n_hidden_classifier': 1, 'learning_rate': 0.0005, 'codings_size': 107, 'beta': 15, 'N': 100.0, 'n_neurons': 516, 'n_hidden': 2}, 'val_loss': <tf.Tensor: shape=(), dtype=float32, numpy=13.668679>}, 3: {'parameters': {'n_neurons_classifier': 52, 'n_hidden_classifier': 1, 'learning_rate': 0.001, 'codings_size': 208, 'beta': 10, 'N': 10.0, 'n_neurons': 320, 'n_hidden': 2}, 'val_loss': <tf.Tensor: shape=(), dtype=float32, numpy=12.411133>}, 4: {'parameters': {'n_neurons_classifier': 52, 'n_hidden_classifier': 2, 'learning_rate': 0.0005, 'codings_size': 91, 'beta': 10, 'N': 1.0, 'n_neurons': 152, 'n_hidden': 2}, 'val_loss': <tf.Tensor: shape=(), dtype=float32, numpy=12.23399>}}\n",
      "Epoch 0/100\n",
      "WARNING:tensorflow:Layer full_model_mmd is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because it's dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "2240/2278 [============================>.] - Loss for batch: 44.8943WARNING:tensorflow:Layer concatenate_1 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because it's dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2278/2278 [==============================] - trainLoss: 44.8943  Val_loss: 19.7968 \n",
      "Epoch 1/100\n",
      "2278/2278 [==============================] - trainLoss: 32.3767  Val_loss: 18.6898 \n",
      "Epoch 2/100\n",
      "2278/2278 [==============================] - trainLoss: 30.4540  Val_loss: 18.2388 \n",
      "Epoch 3/100\n",
      "2278/2278 [==============================] - trainLoss: 29.1374  Val_loss: 17.9050 \n",
      "Epoch 4/100\n",
      "2278/2278 [==============================] - trainLoss: 28.8820  Val_loss: 17.7060 \n",
      "Epoch 5/100\n",
      "2278/2278 [==============================] - trainLoss: 28.4973  Val_loss: 17.4092 \n",
      "Epoch 6/100\n",
      "2278/2278 [==============================] - trainLoss: 27.2932  Val_loss: 17.2276 \n",
      "Epoch 7/100\n",
      "2278/2278 [==============================] - trainLoss: 26.6696  Val_loss: 17.1601 \n",
      "Epoch 8/100\n",
      "2278/2278 [==============================] - trainLoss: 27.0306  Val_loss: 16.7980 \n",
      "Epoch 9/100\n",
      "2278/2278 [==============================] - trainLoss: 26.3938  Val_loss: 16.6440 \n",
      "Epoch 10/100\n",
      "2278/2278 [==============================] - trainLoss: 26.3288  Val_loss: 16.4414 \n",
      "Epoch 11/100\n",
      "2278/2278 [==============================] - trainLoss: 25.8816  Val_loss: 16.2299 \n",
      "Epoch 12/100\n",
      "2278/2278 [==============================] - trainLoss: 25.9399  Val_loss: 15.9414 \n",
      "Epoch 13/100\n",
      "2278/2278 [==============================] - trainLoss: 25.6894  Val_loss: 16.0061 \n",
      "Epoch 14/100\n",
      "2278/2278 [==============================] - trainLoss: 25.5313  Val_loss: 15.8460 \n",
      "Epoch 15/100\n",
      "2278/2278 [==============================] - trainLoss: 25.2407  Val_loss: 15.4634 \n",
      "Epoch 16/100\n",
      "2278/2278 [==============================] - trainLoss: 24.6544  Val_loss: 15.4571 \n",
      "Epoch 17/100\n",
      "2278/2278 [==============================] - trainLoss: 24.6245  Val_loss: 15.3094 \n",
      "Epoch 18/100\n",
      "2278/2278 [==============================] - trainLoss: 24.2378  Val_loss: 15.0534 \n",
      "Epoch 19/100\n",
      "2278/2278 [==============================] - trainLoss: 24.0906  Val_loss: 14.9010 \n",
      "Epoch 20/100\n",
      "2278/2278 [==============================] - trainLoss: 24.0310  Val_loss: 14.8619 \n",
      "Epoch 21/100\n",
      "2278/2278 [==============================] - trainLoss: 23.6507  Val_loss: 14.7067 \n",
      "Epoch 22/100\n",
      "2278/2278 [==============================] - trainLoss: 23.9045  Val_loss: 14.6304 \n",
      "Epoch 23/100\n",
      "2278/2278 [==============================] - trainLoss: 23.5665  Val_loss: 14.4672 \n",
      "Epoch 24/100\n",
      "2278/2278 [==============================] - trainLoss: 22.9979  Val_loss: 14.4262 \n",
      "Epoch 25/100\n",
      "2278/2278 [==============================] - trainLoss: 23.3007  Val_loss: 14.2769 \n",
      "Epoch 26/100\n",
      "2278/2278 [==============================] - trainLoss: 23.2782  Val_loss: 14.0625 \n",
      "Epoch 27/100\n",
      "2278/2278 [==============================] - trainLoss: 22.8984  Val_loss: 13.9891 \n",
      "Epoch 28/100\n",
      "2278/2278 [==============================] - trainLoss: 22.2207  Val_loss: 13.8195 \n",
      "Epoch 29/100\n",
      "2278/2278 [==============================] - trainLoss: 23.3321  Val_loss: 13.8716 \n",
      "Epoch 30/100\n",
      "2278/2278 [==============================] - trainLoss: 22.2036  Val_loss: 13.6880 \n",
      "Epoch 31/100\n",
      "2278/2278 [==============================] - trainLoss: 22.3247  Val_loss: 13.6051 \n",
      "Epoch 32/100\n",
      "2278/2278 [==============================] - trainLoss: 22.0084  Val_loss: 13.5241 \n",
      "Epoch 33/100\n",
      "2278/2278 [==============================] - trainLoss: 22.1649  Val_loss: 13.4576 \n",
      "Epoch 34/100\n",
      "2278/2278 [==============================] - trainLoss: 22.6209  Val_loss: 13.3431 \n",
      "Epoch 35/100\n",
      "2278/2278 [==============================] - trainLoss: 21.8095  Val_loss: 13.2900 \n",
      "Epoch 36/100\n",
      "2278/2278 [==============================] - trainLoss: 22.0835  Val_loss: 13.1825 \n",
      "Epoch 37/100\n",
      "2278/2278 [==============================] - trainLoss: 21.8044  Val_loss: 13.1862 \n",
      "Epoch 38/100\n",
      "2278/2278 [==============================] - trainLoss: 22.1582  Val_loss: 13.0194 \n",
      "Epoch 39/100\n",
      "2278/2278 [==============================] - trainLoss: 21.3332  Val_loss: 13.0049 \n",
      "Epoch 40/100\n",
      "2278/2278 [==============================] - trainLoss: 20.8559  Val_loss: 12.9324 \n",
      "Epoch 41/100\n",
      "2278/2278 [==============================] - trainLoss: 21.8281  Val_loss: 12.8887 \n",
      "Epoch 42/100\n",
      "2278/2278 [==============================] - trainLoss: 20.8355  Val_loss: 12.8785 \n",
      "Epoch 43/100\n",
      "2278/2278 [==============================] - trainLoss: 21.0122  Val_loss: 12.7497 \n",
      "Epoch 44/100\n",
      "2278/2278 [==============================] - trainLoss: 21.2365  Val_loss: 12.7811 \n",
      "Epoch 45/100\n",
      "2278/2278 [==============================] - trainLoss: 20.5184  Val_loss: 12.7819 \n",
      "Epoch 46/100\n",
      "2278/2278 [==============================] - trainLoss: 21.1770  Val_loss: 12.7056 \n",
      "Epoch 47/100\n",
      "2278/2278 [==============================] - trainLoss: 20.3800  Val_loss: 12.7367 \n",
      "Epoch 48/100\n",
      "2278/2278 [==============================] - trainLoss: 21.0493  Val_loss: 12.7511 \n",
      "Epoch 49/100\n",
      "2278/2278 [==============================] - trainLoss: 20.5650  Val_loss: 12.7679 \n",
      "Epoch 50/100\n",
      "2278/2278 [==============================] - trainLoss: 20.8651  Val_loss: 12.5903 \n",
      "Epoch 51/100\n",
      "2278/2278 [==============================] - trainLoss: 19.8960  Val_loss: 12.6939 \n",
      "Epoch 52/100\n",
      "2278/2278 [==============================] - trainLoss: 19.9546  Val_loss: 12.6486 \n",
      "Epoch 53/100\n",
      "2278/2278 [==============================] - trainLoss: 19.2429  Val_loss: 12.7000 \n",
      "Epoch 54/100\n",
      "2278/2278 [==============================] - trainLoss: 19.8471  Val_loss: 12.5709 \n",
      "Epoch 55/100\n",
      "2278/2278 [==============================] - trainLoss: 19.5978  Val_loss: 12.5989 \n",
      "Epoch 56/100\n",
      "2278/2278 [==============================] - trainLoss: 19.6886  Val_loss: 12.5817 \n",
      "Epoch 57/100\n",
      "2278/2278 [==============================] - trainLoss: 19.9296  Val_loss: 12.5344 \n",
      "Epoch 58/100\n",
      "2278/2278 [==============================] - trainLoss: 19.2291  Val_loss: 12.5038 \n",
      "Epoch 59/100\n",
      "2278/2278 [==============================] - trainLoss: 19.5129  Val_loss: 12.5209 \n",
      "Epoch 60/100\n",
      "2278/2278 [==============================] - trainLoss: 19.4027  Val_loss: 12.5017 \n",
      "Epoch 61/100\n",
      "2278/2278 [==============================] - trainLoss: 19.7305  Val_loss: 12.4646 \n",
      "Epoch 62/100\n",
      "2278/2278 [==============================] - trainLoss: 19.3603  Val_loss: 12.4740 \n",
      "Epoch 63/100\n",
      "2278/2278 [==============================] - trainLoss: 19.3771  Val_loss: 12.4741 \n",
      "Epoch 64/100\n",
      "2278/2278 [==============================] - trainLoss: 19.1237  Val_loss: 12.5330 \n",
      "Epoch 65/100\n",
      "2278/2278 [==============================] - trainLoss: 19.0710  Val_loss: 12.4603 \n",
      "Epoch 66/100\n",
      "2278/2278 [==============================] - trainLoss: 18.6210  Val_loss: 12.4364 \n",
      "Epoch 67/100\n",
      "2278/2278 [==============================] - trainLoss: 18.0644  Val_loss: 12.4002 \n",
      "Epoch 68/100\n",
      "2278/2278 [==============================] - trainLoss: 18.4953  Val_loss: 12.3582 \n",
      "Epoch 69/100\n",
      "2278/2278 [==============================] - trainLoss: 18.9800  Val_loss: 12.3424 \n",
      "Epoch 70/100\n",
      "2278/2278 [==============================] - trainLoss: 18.1969  Val_loss: 12.2987 \n",
      "Epoch 71/100\n",
      "2278/2278 [==============================] - trainLoss: 18.6214  Val_loss: 12.3671 \n",
      "Epoch 72/100\n",
      "2278/2278 [==============================] - trainLoss: 18.2656  Val_loss: 12.2803 \n",
      "Epoch 73/100\n",
      "2278/2278 [==============================] - trainLoss: 18.0792  Val_loss: 12.3695 \n",
      "Epoch 74/100\n",
      "2278/2278 [==============================] - trainLoss: 18.2925  Val_loss: 12.4024 \n",
      "Epoch 75/100\n",
      "2278/2278 [==============================] - trainLoss: 17.7493  Val_loss: 12.3691 \n",
      "Epoch 76/100\n",
      "2278/2278 [==============================] - trainLoss: 17.9023  Val_loss: 12.3014 \n",
      "Epoch 77/100\n",
      "2278/2278 [==============================] - trainLoss: 17.7898  Val_loss: 12.3044 \n",
      "Epoch 78/100\n",
      "2278/2278 [==============================] - trainLoss: 17.3858  Val_loss: 12.3152 \n",
      "Epoch 79/100\n",
      "2278/2278 [==============================] - trainLoss: 17.4748  Val_loss: 12.3543 \n",
      "Epoch 80/100\n",
      "2278/2278 [==============================] - trainLoss: 17.8508  Val_loss: 12.3320 \n",
      "Epoch 81/100\n",
      "2278/2278 [==============================] - trainLoss: 17.2754  Val_loss: 12.3595 \n",
      "WARNING:tensorflow:No training configuration found in save file: the model was *not* compiled. Compile it manually.\n",
      "WARNING:tensorflow:No training configuration found in save file: the model was *not* compiled. Compile it manually.\n",
      "WARNING:tensorflow:No training configuration found in save file: the model was *not* compiled. Compile it manually.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:No training configuration found in save file: the model was *not* compiled. Compile it manually.\n",
      "Elapsed/s:  97.48097538948059\n",
      "Final training loss:  tf.Tensor(17.275396, shape=(), dtype=float32)\n",
      "best val loss:  tf.Tensor(12.280304, shape=(), dtype=float32)\n",
      "{0: {'parameters': {'n_neurons_classifier': 43, 'n_hidden_classifier': 1, 'learning_rate': 0.001, 'codings_size': 208, 'beta': 15, 'N': 50.0, 'n_neurons': 516, 'n_hidden': 1}, 'val_loss': <tf.Tensor: shape=(), dtype=float32, numpy=18.3234>}, 1: {'parameters': {'n_neurons_classifier': 49, 'n_hidden_classifier': 2, 'learning_rate': 0.0005, 'codings_size': 149, 'beta': 1, 'N': 50.0, 'n_neurons': 238, 'n_hidden': 1}, 'val_loss': <tf.Tensor: shape=(), dtype=float32, numpy=16.69674>}, 2: {'parameters': {'n_neurons_classifier': 94, 'n_hidden_classifier': 1, 'learning_rate': 0.0005, 'codings_size': 107, 'beta': 15, 'N': 100.0, 'n_neurons': 516, 'n_hidden': 2}, 'val_loss': <tf.Tensor: shape=(), dtype=float32, numpy=13.668679>}, 3: {'parameters': {'n_neurons_classifier': 52, 'n_hidden_classifier': 1, 'learning_rate': 0.001, 'codings_size': 208, 'beta': 10, 'N': 10.0, 'n_neurons': 320, 'n_hidden': 2}, 'val_loss': <tf.Tensor: shape=(), dtype=float32, numpy=12.411133>}, 4: {'parameters': {'n_neurons_classifier': 52, 'n_hidden_classifier': 2, 'learning_rate': 0.0005, 'codings_size': 91, 'beta': 10, 'N': 1.0, 'n_neurons': 152, 'n_hidden': 2}, 'val_loss': <tf.Tensor: shape=(), dtype=float32, numpy=12.23399>}, 5: {'parameters': {'n_neurons_classifier': 49, 'n_hidden_classifier': 1, 'learning_rate': 0.0005, 'codings_size': 126, 'beta': 1, 'N': 100.0, 'n_neurons': 238, 'n_hidden': 2}, 'val_loss': <tf.Tensor: shape=(), dtype=float32, numpy=12.280304>}}\n",
      "Epoch 0/100\n",
      "WARNING:tensorflow:Layer full_model_mmd is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because it's dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "2240/2278 [============================>.] - Loss for batch: 16.5394WARNING:tensorflow:Layer concatenate_1 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because it's dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "2278/2278 [==============================] - trainLoss: 16.5394  Val_loss: 22.3492 \n",
      "Epoch 1/100\n",
      "2278/2278 [==============================] - trainLoss: 8.8242  Val_loss: 21.4639 \n",
      "Epoch 2/100\n",
      "2278/2278 [==============================] - trainLoss: 7.6014  Val_loss: 21.0593 \n",
      "Epoch 3/100\n",
      "2278/2278 [==============================] - trainLoss: 6.9310  Val_loss: 20.5932 \n",
      "Epoch 4/100\n",
      "2278/2278 [==============================] - trainLoss: 6.5485  Val_loss: 20.1572 \n",
      "Epoch 5/100\n",
      "2278/2278 [==============================] - trainLoss: 6.1100  Val_loss: 19.9260 \n",
      "Epoch 6/100\n",
      "2278/2278 [==============================] - trainLoss: 5.8257  Val_loss: 19.6502 \n",
      "Epoch 7/100\n",
      "2278/2278 [==============================] - trainLoss: 5.5636  Val_loss: 19.4282 \n",
      "Epoch 8/100\n",
      "2278/2278 [==============================] - trainLoss: 5.4173  Val_loss: 19.1858 \n",
      "Epoch 9/100\n",
      "2278/2278 [==============================] - trainLoss: 5.3035  Val_loss: 19.0034 \n",
      "Epoch 10/100\n",
      "2278/2278 [==============================] - trainLoss: 5.1740  Val_loss: 19.0193 \n",
      "Epoch 11/100\n",
      "2278/2278 [==============================] - trainLoss: 5.0294  Val_loss: 18.7304 \n",
      "Epoch 12/100\n",
      "2278/2278 [==============================] - trainLoss: 4.9356  Val_loss: 18.5292 \n",
      "Epoch 13/100\n",
      "2278/2278 [==============================] - trainLoss: 4.8776  Val_loss: 18.7569 \n",
      "Epoch 14/100\n",
      "2278/2278 [==============================] - trainLoss: 4.8253  Val_loss: 18.3820 \n",
      "Epoch 15/100\n",
      "2278/2278 [==============================] - trainLoss: 4.7256  Val_loss: 18.3030 \n",
      "Epoch 16/100\n",
      "2278/2278 [==============================] - trainLoss: 4.6150  Val_loss: 18.1192 \n",
      "Epoch 17/100\n",
      "2278/2278 [==============================] - trainLoss: 4.5928  Val_loss: 18.2863 \n",
      "Epoch 18/100\n",
      "2278/2278 [==============================] - trainLoss: 4.5534  Val_loss: 17.9532 \n",
      "Epoch 19/100\n",
      "2278/2278 [==============================] - trainLoss: 4.4541  Val_loss: 17.8397 \n",
      "Epoch 20/100\n",
      "2278/2278 [==============================] - trainLoss: 4.3912  Val_loss: 17.7257 \n",
      "Epoch 21/100\n",
      "2278/2278 [==============================] - trainLoss: 4.3907  Val_loss: 17.7621 \n",
      "Epoch 22/100\n",
      "2278/2278 [==============================] - trainLoss: 4.3842  Val_loss: 17.7045 \n",
      "Epoch 23/100\n",
      "2278/2278 [==============================] - trainLoss: 4.3716  Val_loss: 17.6374 \n",
      "Epoch 24/100\n",
      "2278/2278 [==============================] - trainLoss: 4.3202  Val_loss: 17.7483 \n",
      "Epoch 25/100\n",
      "2278/2278 [==============================] - trainLoss: 4.2385  Val_loss: 17.6023 \n",
      "Epoch 26/100\n",
      "2278/2278 [==============================] - trainLoss: 4.2301  Val_loss: 17.4440 \n",
      "Epoch 27/100\n",
      "2278/2278 [==============================] - trainLoss: 4.2388  Val_loss: 17.3129 \n",
      "Epoch 28/100\n",
      "2278/2278 [==============================] - trainLoss: 4.1732  Val_loss: 17.2379 \n",
      "Epoch 29/100\n",
      "2278/2278 [==============================] - trainLoss: 4.1947  Val_loss: 17.3439 \n",
      "Epoch 30/100\n",
      "2278/2278 [==============================] - trainLoss: 4.1541  Val_loss: 17.1304 \n",
      "Epoch 31/100\n",
      "2278/2278 [==============================] - trainLoss: 4.1340  Val_loss: 17.1809 \n",
      "Epoch 32/100\n",
      "2278/2278 [==============================] - trainLoss: 4.0855  Val_loss: 17.1594 \n",
      "Epoch 33/100\n",
      "2278/2278 [==============================] - trainLoss: 4.1302  Val_loss: 17.2543 \n",
      "Epoch 34/100\n",
      "2278/2278 [==============================] - trainLoss: 4.0374  Val_loss: 17.1559 \n",
      "Epoch 35/100\n",
      "2278/2278 [==============================] - trainLoss: 4.0372  Val_loss: 17.2071 \n",
      "Epoch 36/100\n",
      "2278/2278 [==============================] - trainLoss: 4.0289  Val_loss: 17.1126 \n",
      "Epoch 37/100\n",
      "2278/2278 [==============================] - trainLoss: 4.0519  Val_loss: 17.0890 \n",
      "Epoch 38/100\n",
      "2278/2278 [==============================] - trainLoss: 4.0336  Val_loss: 17.0529 \n",
      "Epoch 39/100\n",
      "2278/2278 [==============================] - trainLoss: 3.9757  Val_loss: 16.9020 \n",
      "Epoch 40/100\n",
      "2278/2278 [==============================] - trainLoss: 3.9733  Val_loss: 16.9861 \n",
      "Epoch 41/100\n",
      "2278/2278 [==============================] - trainLoss: 3.9772  Val_loss: 16.9787 \n",
      "Epoch 42/100\n",
      "2278/2278 [==============================] - trainLoss: 3.9716  Val_loss: 16.9000 \n",
      "Epoch 43/100\n",
      "2278/2278 [==============================] - trainLoss: 3.9123  Val_loss: 16.9501 \n",
      "Epoch 44/100\n",
      "2278/2278 [==============================] - trainLoss: 3.9628  Val_loss: 16.9085 \n",
      "Epoch 45/100\n",
      "2278/2278 [==============================] - trainLoss: 3.8802  Val_loss: 16.9227 \n",
      "Epoch 46/100\n",
      "2278/2278 [==============================] - trainLoss: 3.9210  Val_loss: 16.9616 \n",
      "Epoch 47/100\n",
      "2278/2278 [==============================] - trainLoss: 3.9177  Val_loss: 17.0303 \n",
      "Epoch 48/100\n",
      "2278/2278 [==============================] - trainLoss: 3.9200  Val_loss: 16.9153 \n",
      "Epoch 49/100\n",
      "2278/2278 [==============================] - trainLoss: 3.8797  Val_loss: 16.8567 \n",
      "Epoch 50/100\n",
      "2278/2278 [==============================] - trainLoss: 3.8838  Val_loss: 16.8726 \n",
      "Epoch 51/100\n",
      "2278/2278 [==============================] - trainLoss: 3.9196  Val_loss: 16.9169 \n",
      "Epoch 52/100\n",
      "2278/2278 [==============================] - trainLoss: 3.8753  Val_loss: 16.8538 \n",
      "Epoch 53/100\n",
      "2278/2278 [==============================] - trainLoss: 3.8667  Val_loss: 16.8785 \n",
      "Epoch 54/100\n",
      "2278/2278 [==============================] - trainLoss: 3.8801  Val_loss: 16.8268 \n",
      "Epoch 55/100\n",
      "2278/2278 [==============================] - trainLoss: 3.8880  Val_loss: 16.9199 \n",
      "Epoch 56/100\n",
      "2278/2278 [==============================] - trainLoss: 3.8654  Val_loss: 16.8287 \n",
      "Epoch 57/100\n",
      "2278/2278 [==============================] - trainLoss: 3.8608  Val_loss: 16.8126 \n",
      "Epoch 58/100\n",
      "2278/2278 [==============================] - trainLoss: 3.8457  Val_loss: 16.8513 \n",
      "Epoch 59/100\n",
      "2278/2278 [==============================] - trainLoss: 3.8097  Val_loss: 16.8215 \n",
      "Epoch 60/100\n",
      "2278/2278 [==============================] - trainLoss: 3.8573  Val_loss: 16.7278 \n",
      "Epoch 61/100\n",
      "2278/2278 [==============================] - trainLoss: 3.8442  Val_loss: 16.7172 \n",
      "Epoch 62/100\n",
      "2278/2278 [==============================] - trainLoss: 3.7985  Val_loss: 16.7403 \n",
      "Epoch 63/100\n",
      "2278/2278 [==============================] - trainLoss: 3.7876  Val_loss: 16.7223 \n",
      "Epoch 64/100\n",
      "2278/2278 [==============================] - trainLoss: 3.8065  Val_loss: 16.7762 \n",
      "Epoch 65/100\n",
      "2278/2278 [==============================] - trainLoss: 3.7830  Val_loss: 16.7439 \n",
      "Epoch 66/100\n",
      "2278/2278 [==============================] - trainLoss: 3.8443  Val_loss: 16.7448 \n",
      "Epoch 67/100\n",
      "2278/2278 [==============================] - trainLoss: 3.8368  Val_loss: 16.8419 \n",
      "Epoch 68/100\n",
      "2278/2278 [==============================] - trainLoss: 3.8053  Val_loss: 16.8110 \n",
      "Epoch 69/100\n",
      "2278/2278 [==============================] - trainLoss: 3.7622  Val_loss: 16.7250 \n",
      "Epoch 70/100\n",
      "2278/2278 [==============================] - trainLoss: 3.8018  Val_loss: 16.7044 \n",
      "Epoch 71/100\n",
      "2278/2278 [==============================] - trainLoss: 3.7899  Val_loss: 16.7209 \n",
      "Epoch 72/100\n",
      "2278/2278 [==============================] - trainLoss: 3.7657  Val_loss: 16.6742 \n",
      "Epoch 73/100\n",
      "2278/2278 [==============================] - trainLoss: 3.7830  Val_loss: 16.6820 \n",
      "Epoch 74/100\n",
      "2278/2278 [==============================] - trainLoss: 3.7476  Val_loss: 16.6484 \n",
      "Epoch 75/100\n",
      "2278/2278 [==============================] - trainLoss: 3.7820  Val_loss: 16.6628 \n",
      "Epoch 76/100\n",
      "2278/2278 [==============================] - trainLoss: 3.7777  Val_loss: 16.6648 \n",
      "Epoch 77/100\n",
      "2278/2278 [==============================] - trainLoss: 3.7922  Val_loss: 16.5955 \n",
      "Epoch 78/100\n",
      "2278/2278 [==============================] - trainLoss: 3.7458  Val_loss: 16.5937 \n",
      "Epoch 79/100\n",
      "2278/2278 [==============================] - trainLoss: 3.7680  Val_loss: 16.6024 \n",
      "Epoch 80/100\n",
      "2278/2278 [==============================] - trainLoss: 3.7245  Val_loss: 16.5336 \n",
      "Epoch 81/100\n",
      "2278/2278 [==============================] - trainLoss: 3.7680  Val_loss: 16.6296 \n",
      "Epoch 82/100\n",
      "2278/2278 [==============================] - trainLoss: 3.8019  Val_loss: 16.5454 \n",
      "Epoch 83/100\n",
      "2278/2278 [==============================] - trainLoss: 3.7577  Val_loss: 16.5329 \n",
      "Epoch 84/100\n",
      "2278/2278 [==============================] - trainLoss: 3.7468  Val_loss: 16.5865 \n",
      "Epoch 85/100\n",
      "2278/2278 [==============================] - trainLoss: 3.7334  Val_loss: 16.5667 \n",
      "Epoch 86/100\n",
      "2278/2278 [==============================] - trainLoss: 3.7345  Val_loss: 16.4680 \n",
      "Epoch 87/100\n",
      "2278/2278 [==============================] - trainLoss: 3.7372  Val_loss: 16.5475 \n",
      "Epoch 88/100\n",
      "2278/2278 [==============================] - trainLoss: 3.7272  Val_loss: 16.4325 \n",
      "Epoch 89/100\n",
      "2278/2278 [==============================] - trainLoss: 3.7181  Val_loss: 16.4590 \n",
      "Epoch 90/100\n",
      "2278/2278 [==============================] - trainLoss: 3.7338  Val_loss: 16.5242 \n",
      "Epoch 91/100\n",
      "2278/2278 [==============================] - trainLoss: 3.7078  Val_loss: 16.5369 \n",
      "Epoch 92/100\n",
      "2278/2278 [==============================] - trainLoss: 3.7046  Val_loss: 16.4430 \n",
      "Epoch 93/100\n",
      "2278/2278 [==============================] - trainLoss: 3.7228  Val_loss: 16.4840 \n",
      "Epoch 94/100\n",
      "2278/2278 [==============================] - trainLoss: 3.7398  Val_loss: 16.4256 \n",
      "Epoch 95/100\n",
      "2278/2278 [==============================] - trainLoss: 3.7188  Val_loss: 16.4232 \n",
      "Epoch 96/100\n",
      "2278/2278 [==============================] - trainLoss: 3.7423  Val_loss: 16.3837 \n",
      "Epoch 97/100\n",
      "2278/2278 [==============================] - trainLoss: 3.7428  Val_loss: 16.5356 \n",
      "Epoch 98/100\n",
      "2278/2278 [==============================] - trainLoss: 3.7534  Val_loss: 16.4083 \n",
      "Epoch 99/100\n",
      "2278/2278 [==============================] - trainLoss: 3.7257  Val_loss: 16.5114 \n",
      "WARNING:tensorflow:No training configuration found in save file: the model was *not* compiled. Compile it manually.\n",
      "WARNING:tensorflow:No training configuration found in save file: the model was *not* compiled. Compile it manually.\n",
      "WARNING:tensorflow:No training configuration found in save file: the model was *not* compiled. Compile it manually.\n",
      "WARNING:tensorflow:No training configuration found in save file: the model was *not* compiled. Compile it manually.\n",
      "Elapsed/s:  101.75112247467041\n",
      "Final training loss:  tf.Tensor(3.7257025, shape=(), dtype=float32)\n",
      "best val loss:  tf.Tensor(16.383682, shape=(), dtype=float32)\n",
      "{0: {'parameters': {'n_neurons_classifier': 43, 'n_hidden_classifier': 1, 'learning_rate': 0.001, 'codings_size': 208, 'beta': 15, 'N': 50.0, 'n_neurons': 516, 'n_hidden': 1}, 'val_loss': <tf.Tensor: shape=(), dtype=float32, numpy=18.3234>}, 1: {'parameters': {'n_neurons_classifier': 49, 'n_hidden_classifier': 2, 'learning_rate': 0.0005, 'codings_size': 149, 'beta': 1, 'N': 50.0, 'n_neurons': 238, 'n_hidden': 1}, 'val_loss': <tf.Tensor: shape=(), dtype=float32, numpy=16.69674>}, 2: {'parameters': {'n_neurons_classifier': 94, 'n_hidden_classifier': 1, 'learning_rate': 0.0005, 'codings_size': 107, 'beta': 15, 'N': 100.0, 'n_neurons': 516, 'n_hidden': 2}, 'val_loss': <tf.Tensor: shape=(), dtype=float32, numpy=13.668679>}, 3: {'parameters': {'n_neurons_classifier': 52, 'n_hidden_classifier': 1, 'learning_rate': 0.001, 'codings_size': 208, 'beta': 10, 'N': 10.0, 'n_neurons': 320, 'n_hidden': 2}, 'val_loss': <tf.Tensor: shape=(), dtype=float32, numpy=12.411133>}, 4: {'parameters': {'n_neurons_classifier': 52, 'n_hidden_classifier': 2, 'learning_rate': 0.0005, 'codings_size': 91, 'beta': 10, 'N': 1.0, 'n_neurons': 152, 'n_hidden': 2}, 'val_loss': <tf.Tensor: shape=(), dtype=float32, numpy=12.23399>}, 5: {'parameters': {'n_neurons_classifier': 49, 'n_hidden_classifier': 1, 'learning_rate': 0.0005, 'codings_size': 126, 'beta': 1, 'N': 100.0, 'n_neurons': 238, 'n_hidden': 2}, 'val_loss': <tf.Tensor: shape=(), dtype=float32, numpy=12.280304>}, 6: {'parameters': {'n_neurons_classifier': 34, 'n_hidden_classifier': 1, 'learning_rate': 0.0005, 'codings_size': 126, 'beta': 1, 'N': 0.1, 'n_neurons': 238, 'n_hidden': 1}, 'val_loss': <tf.Tensor: shape=(), dtype=float32, numpy=16.383682>}}\n",
      "Epoch 0/100\n",
      "WARNING:tensorflow:Layer full_model_mmd is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because it's dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "2240/2278 [============================>.] - Loss for batch: 17.0571WARNING:tensorflow:Layer concatenate_1 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because it's dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2278/2278 [==============================] - trainLoss: 17.0571  Val_loss: 17.4586 \n",
      "Epoch 1/100\n",
      "2278/2278 [==============================] - trainLoss: 8.7059  Val_loss: 16.7902 \n",
      "Epoch 2/100\n",
      "2278/2278 [==============================] - trainLoss: 7.6669  Val_loss: 16.3680 \n",
      "Epoch 3/100\n",
      "2278/2278 [==============================] - trainLoss: 7.1652  Val_loss: 16.1042 \n",
      "Epoch 4/100\n",
      "2278/2278 [==============================] - trainLoss: 6.7105  Val_loss: 15.9296 \n",
      "Epoch 5/100\n",
      "2278/2278 [==============================] - trainLoss: 6.4906  Val_loss: 15.6826 \n",
      "Epoch 6/100\n",
      "2278/2278 [==============================] - trainLoss: 6.2527  Val_loss: 15.3373 \n",
      "Epoch 7/100\n",
      "2278/2278 [==============================] - trainLoss: 6.0230  Val_loss: 15.2350 \n",
      "Epoch 8/100\n",
      "2278/2278 [==============================] - trainLoss: 5.9219  Val_loss: 14.9109 \n",
      "Epoch 9/100\n",
      "2278/2278 [==============================] - trainLoss: 5.8314  Val_loss: 14.9384 \n",
      "Epoch 10/100\n",
      "2278/2278 [==============================] - trainLoss: 5.7178  Val_loss: 14.7234 \n",
      "Epoch 11/100\n",
      "2278/2278 [==============================] - trainLoss: 5.5563  Val_loss: 14.4045 \n",
      "Epoch 12/100\n",
      "2278/2278 [==============================] - trainLoss: 5.5453  Val_loss: 14.3167 \n",
      "Epoch 13/100\n",
      "2278/2278 [==============================] - trainLoss: 5.4548  Val_loss: 14.1250 \n",
      "Epoch 14/100\n",
      "2278/2278 [==============================] - trainLoss: 5.3339  Val_loss: 14.0960 \n",
      "Epoch 15/100\n",
      "2278/2278 [==============================] - trainLoss: 5.3091  Val_loss: 13.8751 \n",
      "Epoch 16/100\n",
      "2278/2278 [==============================] - trainLoss: 5.2759  Val_loss: 13.6739 \n",
      "Epoch 17/100\n",
      "2278/2278 [==============================] - trainLoss: 5.2170  Val_loss: 13.5596 \n",
      "Epoch 18/100\n",
      "2278/2278 [==============================] - trainLoss: 5.2125  Val_loss: 13.5025 \n",
      "Epoch 19/100\n",
      "2278/2278 [==============================] - trainLoss: 5.1183  Val_loss: 13.3669 \n",
      "Epoch 20/100\n",
      "2278/2278 [==============================] - trainLoss: 5.1564  Val_loss: 13.3419 \n",
      "Epoch 21/100\n",
      "2278/2278 [==============================] - trainLoss: 5.1394  Val_loss: 13.2334 \n",
      "Epoch 22/100\n",
      "2278/2278 [==============================] - trainLoss: 5.1130  Val_loss: 13.2329 \n",
      "Epoch 23/100\n",
      "2278/2278 [==============================] - trainLoss: 5.0728  Val_loss: 13.1069 \n",
      "Epoch 24/100\n",
      "2278/2278 [==============================] - trainLoss: 5.0434  Val_loss: 13.0404 \n",
      "Epoch 25/100\n",
      "2278/2278 [==============================] - trainLoss: 5.0885  Val_loss: 12.9982 \n",
      "Epoch 26/100\n",
      "2278/2278 [==============================] - trainLoss: 5.0686  Val_loss: 12.9445 \n",
      "Epoch 27/100\n",
      "2278/2278 [==============================] - trainLoss: 5.0291  Val_loss: 12.9427 \n",
      "Epoch 28/100\n",
      "2278/2278 [==============================] - trainLoss: 5.0261  Val_loss: 12.8453 \n",
      "Epoch 29/100\n",
      "2278/2278 [==============================] - trainLoss: 5.0039  Val_loss: 12.7426 \n",
      "Epoch 30/100\n",
      "2278/2278 [==============================] - trainLoss: 4.9849  Val_loss: 12.7660 \n",
      "Epoch 31/100\n",
      "2278/2278 [==============================] - trainLoss: 4.9983  Val_loss: 12.7704 \n",
      "Epoch 32/100\n",
      "2278/2278 [==============================] - trainLoss: 4.9707  Val_loss: 12.6984 \n",
      "Epoch 33/100\n",
      "2278/2278 [==============================] - trainLoss: 4.9576  Val_loss: 12.5763 \n",
      "Epoch 34/100\n",
      "2278/2278 [==============================] - trainLoss: 4.9401  Val_loss: 12.6070 \n",
      "Epoch 35/100\n",
      "2278/2278 [==============================] - trainLoss: 4.9334  Val_loss: 12.5904 \n",
      "Epoch 36/100\n",
      "2278/2278 [==============================] - trainLoss: 4.9823  Val_loss: 12.5587 \n",
      "Epoch 37/100\n",
      "2278/2278 [==============================] - trainLoss: 4.9484  Val_loss: 12.4784 \n",
      "Epoch 38/100\n",
      "2278/2278 [==============================] - trainLoss: 5.0114  Val_loss: 12.5136 \n",
      "Epoch 39/100\n",
      "2278/2278 [==============================] - trainLoss: 4.9866  Val_loss: 12.4637 \n",
      "Epoch 40/100\n",
      "2278/2278 [==============================] - trainLoss: 4.8975  Val_loss: 12.4356 \n",
      "Epoch 41/100\n",
      "2278/2278 [==============================] - trainLoss: 4.9671  Val_loss: 12.4117 \n",
      "Epoch 42/100\n",
      "2278/2278 [==============================] - trainLoss: 4.9019  Val_loss: 12.3426 \n",
      "Epoch 43/100\n",
      "2278/2278 [==============================] - trainLoss: 4.8501  Val_loss: 12.2891 \n",
      "Epoch 44/100\n",
      "2278/2278 [==============================] - trainLoss: 4.9262  Val_loss: 12.3212 \n",
      "Epoch 45/100\n",
      "2278/2278 [==============================] - trainLoss: 4.8456  Val_loss: 12.3175 \n",
      "Epoch 46/100\n",
      "2278/2278 [==============================] - trainLoss: 4.9130  Val_loss: 12.2284 \n",
      "Epoch 47/100\n",
      "2278/2278 [==============================] - trainLoss: 4.8833  Val_loss: 12.2741 \n",
      "Epoch 48/100\n",
      "2278/2278 [==============================] - trainLoss: 4.9288  Val_loss: 12.2256 \n",
      "Epoch 49/100\n",
      "2278/2278 [==============================] - trainLoss: 4.8587  Val_loss: 12.1953 \n",
      "Epoch 50/100\n",
      "2278/2278 [==============================] - trainLoss: 4.8417  Val_loss: 12.2890 \n",
      "Epoch 51/100\n",
      "2278/2278 [==============================] - trainLoss: 4.9140  Val_loss: 12.1898 \n",
      "Epoch 52/100\n",
      "2278/2278 [==============================] - trainLoss: 4.8361  Val_loss: 12.2585 \n",
      "Epoch 53/100\n",
      "2278/2278 [==============================] - trainLoss: 4.9166  Val_loss: 12.2180 \n",
      "Epoch 54/100\n",
      "2278/2278 [==============================] - trainLoss: 4.9573  Val_loss: 12.1864 \n",
      "Epoch 55/100\n",
      "2278/2278 [==============================] - trainLoss: 4.8795  Val_loss: 12.1206 \n",
      "Epoch 56/100\n",
      "2278/2278 [==============================] - trainLoss: 4.8500  Val_loss: 12.2115 \n",
      "Epoch 57/100\n",
      "2278/2278 [==============================] - trainLoss: 4.8375  Val_loss: 12.1020 \n",
      "Epoch 58/100\n",
      "2278/2278 [==============================] - trainLoss: 4.8085  Val_loss: 12.1380 \n",
      "Epoch 59/100\n",
      "2278/2278 [==============================] - trainLoss: 4.8106  Val_loss: 12.1528 \n",
      "Epoch 60/100\n",
      "2278/2278 [==============================] - trainLoss: 4.7690  Val_loss: 12.1782 \n",
      "Epoch 61/100\n",
      "2278/2278 [==============================] - trainLoss: 4.8070  Val_loss: 12.0964 \n",
      "Epoch 62/100\n",
      "2278/2278 [==============================] - trainLoss: 4.7957  Val_loss: 12.1620 \n",
      "Epoch 63/100\n",
      "2278/2278 [==============================] - trainLoss: 4.7863  Val_loss: 12.0786 \n",
      "Epoch 64/100\n",
      "2278/2278 [==============================] - trainLoss: 4.9021  Val_loss: 12.0770 \n",
      "Epoch 65/100\n",
      "2278/2278 [==============================] - trainLoss: 4.7628  Val_loss: 12.1046 \n",
      "Epoch 66/100\n",
      "2278/2278 [==============================] - trainLoss: 4.7601  Val_loss: 12.0805 \n",
      "Epoch 67/100\n",
      "2278/2278 [==============================] - trainLoss: 4.7593  Val_loss: 12.0463 \n",
      "Epoch 68/100\n",
      "2278/2278 [==============================] - trainLoss: 4.8007  Val_loss: 12.0227 \n",
      "Epoch 69/100\n",
      "2278/2278 [==============================] - trainLoss: 4.7275  Val_loss: 12.0616 \n",
      "Epoch 70/100\n",
      "2278/2278 [==============================] - trainLoss: 4.8015  Val_loss: 12.0944 \n",
      "Epoch 71/100\n",
      "2278/2278 [==============================] - trainLoss: 4.7321  Val_loss: 12.0362 \n",
      "Epoch 72/100\n",
      "2278/2278 [==============================] - trainLoss: 4.7194  Val_loss: 12.0213 \n",
      "Epoch 73/100\n",
      "2278/2278 [==============================] - trainLoss: 4.8223  Val_loss: 12.0979 \n",
      "Epoch 74/100\n",
      "2278/2278 [==============================] - trainLoss: 4.7777  Val_loss: 12.1662 \n",
      "Epoch 75/100\n",
      "2278/2278 [==============================] - trainLoss: 4.7228  Val_loss: 12.1304 \n",
      "Epoch 76/100\n",
      "2278/2278 [==============================] - trainLoss: 4.7625  Val_loss: 12.0591 \n",
      "Epoch 77/100\n",
      "2278/2278 [==============================] - trainLoss: 4.7031  Val_loss: 12.1204 \n",
      "Epoch 78/100\n",
      "2278/2278 [==============================] - trainLoss: 4.7353  Val_loss: 12.1328 \n",
      "Epoch 79/100\n",
      "2278/2278 [==============================] - trainLoss: 4.6820  Val_loss: 12.0932 \n",
      "Epoch 80/100\n",
      "2278/2278 [==============================] - trainLoss: 4.7080  Val_loss: 12.1099 \n",
      "Epoch 81/100\n",
      "2278/2278 [==============================] - trainLoss: 4.7211  Val_loss: 12.1357 \n",
      "WARNING:tensorflow:No training configuration found in save file: the model was *not* compiled. Compile it manually.\n",
      "WARNING:tensorflow:No training configuration found in save file: the model was *not* compiled. Compile it manually.\n",
      "WARNING:tensorflow:No training configuration found in save file: the model was *not* compiled. Compile it manually.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:No training configuration found in save file: the model was *not* compiled. Compile it manually.\n",
      "Elapsed/s:  103.80203437805176\n",
      "Final training loss:  tf.Tensor(4.721092, shape=(), dtype=float32)\n",
      "best val loss:  tf.Tensor(12.021252, shape=(), dtype=float32)\n",
      "{0: {'parameters': {'n_neurons_classifier': 43, 'n_hidden_classifier': 1, 'learning_rate': 0.001, 'codings_size': 208, 'beta': 15, 'N': 50.0, 'n_neurons': 516, 'n_hidden': 1}, 'val_loss': <tf.Tensor: shape=(), dtype=float32, numpy=18.3234>}, 1: {'parameters': {'n_neurons_classifier': 49, 'n_hidden_classifier': 2, 'learning_rate': 0.0005, 'codings_size': 149, 'beta': 1, 'N': 50.0, 'n_neurons': 238, 'n_hidden': 1}, 'val_loss': <tf.Tensor: shape=(), dtype=float32, numpy=16.69674>}, 2: {'parameters': {'n_neurons_classifier': 94, 'n_hidden_classifier': 1, 'learning_rate': 0.0005, 'codings_size': 107, 'beta': 15, 'N': 100.0, 'n_neurons': 516, 'n_hidden': 2}, 'val_loss': <tf.Tensor: shape=(), dtype=float32, numpy=13.668679>}, 3: {'parameters': {'n_neurons_classifier': 52, 'n_hidden_classifier': 1, 'learning_rate': 0.001, 'codings_size': 208, 'beta': 10, 'N': 10.0, 'n_neurons': 320, 'n_hidden': 2}, 'val_loss': <tf.Tensor: shape=(), dtype=float32, numpy=12.411133>}, 4: {'parameters': {'n_neurons_classifier': 52, 'n_hidden_classifier': 2, 'learning_rate': 0.0005, 'codings_size': 91, 'beta': 10, 'N': 1.0, 'n_neurons': 152, 'n_hidden': 2}, 'val_loss': <tf.Tensor: shape=(), dtype=float32, numpy=12.23399>}, 5: {'parameters': {'n_neurons_classifier': 49, 'n_hidden_classifier': 1, 'learning_rate': 0.0005, 'codings_size': 126, 'beta': 1, 'N': 100.0, 'n_neurons': 238, 'n_hidden': 2}, 'val_loss': <tf.Tensor: shape=(), dtype=float32, numpy=12.280304>}, 6: {'parameters': {'n_neurons_classifier': 34, 'n_hidden_classifier': 1, 'learning_rate': 0.0005, 'codings_size': 126, 'beta': 1, 'N': 0.1, 'n_neurons': 238, 'n_hidden': 1}, 'val_loss': <tf.Tensor: shape=(), dtype=float32, numpy=16.383682>}, 7: {'parameters': {'n_neurons_classifier': 79, 'n_hidden_classifier': 2, 'learning_rate': 0.001, 'codings_size': 255, 'beta': 10, 'N': 1.0, 'n_neurons': 137, 'n_hidden': 2}, 'val_loss': <tf.Tensor: shape=(), dtype=float32, numpy=12.021252>}}\n",
      "Epoch 0/100\n",
      "WARNING:tensorflow:Layer full_model_mmd is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because it's dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "2240/2278 [============================>.] - Loss for batch: 24.0696WARNING:tensorflow:Layer concatenate_1 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because it's dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "2278/2278 [==============================] - trainLoss: 24.0696  Val_loss: 18.8715 \n",
      "Epoch 1/100\n",
      "2278/2278 [==============================] - trainLoss: 13.3992  Val_loss: 17.7529 \n",
      "Epoch 2/100\n",
      "2278/2278 [==============================] - trainLoss: 11.1516  Val_loss: 17.0047 \n",
      "Epoch 3/100\n",
      "2278/2278 [==============================] - trainLoss: 10.3563  Val_loss: 16.7911 \n",
      "Epoch 4/100\n",
      "2278/2278 [==============================] - trainLoss: 9.6713  Val_loss: 16.4466 \n",
      "Epoch 5/100\n",
      "2278/2278 [==============================] - trainLoss: 9.4461  Val_loss: 16.0746 \n",
      "Epoch 6/100\n",
      "2278/2278 [==============================] - trainLoss: 9.1095  Val_loss: 15.8939 \n",
      "Epoch 7/100\n",
      "2278/2278 [==============================] - trainLoss: 8.8670  Val_loss: 15.7125 \n",
      "Epoch 8/100\n",
      "2278/2278 [==============================] - trainLoss: 8.6935  Val_loss: 15.4825 \n",
      "Epoch 9/100\n",
      "2278/2278 [==============================] - trainLoss: 8.4704  Val_loss: 15.3040 \n",
      "Epoch 10/100\n",
      "2278/2278 [==============================] - trainLoss: 8.3374  Val_loss: 15.1112 \n",
      "Epoch 11/100\n",
      "2278/2278 [==============================] - trainLoss: 8.1312  Val_loss: 14.9014 \n",
      "Epoch 12/100\n",
      "2278/2278 [==============================] - trainLoss: 7.9975  Val_loss: 14.7259 \n",
      "Epoch 13/100\n",
      "2278/2278 [==============================] - trainLoss: 7.8593  Val_loss: 14.5745 \n",
      "Epoch 14/100\n",
      "2278/2278 [==============================] - trainLoss: 7.6936  Val_loss: 14.3635 \n",
      "Epoch 15/100\n",
      "2278/2278 [==============================] - trainLoss: 7.7020  Val_loss: 14.2323 \n",
      "Epoch 16/100\n",
      "2278/2278 [==============================] - trainLoss: 7.5458  Val_loss: 14.2092 \n",
      "Epoch 17/100\n",
      "2278/2278 [==============================] - trainLoss: 7.4013  Val_loss: 14.1067 \n",
      "Epoch 18/100\n",
      "2278/2278 [==============================] - trainLoss: 7.3326  Val_loss: 13.9971 \n",
      "Epoch 19/100\n",
      "2278/2278 [==============================] - trainLoss: 7.3706  Val_loss: 13.8889 \n",
      "Epoch 20/100\n",
      "2278/2278 [==============================] - trainLoss: 7.2902  Val_loss: 13.7029 \n",
      "Epoch 21/100\n",
      "2278/2278 [==============================] - trainLoss: 7.1717  Val_loss: 13.6294 \n",
      "Epoch 22/100\n",
      "2278/2278 [==============================] - trainLoss: 7.1800  Val_loss: 13.5614 \n",
      "Epoch 23/100\n",
      "2278/2278 [==============================] - trainLoss: 7.0784  Val_loss: 13.5373 \n",
      "Epoch 24/100\n",
      "2278/2278 [==============================] - trainLoss: 7.1053  Val_loss: 13.2753 \n",
      "Epoch 25/100\n",
      "2278/2278 [==============================] - trainLoss: 7.0781  Val_loss: 13.2601 \n",
      "Epoch 26/100\n",
      "2278/2278 [==============================] - trainLoss: 7.0045  Val_loss: 13.2001 \n",
      "Epoch 27/100\n",
      "2278/2278 [==============================] - trainLoss: 6.8840  Val_loss: 13.1628 \n",
      "Epoch 28/100\n",
      "2278/2278 [==============================] - trainLoss: 6.9453  Val_loss: 13.0449 \n",
      "Epoch 29/100\n",
      "2278/2278 [==============================] - trainLoss: 6.9655  Val_loss: 13.0671 \n",
      "Epoch 30/100\n",
      "2278/2278 [==============================] - trainLoss: 6.8276  Val_loss: 13.0281 \n",
      "Epoch 31/100\n",
      "2278/2278 [==============================] - trainLoss: 6.7623  Val_loss: 12.9790 \n",
      "Epoch 32/100\n",
      "2278/2278 [==============================] - trainLoss: 6.8870  Val_loss: 12.8956 \n",
      "Epoch 33/100\n",
      "2278/2278 [==============================] - trainLoss: 6.7860  Val_loss: 12.8344 \n",
      "Epoch 34/100\n",
      "2278/2278 [==============================] - trainLoss: 6.7811  Val_loss: 12.8383 \n",
      "Epoch 35/100\n",
      "2278/2278 [==============================] - trainLoss: 6.7094  Val_loss: 12.7756 \n",
      "Epoch 36/100\n",
      "2278/2278 [==============================] - trainLoss: 6.6502  Val_loss: 12.7174 \n",
      "Epoch 37/100\n",
      "2278/2278 [==============================] - trainLoss: 6.7408  Val_loss: 12.6938 \n",
      "Epoch 38/100\n",
      "2278/2278 [==============================] - trainLoss: 6.7097  Val_loss: 12.6656 \n",
      "Epoch 39/100\n",
      "2278/2278 [==============================] - trainLoss: 6.6419  Val_loss: 12.7124 \n",
      "Epoch 40/100\n",
      "2278/2278 [==============================] - trainLoss: 6.5749  Val_loss: 12.5921 \n",
      "Epoch 41/100\n",
      "2278/2278 [==============================] - trainLoss: 6.6503  Val_loss: 12.6055 \n",
      "Epoch 42/100\n",
      "2278/2278 [==============================] - trainLoss: 6.5745  Val_loss: 12.5844 \n",
      "Epoch 43/100\n",
      "2278/2278 [==============================] - trainLoss: 6.5988  Val_loss: 12.5855 \n",
      "Epoch 44/100\n",
      "2278/2278 [==============================] - trainLoss: 6.5639  Val_loss: 12.5938 \n",
      "Epoch 45/100\n",
      "2278/2278 [==============================] - trainLoss: 6.5014  Val_loss: 12.5488 \n",
      "Epoch 46/100\n",
      "2278/2278 [==============================] - trainLoss: 6.5316  Val_loss: 12.5015 \n",
      "Epoch 47/100\n",
      "2278/2278 [==============================] - trainLoss: 6.4516  Val_loss: 12.4064 \n",
      "Epoch 48/100\n",
      "2278/2278 [==============================] - trainLoss: 6.5266  Val_loss: 12.4542 \n",
      "Epoch 49/100\n",
      "2278/2278 [==============================] - trainLoss: 6.5404  Val_loss: 12.4770 \n",
      "Epoch 50/100\n",
      "2278/2278 [==============================] - trainLoss: 6.4352  Val_loss: 12.3910 \n",
      "Epoch 51/100\n",
      "2278/2278 [==============================] - trainLoss: 6.4922  Val_loss: 12.4160 \n",
      "Epoch 52/100\n",
      "2278/2278 [==============================] - trainLoss: 6.4221  Val_loss: 12.3834 \n",
      "Epoch 53/100\n",
      "2278/2278 [==============================] - trainLoss: 6.3616  Val_loss: 12.4060 \n",
      "Epoch 54/100\n",
      "2278/2278 [==============================] - trainLoss: 6.3922  Val_loss: 12.3831 \n",
      "Epoch 55/100\n",
      "2278/2278 [==============================] - trainLoss: 6.3839  Val_loss: 12.3261 \n",
      "Epoch 56/100\n",
      "2278/2278 [==============================] - trainLoss: 6.4612  Val_loss: 12.3560 \n",
      "Epoch 57/100\n",
      "2278/2278 [==============================] - trainLoss: 6.4142  Val_loss: 12.3550 \n",
      "Epoch 58/100\n",
      "2278/2278 [==============================] - trainLoss: 6.3677  Val_loss: 12.2847 \n",
      "Epoch 59/100\n",
      "2278/2278 [==============================] - trainLoss: 6.3491  Val_loss: 12.3492 \n",
      "Epoch 60/100\n",
      "2278/2278 [==============================] - trainLoss: 6.4065  Val_loss: 12.3534 \n",
      "Epoch 61/100\n",
      "2278/2278 [==============================] - trainLoss: 6.3185  Val_loss: 12.3138 \n",
      "Epoch 62/100\n",
      "2278/2278 [==============================] - trainLoss: 6.3191  Val_loss: 12.2827 \n",
      "Epoch 63/100\n",
      "2278/2278 [==============================] - trainLoss: 6.2565  Val_loss: 12.2744 \n",
      "Epoch 64/100\n",
      "2278/2278 [==============================] - trainLoss: 6.1833  Val_loss: 12.2812 \n",
      "Epoch 65/100\n",
      "2278/2278 [==============================] - trainLoss: 6.3226  Val_loss: 12.3120 \n",
      "Epoch 66/100\n",
      "2278/2278 [==============================] - trainLoss: 6.3208  Val_loss: 12.2283 \n",
      "Epoch 67/100\n",
      "2278/2278 [==============================] - trainLoss: 6.2706  Val_loss: 12.2615 \n",
      "Epoch 68/100\n",
      "2278/2278 [==============================] - trainLoss: 6.2100  Val_loss: 12.2516 \n",
      "Epoch 69/100\n",
      "2278/2278 [==============================] - trainLoss: 6.2284  Val_loss: 12.2568 \n",
      "Epoch 70/100\n",
      "2278/2278 [==============================] - trainLoss: 6.2516  Val_loss: 12.2185 \n",
      "Epoch 71/100\n",
      "2278/2278 [==============================] - trainLoss: 6.2155  Val_loss: 12.2786 \n",
      "Epoch 72/100\n",
      "2278/2278 [==============================] - trainLoss: 6.1727  Val_loss: 12.2451 \n",
      "Epoch 73/100\n",
      "2278/2278 [==============================] - trainLoss: 6.1250  Val_loss: 12.2370 \n",
      "Epoch 74/100\n",
      "2278/2278 [==============================] - trainLoss: 6.1178  Val_loss: 12.2082 \n",
      "Epoch 75/100\n",
      "2278/2278 [==============================] - trainLoss: 6.0966  Val_loss: 12.2375 \n",
      "Epoch 76/100\n",
      "2278/2278 [==============================] - trainLoss: 6.2042  Val_loss: 12.2290 \n",
      "Epoch 77/100\n",
      "2278/2278 [==============================] - trainLoss: 6.1369  Val_loss: 12.2211 \n",
      "Epoch 78/100\n",
      "2278/2278 [==============================] - trainLoss: 6.2255  Val_loss: 12.2352 \n",
      "Epoch 79/100\n",
      "2278/2278 [==============================] - trainLoss: 6.0405  Val_loss: 12.2329 \n",
      "Epoch 80/100\n",
      "2278/2278 [==============================] - trainLoss: 6.1378  Val_loss: 12.3145 \n",
      "Epoch 81/100\n",
      "2278/2278 [==============================] - trainLoss: 6.0716  Val_loss: 12.2477 \n",
      "Epoch 82/100\n",
      "2278/2278 [==============================] - trainLoss: 6.1402  Val_loss: 12.2259 \n",
      "Epoch 83/100\n",
      "2278/2278 [==============================] - trainLoss: 6.0635  Val_loss: 12.2284 \n",
      "WARNING:tensorflow:No training configuration found in save file: the model was *not* compiled. Compile it manually.\n",
      "WARNING:tensorflow:No training configuration found in save file: the model was *not* compiled. Compile it manually.\n",
      "WARNING:tensorflow:No training configuration found in save file: the model was *not* compiled. Compile it manually.\n",
      "WARNING:tensorflow:No training configuration found in save file: the model was *not* compiled. Compile it manually.\n",
      "Elapsed/s:  101.79318261146545\n",
      "Final training loss:  tf.Tensor(6.063508, shape=(), dtype=float32)\n",
      "best val loss:  tf.Tensor(12.208234, shape=(), dtype=float32)\n",
      "{0: {'parameters': {'n_neurons_classifier': 43, 'n_hidden_classifier': 1, 'learning_rate': 0.001, 'codings_size': 208, 'beta': 15, 'N': 50.0, 'n_neurons': 516, 'n_hidden': 1}, 'val_loss': <tf.Tensor: shape=(), dtype=float32, numpy=18.3234>}, 1: {'parameters': {'n_neurons_classifier': 49, 'n_hidden_classifier': 2, 'learning_rate': 0.0005, 'codings_size': 149, 'beta': 1, 'N': 50.0, 'n_neurons': 238, 'n_hidden': 1}, 'val_loss': <tf.Tensor: shape=(), dtype=float32, numpy=16.69674>}, 2: {'parameters': {'n_neurons_classifier': 94, 'n_hidden_classifier': 1, 'learning_rate': 0.0005, 'codings_size': 107, 'beta': 15, 'N': 100.0, 'n_neurons': 516, 'n_hidden': 2}, 'val_loss': <tf.Tensor: shape=(), dtype=float32, numpy=13.668679>}, 3: {'parameters': {'n_neurons_classifier': 52, 'n_hidden_classifier': 1, 'learning_rate': 0.001, 'codings_size': 208, 'beta': 10, 'N': 10.0, 'n_neurons': 320, 'n_hidden': 2}, 'val_loss': <tf.Tensor: shape=(), dtype=float32, numpy=12.411133>}, 4: {'parameters': {'n_neurons_classifier': 52, 'n_hidden_classifier': 2, 'learning_rate': 0.0005, 'codings_size': 91, 'beta': 10, 'N': 1.0, 'n_neurons': 152, 'n_hidden': 2}, 'val_loss': <tf.Tensor: shape=(), dtype=float32, numpy=12.23399>}, 5: {'parameters': {'n_neurons_classifier': 49, 'n_hidden_classifier': 1, 'learning_rate': 0.0005, 'codings_size': 126, 'beta': 1, 'N': 100.0, 'n_neurons': 238, 'n_hidden': 2}, 'val_loss': <tf.Tensor: shape=(), dtype=float32, numpy=12.280304>}, 6: {'parameters': {'n_neurons_classifier': 34, 'n_hidden_classifier': 1, 'learning_rate': 0.0005, 'codings_size': 126, 'beta': 1, 'N': 0.1, 'n_neurons': 238, 'n_hidden': 1}, 'val_loss': <tf.Tensor: shape=(), dtype=float32, numpy=16.383682>}, 7: {'parameters': {'n_neurons_classifier': 79, 'n_hidden_classifier': 2, 'learning_rate': 0.001, 'codings_size': 255, 'beta': 10, 'N': 1.0, 'n_neurons': 137, 'n_hidden': 2}, 'val_loss': <tf.Tensor: shape=(), dtype=float32, numpy=12.021252>}, 8: {'parameters': {'n_neurons_classifier': 41, 'n_hidden_classifier': 2, 'learning_rate': 0.0005, 'codings_size': 123, 'beta': 10, 'N': 10.0, 'n_neurons': 156, 'n_hidden': 2}, 'val_loss': <tf.Tensor: shape=(), dtype=float32, numpy=12.208234>}}\n",
      "Epoch 0/100\n",
      "WARNING:tensorflow:Layer full_model_mmd is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because it's dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "2240/2278 [============================>.] - Loss for batch: 25.6073WARNING:tensorflow:Layer concatenate_1 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because it's dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "2278/2278 [==============================] - trainLoss: 25.6073  Val_loss: 15.6608 \n",
      "Epoch 1/100\n",
      "2278/2278 [==============================] - trainLoss: 14.6476  Val_loss: 13.5911 \n",
      "Epoch 2/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2278/2278 [==============================] - trainLoss: 12.1991  Val_loss: 13.0545 \n",
      "Epoch 3/100\n",
      "2278/2278 [==============================] - trainLoss: 11.0594  Val_loss: 12.7152 \n",
      "Epoch 4/100\n",
      "2278/2278 [==============================] - trainLoss: 10.1616  Val_loss: 12.6317 \n",
      "Epoch 5/100\n",
      "2278/2278 [==============================] - trainLoss: 9.6852  Val_loss: 12.4788 \n",
      "Epoch 6/100\n",
      "2278/2278 [==============================] - trainLoss: 9.2057  Val_loss: 12.4202 \n",
      "Epoch 7/100\n",
      "2278/2278 [==============================] - trainLoss: 8.9819  Val_loss: 12.4102 \n",
      "Epoch 8/100\n",
      "2278/2278 [==============================] - trainLoss: 8.7965  Val_loss: 12.3561 \n",
      "Epoch 9/100\n",
      "2278/2278 [==============================] - trainLoss: 8.6333  Val_loss: 12.3712 \n",
      "Epoch 10/100\n",
      "2278/2278 [==============================] - trainLoss: 8.5265  Val_loss: 12.3519 \n",
      "Epoch 11/100\n",
      "2278/2278 [==============================] - trainLoss: 8.4786  Val_loss: 12.3036 \n",
      "Epoch 12/100\n",
      "2278/2278 [==============================] - trainLoss: 8.4259  Val_loss: 12.2539 \n",
      "Epoch 13/100\n",
      "2278/2278 [==============================] - trainLoss: 8.2536  Val_loss: 12.2166 \n",
      "Epoch 14/100\n",
      "2278/2278 [==============================] - trainLoss: 8.1054  Val_loss: 12.2226 \n",
      "Epoch 15/100\n",
      "2278/2278 [==============================] - trainLoss: 8.1948  Val_loss: 12.2662 \n",
      "Epoch 16/100\n",
      "2278/2278 [==============================] - trainLoss: 8.0494  Val_loss: 12.2702 \n",
      "Epoch 17/100\n",
      "2278/2278 [==============================] - trainLoss: 7.9458  Val_loss: 12.2617 \n",
      "Epoch 18/100\n",
      "2278/2278 [==============================] - trainLoss: 7.9260  Val_loss: 12.1659 \n",
      "Epoch 19/100\n",
      "2278/2278 [==============================] - trainLoss: 7.8541  Val_loss: 12.2902 \n",
      "Epoch 20/100\n",
      "2278/2278 [==============================] - trainLoss: 7.8316  Val_loss: 12.2119 \n",
      "Epoch 21/100\n",
      "2278/2278 [==============================] - trainLoss: 7.7918  Val_loss: 12.2556 \n",
      "Epoch 22/100\n",
      "2278/2278 [==============================] - trainLoss: 7.7592  Val_loss: 12.1958 \n",
      "Epoch 23/100\n",
      "2278/2278 [==============================] - trainLoss: 7.6268  Val_loss: 12.2188 \n",
      "Epoch 24/100\n",
      "2278/2278 [==============================] - trainLoss: 7.6559  Val_loss: 12.2164 \n",
      "Epoch 25/100\n",
      "2278/2278 [==============================] - trainLoss: 7.4779  Val_loss: 12.2714 \n",
      "Epoch 26/100\n",
      "2278/2278 [==============================] - trainLoss: 7.5417  Val_loss: 12.2824 \n",
      "Epoch 27/100\n",
      "2278/2278 [==============================] - trainLoss: 7.5873  Val_loss: 12.2459 \n",
      "WARNING:tensorflow:No training configuration found in save file: the model was *not* compiled. Compile it manually.\n",
      "WARNING:tensorflow:No training configuration found in save file: the model was *not* compiled. Compile it manually.\n",
      "WARNING:tensorflow:No training configuration found in save file: the model was *not* compiled. Compile it manually.\n",
      "WARNING:tensorflow:No training configuration found in save file: the model was *not* compiled. Compile it manually.\n",
      "Elapsed/s:  32.17266249656677\n",
      "Final training loss:  tf.Tensor(7.5872684, shape=(), dtype=float32)\n",
      "best val loss:  tf.Tensor(12.16589, shape=(), dtype=float32)\n",
      "{0: {'parameters': {'n_neurons_classifier': 43, 'n_hidden_classifier': 1, 'learning_rate': 0.001, 'codings_size': 208, 'beta': 15, 'N': 50.0, 'n_neurons': 516, 'n_hidden': 1}, 'val_loss': <tf.Tensor: shape=(), dtype=float32, numpy=18.3234>}, 1: {'parameters': {'n_neurons_classifier': 49, 'n_hidden_classifier': 2, 'learning_rate': 0.0005, 'codings_size': 149, 'beta': 1, 'N': 50.0, 'n_neurons': 238, 'n_hidden': 1}, 'val_loss': <tf.Tensor: shape=(), dtype=float32, numpy=16.69674>}, 2: {'parameters': {'n_neurons_classifier': 94, 'n_hidden_classifier': 1, 'learning_rate': 0.0005, 'codings_size': 107, 'beta': 15, 'N': 100.0, 'n_neurons': 516, 'n_hidden': 2}, 'val_loss': <tf.Tensor: shape=(), dtype=float32, numpy=13.668679>}, 3: {'parameters': {'n_neurons_classifier': 52, 'n_hidden_classifier': 1, 'learning_rate': 0.001, 'codings_size': 208, 'beta': 10, 'N': 10.0, 'n_neurons': 320, 'n_hidden': 2}, 'val_loss': <tf.Tensor: shape=(), dtype=float32, numpy=12.411133>}, 4: {'parameters': {'n_neurons_classifier': 52, 'n_hidden_classifier': 2, 'learning_rate': 0.0005, 'codings_size': 91, 'beta': 10, 'N': 1.0, 'n_neurons': 152, 'n_hidden': 2}, 'val_loss': <tf.Tensor: shape=(), dtype=float32, numpy=12.23399>}, 5: {'parameters': {'n_neurons_classifier': 49, 'n_hidden_classifier': 1, 'learning_rate': 0.0005, 'codings_size': 126, 'beta': 1, 'N': 100.0, 'n_neurons': 238, 'n_hidden': 2}, 'val_loss': <tf.Tensor: shape=(), dtype=float32, numpy=12.280304>}, 6: {'parameters': {'n_neurons_classifier': 34, 'n_hidden_classifier': 1, 'learning_rate': 0.0005, 'codings_size': 126, 'beta': 1, 'N': 0.1, 'n_neurons': 238, 'n_hidden': 1}, 'val_loss': <tf.Tensor: shape=(), dtype=float32, numpy=16.383682>}, 7: {'parameters': {'n_neurons_classifier': 79, 'n_hidden_classifier': 2, 'learning_rate': 0.001, 'codings_size': 255, 'beta': 10, 'N': 1.0, 'n_neurons': 137, 'n_hidden': 2}, 'val_loss': <tf.Tensor: shape=(), dtype=float32, numpy=12.021252>}, 8: {'parameters': {'n_neurons_classifier': 41, 'n_hidden_classifier': 2, 'learning_rate': 0.0005, 'codings_size': 123, 'beta': 10, 'N': 10.0, 'n_neurons': 156, 'n_hidden': 2}, 'val_loss': <tf.Tensor: shape=(), dtype=float32, numpy=12.208234>}, 9: {'parameters': {'n_neurons_classifier': 21, 'n_hidden_classifier': 1, 'learning_rate': 0.0005, 'codings_size': 21, 'beta': 10, 'N': 10.0, 'n_neurons': 70, 'n_hidden': 2}, 'val_loss': <tf.Tensor: shape=(), dtype=float32, numpy=12.16589>}}\n",
      "WARNING:tensorflow:No training configuration found in save file: the model was *not* compiled. Compile it manually.\n",
      "WARNING:tensorflow:No training configuration found in save file: the model was *not* compiled. Compile it manually.\n",
      "WARNING:tensorflow:No training configuration found in save file: the model was *not* compiled. Compile it manually.\n",
      "WARNING:tensorflow:No training configuration found in save file: the model was *not* compiled. Compile it manually.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[(7,\n",
       "  {'parameters': {'N': 1.0,\n",
       "    'beta': 10,\n",
       "    'codings_size': 255,\n",
       "    'learning_rate': 0.001,\n",
       "    'n_hidden': 2,\n",
       "    'n_hidden_classifier': 2,\n",
       "    'n_neurons': 137,\n",
       "    'n_neurons_classifier': 79},\n",
       "   'val_loss': <tf.Tensor: shape=(), dtype=float32, numpy=12.021252>}),\n",
       " (9,\n",
       "  {'parameters': {'N': 10.0,\n",
       "    'beta': 10,\n",
       "    'codings_size': 21,\n",
       "    'learning_rate': 0.0005,\n",
       "    'n_hidden': 2,\n",
       "    'n_hidden_classifier': 1,\n",
       "    'n_neurons': 70,\n",
       "    'n_neurons_classifier': 21},\n",
       "   'val_loss': <tf.Tensor: shape=(), dtype=float32, numpy=12.16589>}),\n",
       " (8,\n",
       "  {'parameters': {'N': 10.0,\n",
       "    'beta': 10,\n",
       "    'codings_size': 123,\n",
       "    'learning_rate': 0.0005,\n",
       "    'n_hidden': 2,\n",
       "    'n_hidden_classifier': 2,\n",
       "    'n_neurons': 156,\n",
       "    'n_neurons_classifier': 41},\n",
       "   'val_loss': <tf.Tensor: shape=(), dtype=float32, numpy=12.208234>}),\n",
       " (4,\n",
       "  {'parameters': {'N': 1.0,\n",
       "    'beta': 10,\n",
       "    'codings_size': 91,\n",
       "    'learning_rate': 0.0005,\n",
       "    'n_hidden': 2,\n",
       "    'n_hidden_classifier': 2,\n",
       "    'n_neurons': 152,\n",
       "    'n_neurons_classifier': 52},\n",
       "   'val_loss': <tf.Tensor: shape=(), dtype=float32, numpy=12.23399>}),\n",
       " (5,\n",
       "  {'parameters': {'N': 100.0,\n",
       "    'beta': 1,\n",
       "    'codings_size': 126,\n",
       "    'learning_rate': 0.0005,\n",
       "    'n_hidden': 2,\n",
       "    'n_hidden_classifier': 1,\n",
       "    'n_neurons': 238,\n",
       "    'n_neurons_classifier': 49},\n",
       "   'val_loss': <tf.Tensor: shape=(), dtype=float32, numpy=12.280304>}),\n",
       " (3,\n",
       "  {'parameters': {'N': 10.0,\n",
       "    'beta': 10,\n",
       "    'codings_size': 208,\n",
       "    'learning_rate': 0.001,\n",
       "    'n_hidden': 2,\n",
       "    'n_hidden_classifier': 1,\n",
       "    'n_neurons': 320,\n",
       "    'n_neurons_classifier': 52},\n",
       "   'val_loss': <tf.Tensor: shape=(), dtype=float32, numpy=12.411133>}),\n",
       " (2,\n",
       "  {'parameters': {'N': 100.0,\n",
       "    'beta': 15,\n",
       "    'codings_size': 107,\n",
       "    'learning_rate': 0.0005,\n",
       "    'n_hidden': 2,\n",
       "    'n_hidden_classifier': 1,\n",
       "    'n_neurons': 516,\n",
       "    'n_neurons_classifier': 94},\n",
       "   'val_loss': <tf.Tensor: shape=(), dtype=float32, numpy=13.668679>}),\n",
       " (6,\n",
       "  {'parameters': {'N': 0.1,\n",
       "    'beta': 1,\n",
       "    'codings_size': 126,\n",
       "    'learning_rate': 0.0005,\n",
       "    'n_hidden': 1,\n",
       "    'n_hidden_classifier': 1,\n",
       "    'n_neurons': 238,\n",
       "    'n_neurons_classifier': 34},\n",
       "   'val_loss': <tf.Tensor: shape=(), dtype=float32, numpy=16.383682>}),\n",
       " (1,\n",
       "  {'parameters': {'N': 50.0,\n",
       "    'beta': 1,\n",
       "    'codings_size': 149,\n",
       "    'learning_rate': 0.0005,\n",
       "    'n_hidden': 1,\n",
       "    'n_hidden_classifier': 2,\n",
       "    'n_neurons': 238,\n",
       "    'n_neurons_classifier': 49},\n",
       "   'val_loss': <tf.Tensor: shape=(), dtype=float32, numpy=16.69674>}),\n",
       " (0,\n",
       "  {'parameters': {'N': 50.0,\n",
       "    'beta': 15,\n",
       "    'codings_size': 208,\n",
       "    'learning_rate': 0.001,\n",
       "    'n_hidden': 1,\n",
       "    'n_hidden_classifier': 1,\n",
       "    'n_neurons': 516,\n",
       "    'n_neurons_classifier': 43},\n",
       "   'val_loss': <tf.Tensor: shape=(), dtype=float32, numpy=18.3234>})]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result,variational_encoder,variational_decoder,classifier,y_distribution = hyperparameter_search_mmd(param_distribs=param_distribs,\n",
    "                                                                        epochs=100,patience=10,n_iter=10)\n",
    "result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Best MMD-based model has val nloglik of 11.8473425. 61 epochs of training.\n",
    "\n",
    "'parameters': {'N': 0.1,\n",
    "    'beta': 1,\n",
    "    'codings_size': 119,\n",
    "    'learning_rate': 0.001,\n",
    "    'n_hidden': 2,\n",
    "    'n_hidden_classifier': 2,\n",
    "    'n_neurons': 137,\n",
    "    'n_neurons_classifier': 80},\n",
    "   'val_loss': <tf.Tensor: shape=(), dtype=float32, numpy=11.922641>}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Single run # "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "variational_encoder,variational_decoder,classifier,y_distribution,model = build_model_mmd(n_hidden=2, n_neurons=137,input_shape=input_shape,beta=1,n_hidden_classifier=2,\n",
    "              n_neurons_classifier=137,N=0.1,codings_size=119)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0/100\n",
      "WARNING:tensorflow:Layer full_model_mmd is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because it's dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "2240/2278 [============================>.] - Loss for batch: 17.4197WARNING:tensorflow:Layer concatenate_1 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because it's dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "2278/2278 [==============================] - trainLoss: 17.4197  Val_loss: 16.2852 \n",
      "Epoch 1/100\n",
      "2278/2278 [==============================] - trainLoss: 8.5395  Val_loss: 15.6127 \n",
      "Epoch 2/100\n",
      "2278/2278 [==============================] - trainLoss: 7.3899  Val_loss: 15.3383 \n",
      "Epoch 3/100\n",
      "2278/2278 [==============================] - trainLoss: 6.9031  Val_loss: 15.0601 \n",
      "Epoch 4/100\n",
      "2278/2278 [==============================] - trainLoss: 6.6025  Val_loss: 15.0205 \n",
      "Epoch 5/100\n",
      "2278/2278 [==============================] - trainLoss: 6.2700  Val_loss: 14.7852 \n",
      "Epoch 6/100\n",
      "2278/2278 [==============================] - trainLoss: 6.0789  Val_loss: 14.7057 \n",
      "Epoch 7/100\n",
      "2278/2278 [==============================] - trainLoss: 5.9636  Val_loss: 14.5076 \n",
      "Epoch 8/100\n",
      "2278/2278 [==============================] - trainLoss: 5.7621  Val_loss: 14.4218 \n",
      "Epoch 9/100\n",
      "2278/2278 [==============================] - trainLoss: 5.6137  Val_loss: 14.2127 \n",
      "Epoch 10/100\n",
      "2278/2278 [==============================] - trainLoss: 5.4453  Val_loss: 14.0122 \n",
      "Epoch 11/100\n",
      "2278/2278 [==============================] - trainLoss: 5.4114  Val_loss: 14.0413 \n",
      "Epoch 12/100\n",
      "2278/2278 [==============================] - trainLoss: 5.3443  Val_loss: 13.8610 \n",
      "Epoch 13/100\n",
      "2278/2278 [==============================] - trainLoss: 5.1327  Val_loss: 13.6743 \n",
      "Epoch 14/100\n",
      "2278/2278 [==============================] - trainLoss: 5.2193  Val_loss: 13.6472 \n",
      "Epoch 15/100\n",
      "2278/2278 [==============================] - trainLoss: 5.1251  Val_loss: 13.5097 \n",
      "Epoch 16/100\n",
      "2278/2278 [==============================] - trainLoss: 5.0869  Val_loss: 13.4431 \n",
      "Epoch 17/100\n",
      "2278/2278 [==============================] - trainLoss: 5.0401  Val_loss: 13.2130 \n",
      "Epoch 18/100\n",
      "2278/2278 [==============================] - trainLoss: 5.0361  Val_loss: 13.2221 \n",
      "Epoch 19/100\n",
      "2278/2278 [==============================] - trainLoss: 4.9872  Val_loss: 13.2356 \n",
      "Epoch 20/100\n",
      "2278/2278 [==============================] - trainLoss: 4.9508  Val_loss: 13.0645 \n",
      "Epoch 21/100\n",
      "2278/2278 [==============================] - trainLoss: 4.8805  Val_loss: 12.8927 \n",
      "Epoch 22/100\n",
      "2278/2278 [==============================] - trainLoss: 4.8654  Val_loss: 12.8801 \n",
      "Epoch 23/100\n",
      "2278/2278 [==============================] - trainLoss: 4.8876  Val_loss: 12.8828 \n",
      "Epoch 24/100\n",
      "2278/2278 [==============================] - trainLoss: 4.9053  Val_loss: 12.7578 \n",
      "Epoch 25/100\n",
      "2278/2278 [==============================] - trainLoss: 4.8534  Val_loss: 12.7474 \n",
      "Epoch 26/100\n",
      "2278/2278 [==============================] - trainLoss: 4.7815  Val_loss: 12.6593 \n",
      "Epoch 27/100\n",
      "2278/2278 [==============================] - trainLoss: 4.8293  Val_loss: 12.7063 \n",
      "Epoch 28/100\n",
      "2278/2278 [==============================] - trainLoss: 4.8491  Val_loss: 12.5654 \n",
      "Epoch 29/100\n",
      "2278/2278 [==============================] - trainLoss: 4.9061  Val_loss: 12.5478 \n",
      "Epoch 30/100\n",
      "2278/2278 [==============================] - trainLoss: 4.8110  Val_loss: 12.4486 \n",
      "Epoch 31/100\n",
      "2278/2278 [==============================] - trainLoss: 4.8357  Val_loss: 12.4553 \n",
      "Epoch 32/100\n",
      "2278/2278 [==============================] - trainLoss: 4.7390  Val_loss: 12.3617 \n",
      "Epoch 33/100\n",
      "2278/2278 [==============================] - trainLoss: 4.7408  Val_loss: 12.3600 \n",
      "Epoch 34/100\n",
      "2278/2278 [==============================] - trainLoss: 4.7948  Val_loss: 12.3903 \n",
      "Epoch 35/100\n",
      "2278/2278 [==============================] - trainLoss: 4.7232  Val_loss: 12.3619 \n",
      "Epoch 36/100\n",
      "2278/2278 [==============================] - trainLoss: 4.7203  Val_loss: 12.2981 \n",
      "Epoch 37/100\n",
      "2278/2278 [==============================] - trainLoss: 4.8209  Val_loss: 12.2584 \n",
      "Epoch 38/100\n",
      "2278/2278 [==============================] - trainLoss: 4.7075  Val_loss: 12.2238 \n",
      "Epoch 39/100\n",
      "2278/2278 [==============================] - trainLoss: 4.7132  Val_loss: 12.2405 \n",
      "Epoch 40/100\n",
      "2278/2278 [==============================] - trainLoss: 4.7229  Val_loss: 12.1654 \n",
      "Epoch 41/100\n",
      "2278/2278 [==============================] - trainLoss: 4.8087  Val_loss: 12.2037 \n",
      "Epoch 42/100\n",
      "2278/2278 [==============================] - trainLoss: 4.8032  Val_loss: 12.0943 \n",
      "Epoch 43/100\n",
      "2278/2278 [==============================] - trainLoss: 4.7063  Val_loss: 12.1375 \n",
      "Epoch 44/100\n",
      "2278/2278 [==============================] - trainLoss: 4.7110  Val_loss: 12.1050 \n",
      "Epoch 45/100\n",
      "2278/2278 [==============================] - trainLoss: 4.6451  Val_loss: 12.0337 \n",
      "Epoch 46/100\n",
      "2278/2278 [==============================] - trainLoss: 4.7073  Val_loss: 12.0549 \n",
      "Epoch 47/100\n",
      "2278/2278 [==============================] - trainLoss: 4.7055  Val_loss: 12.0353 \n",
      "Epoch 48/100\n",
      "2278/2278 [==============================] - trainLoss: 4.7701  Val_loss: 11.9982 \n",
      "Epoch 49/100\n",
      "2278/2278 [==============================] - trainLoss: 4.6897  Val_loss: 12.0350 \n",
      "Epoch 50/100\n",
      "2278/2278 [==============================] - trainLoss: 4.6933  Val_loss: 11.9627 \n",
      "Epoch 51/100\n",
      "2278/2278 [==============================] - trainLoss: 4.7188  Val_loss: 11.9844 \n",
      "Epoch 52/100\n",
      "2278/2278 [==============================] - trainLoss: 4.6559  Val_loss: 11.9819 \n",
      "Epoch 53/100\n",
      "2278/2278 [==============================] - trainLoss: 4.6854  Val_loss: 12.0270 \n",
      "Epoch 54/100\n",
      "2278/2278 [==============================] - trainLoss: 4.5779  Val_loss: 11.9252 \n",
      "Epoch 55/100\n",
      "2278/2278 [==============================] - trainLoss: 4.5739  Val_loss: 11.9489 \n",
      "Epoch 56/100\n",
      "2278/2278 [==============================] - trainLoss: 4.6306  Val_loss: 11.9216 \n",
      "Epoch 57/100\n",
      "2278/2278 [==============================] - trainLoss: 4.6281  Val_loss: 11.8684 \n",
      "Epoch 58/100\n",
      "2278/2278 [==============================] - trainLoss: 4.6346  Val_loss: 11.8892 \n",
      "Epoch 59/100\n",
      "2278/2278 [==============================] - trainLoss: 4.6615  Val_loss: 11.8485 \n",
      "Epoch 60/100\n",
      "2278/2278 [==============================] - trainLoss: 4.6241  Val_loss: 11.9194 \n",
      "Epoch 61/100\n",
      "2278/2278 [==============================] - trainLoss: 4.6631  Val_loss: 11.8565 \n",
      "Epoch 62/100\n",
      "2278/2278 [==============================] - trainLoss: 4.6615  Val_loss: 11.8546 \n",
      "Epoch 63/100\n",
      "2278/2278 [==============================] - trainLoss: 4.6193  Val_loss: 11.8887 \n",
      "Epoch 64/100\n",
      "2278/2278 [==============================] - trainLoss: 4.5835  Val_loss: 11.9075 \n",
      "Epoch 65/100\n",
      "2278/2278 [==============================] - trainLoss: 4.6013  Val_loss: 11.8473 \n",
      "Epoch 66/100\n",
      "2278/2278 [==============================] - trainLoss: 4.5922  Val_loss: 11.9079 \n",
      "Epoch 67/100\n",
      "2278/2278 [==============================] - trainLoss: 4.5780  Val_loss: 11.9058 \n",
      "Epoch 68/100\n",
      "2278/2278 [==============================] - trainLoss: 4.6316  Val_loss: 11.9165 \n",
      "Epoch 69/100\n",
      "2278/2278 [==============================] - trainLoss: 4.5957  Val_loss: 11.8829 \n",
      "Epoch 70/100\n",
      "2278/2278 [==============================] - trainLoss: 4.5315  Val_loss: 11.8934 \n",
      "Epoch 71/100\n",
      "2278/2278 [==============================] - trainLoss: 4.6380  Val_loss: 11.9797 \n",
      "Epoch 72/100\n",
      "2278/2278 [==============================] - trainLoss: 4.5803  Val_loss: 11.8992 \n",
      "Epoch 73/100\n",
      "2278/2278 [==============================] - trainLoss: 4.6055  Val_loss: 12.0184 \n",
      "Epoch 74/100\n",
      "2278/2278 [==============================] - trainLoss: 4.5180  Val_loss: 12.0160 \n",
      "WARNING:tensorflow:No training configuration found in save file: the model was *not* compiled. Compile it manually.\n",
      "WARNING:tensorflow:No training configuration found in save file: the model was *not* compiled. Compile it manually.\n",
      "WARNING:tensorflow:No training configuration found in save file: the model was *not* compiled. Compile it manually.\n",
      "WARNING:tensorflow:No training configuration found in save file: the model was *not* compiled. Compile it manually.\n",
      "Elapsed/s:  88.20703363418579\n",
      "Final training loss:  tf.Tensor(4.51798, shape=(), dtype=float32)\n",
      "best val loss:  tf.Tensor(11.847345, shape=(), dtype=float32)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "([<tf.Tensor: shape=(), dtype=float32, numpy=17.419695>,\n",
       "  <tf.Tensor: shape=(), dtype=float32, numpy=8.539524>,\n",
       "  <tf.Tensor: shape=(), dtype=float32, numpy=7.3899345>,\n",
       "  <tf.Tensor: shape=(), dtype=float32, numpy=6.9031367>,\n",
       "  <tf.Tensor: shape=(), dtype=float32, numpy=6.602461>,\n",
       "  <tf.Tensor: shape=(), dtype=float32, numpy=6.2700095>,\n",
       "  <tf.Tensor: shape=(), dtype=float32, numpy=6.0789304>,\n",
       "  <tf.Tensor: shape=(), dtype=float32, numpy=5.9635687>,\n",
       "  <tf.Tensor: shape=(), dtype=float32, numpy=5.7620687>,\n",
       "  <tf.Tensor: shape=(), dtype=float32, numpy=5.6136518>,\n",
       "  <tf.Tensor: shape=(), dtype=float32, numpy=5.445345>,\n",
       "  <tf.Tensor: shape=(), dtype=float32, numpy=5.4114447>,\n",
       "  <tf.Tensor: shape=(), dtype=float32, numpy=5.3443465>,\n",
       "  <tf.Tensor: shape=(), dtype=float32, numpy=5.132705>,\n",
       "  <tf.Tensor: shape=(), dtype=float32, numpy=5.2192755>,\n",
       "  <tf.Tensor: shape=(), dtype=float32, numpy=5.12508>,\n",
       "  <tf.Tensor: shape=(), dtype=float32, numpy=5.0868664>,\n",
       "  <tf.Tensor: shape=(), dtype=float32, numpy=5.040071>,\n",
       "  <tf.Tensor: shape=(), dtype=float32, numpy=5.0361247>,\n",
       "  <tf.Tensor: shape=(), dtype=float32, numpy=4.987237>,\n",
       "  <tf.Tensor: shape=(), dtype=float32, numpy=4.950788>,\n",
       "  <tf.Tensor: shape=(), dtype=float32, numpy=4.8805003>,\n",
       "  <tf.Tensor: shape=(), dtype=float32, numpy=4.865427>,\n",
       "  <tf.Tensor: shape=(), dtype=float32, numpy=4.8876276>,\n",
       "  <tf.Tensor: shape=(), dtype=float32, numpy=4.905284>,\n",
       "  <tf.Tensor: shape=(), dtype=float32, numpy=4.8533845>,\n",
       "  <tf.Tensor: shape=(), dtype=float32, numpy=4.78155>,\n",
       "  <tf.Tensor: shape=(), dtype=float32, numpy=4.8292956>,\n",
       "  <tf.Tensor: shape=(), dtype=float32, numpy=4.849063>,\n",
       "  <tf.Tensor: shape=(), dtype=float32, numpy=4.9061456>,\n",
       "  <tf.Tensor: shape=(), dtype=float32, numpy=4.811043>,\n",
       "  <tf.Tensor: shape=(), dtype=float32, numpy=4.835678>,\n",
       "  <tf.Tensor: shape=(), dtype=float32, numpy=4.7389674>,\n",
       "  <tf.Tensor: shape=(), dtype=float32, numpy=4.7408>,\n",
       "  <tf.Tensor: shape=(), dtype=float32, numpy=4.7947626>,\n",
       "  <tf.Tensor: shape=(), dtype=float32, numpy=4.723175>,\n",
       "  <tf.Tensor: shape=(), dtype=float32, numpy=4.7203236>,\n",
       "  <tf.Tensor: shape=(), dtype=float32, numpy=4.8208623>,\n",
       "  <tf.Tensor: shape=(), dtype=float32, numpy=4.7075133>,\n",
       "  <tf.Tensor: shape=(), dtype=float32, numpy=4.7131767>,\n",
       "  <tf.Tensor: shape=(), dtype=float32, numpy=4.7229447>,\n",
       "  <tf.Tensor: shape=(), dtype=float32, numpy=4.8087287>,\n",
       "  <tf.Tensor: shape=(), dtype=float32, numpy=4.8031645>,\n",
       "  <tf.Tensor: shape=(), dtype=float32, numpy=4.706304>,\n",
       "  <tf.Tensor: shape=(), dtype=float32, numpy=4.7110476>,\n",
       "  <tf.Tensor: shape=(), dtype=float32, numpy=4.645139>,\n",
       "  <tf.Tensor: shape=(), dtype=float32, numpy=4.7072773>,\n",
       "  <tf.Tensor: shape=(), dtype=float32, numpy=4.7055044>,\n",
       "  <tf.Tensor: shape=(), dtype=float32, numpy=4.770096>,\n",
       "  <tf.Tensor: shape=(), dtype=float32, numpy=4.6896586>,\n",
       "  <tf.Tensor: shape=(), dtype=float32, numpy=4.693329>,\n",
       "  <tf.Tensor: shape=(), dtype=float32, numpy=4.7187843>,\n",
       "  <tf.Tensor: shape=(), dtype=float32, numpy=4.655877>,\n",
       "  <tf.Tensor: shape=(), dtype=float32, numpy=4.6854243>,\n",
       "  <tf.Tensor: shape=(), dtype=float32, numpy=4.5779386>,\n",
       "  <tf.Tensor: shape=(), dtype=float32, numpy=4.5739026>,\n",
       "  <tf.Tensor: shape=(), dtype=float32, numpy=4.6305656>,\n",
       "  <tf.Tensor: shape=(), dtype=float32, numpy=4.62808>,\n",
       "  <tf.Tensor: shape=(), dtype=float32, numpy=4.634639>,\n",
       "  <tf.Tensor: shape=(), dtype=float32, numpy=4.6614733>,\n",
       "  <tf.Tensor: shape=(), dtype=float32, numpy=4.624059>,\n",
       "  <tf.Tensor: shape=(), dtype=float32, numpy=4.663095>,\n",
       "  <tf.Tensor: shape=(), dtype=float32, numpy=4.6614776>,\n",
       "  <tf.Tensor: shape=(), dtype=float32, numpy=4.6193156>,\n",
       "  <tf.Tensor: shape=(), dtype=float32, numpy=4.5835056>,\n",
       "  <tf.Tensor: shape=(), dtype=float32, numpy=4.6013093>,\n",
       "  <tf.Tensor: shape=(), dtype=float32, numpy=4.5922394>,\n",
       "  <tf.Tensor: shape=(), dtype=float32, numpy=4.5779524>,\n",
       "  <tf.Tensor: shape=(), dtype=float32, numpy=4.631557>,\n",
       "  <tf.Tensor: shape=(), dtype=float32, numpy=4.59573>,\n",
       "  <tf.Tensor: shape=(), dtype=float32, numpy=4.531547>,\n",
       "  <tf.Tensor: shape=(), dtype=float32, numpy=4.6379547>,\n",
       "  <tf.Tensor: shape=(), dtype=float32, numpy=4.5803275>,\n",
       "  <tf.Tensor: shape=(), dtype=float32, numpy=4.60545>,\n",
       "  <tf.Tensor: shape=(), dtype=float32, numpy=4.51798>],\n",
       " <tf.Tensor: shape=(), dtype=float32, numpy=11.847345>)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fit_model(X_train_omics_labelled, train_set_labelled_y, X_train_omics_unlabelled,100,X_valid_omics, valid_set_labelled_y,\n",
    "              10,variational_encoder=variational_encoder,variational_decoder=variational_decoder,\n",
    "             classifier=classifier,y_distribution=y_distribution,model=model,\n",
    "          Sampling=Sampling,y_dist=y_dist,batch_size=32,learning_rate=0.001,valid_set=True,codings_size=119)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test_nlog_lik = tf.Tensor(12.35483, shape=(), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "test_nlog_lik = -validation_log_lik_sampling(test_set_labelled_y,X_test_omics.to_numpy(),\n",
    "                                    variational_decoder=variational_decoder,codings_size=119,samples=2000)\n",
    "print(\"test_nlog_lik = \" + str(test_nlog_lik))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is the best score and therefore the model that will be used."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
