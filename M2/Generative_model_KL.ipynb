{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "assert sys.version_info >= (3, 5)\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "assert tf.__version__ >= \"2.0\"\n",
    "import numpy as np\n",
    "import time\n",
    "K = keras.backend\n",
    "import pandas as pd\n",
    "import math\n",
    "import os\n",
    "from sklearn.metrics import r2_score\n",
    "from scipy.stats import uniform,randint\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import scipy.stats\n",
    "from scipy.stats import norm\n",
    "from sklearn.metrics import silhouette_score\n",
    "from scipy.cluster.hierarchy import linkage, cophenet\n",
    "\n",
    "tf.random.set_seed(42)\n",
    "np.random.seed(42)\n",
    "random_state=42"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_path = 'save_path'\n",
    "os.chdir(save_path)\n",
    "\n",
    "X_train_omics_unlabelled = pd.read_csv(\"X_train_omics_unlabelled.csv\",index_col=0)\n",
    "X_train_omics_labelled = pd.read_csv(\"X_train_omics_labelled.csv\",index_col=0)\n",
    "X_test_omics= pd.read_csv(\"X_test_omics.csv\",index_col=0)\n",
    "X_valid_omics= pd.read_csv(\"X_valid_omics.csv\",index_col=0)\n",
    "features = np.load(\"feature_selection.npy\",allow_pickle=True)\n",
    "\n",
    "train_set_labelled_y= pd.read_csv(\"train_set_labelled_y.csv\",index_col=0)\n",
    "test_set_labelled_y= pd.read_csv(\"test_set_labelled_y.csv\",index_col=0)\n",
    "valid_set_labelled_y= pd.read_csv(\"valid_set_labelled_y.csv\",index_col=0)\n",
    "\n",
    "X_train_omics_unlabelled = X_train_omics_unlabelled[features]\n",
    "X_train_omics_labelled = X_train_omics_labelled[features]\n",
    "X_test_omics = X_test_omics[features]\n",
    "X_valid_omics = X_valid_omics[features]\n",
    "\n",
    "train_set_labelled_c= pd.read_csv(\"train_set_labelled_c.csv\",index_col=0)\n",
    "train_set_unlabelled_c= pd.read_csv(\"train_set_unlabelled_c.csv\",index_col=0)\n",
    "test_set_labelled_c= pd.read_csv(\"test_set_labelled_c.csv\",index_col=0)\n",
    "valid_set_labelled_c= pd.read_csv(\"valid_set_labelled_c.csv\",index_col=0)\n",
    "\n",
    "#bin y \n",
    "from sklearn.preprocessing import KBinsDiscretizer\n",
    "binner = KBinsDiscretizer(n_bins=10,encode=\"onehot-dense\",strategy=\"uniform\")\n",
    "train_set_labelled_y = binner.fit_transform(train_set_labelled_y)\n",
    "valid_set_labelled_y = binner.transform(valid_set_labelled_y)\n",
    "test_set_labelled_y=binner.transform(test_set_labelled_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_model_path = 'save_model_path'\n",
    "os.chdir(save_model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_shape = X_train_omics_labelled.shape[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Custom parts # "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Helpful functions ##"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def validation_log_lik_sampling(y_val,x_val,variational_decoder,codings_size,samples=200):\n",
    "    \"\"\"\n",
    "    Samples a value of z for the expectation, and calculates something proportional to loglikelihood.\n",
    "    \n",
    "    The more samples of z, the better the MC approximation to loglik.\n",
    "    \n",
    "    This is how we do our evaluation on the validation and also test set. \n",
    "    \n",
    "    We look at the ability to generate x given y i.e. loglik(x|y)\"\"\"\n",
    "    \n",
    "    x_val_len = len(x_val)\n",
    "    expectation = 0\n",
    "    for i in range(samples):\n",
    "        z = np.random.normal(loc=0,scale=1,size=codings_size*x_val_len).reshape(x_val_len,codings_size)\n",
    "        x_pred = variational_decoder([z,y_val])\n",
    "        diff = (x_val-x_pred)**2\n",
    "        pdf = K.sum(diff,axis=-1)\n",
    "        pdf = K.exp(-pdf)\n",
    "        expectation += pdf \n",
    "    expectation = expectation / samples\n",
    "    lik = tf.math.log(expectation)\n",
    "    lik = K.mean(lik)    \n",
    "    return lik\n",
    "\n",
    "def create_batch(x_label, y_label, x_unlabel, batch_s=32):\n",
    "    '''\n",
    "    Creates batches of labelled and unlabelled data. The total number of points in both batches is equal to batch_s.\n",
    "    Thanks to Omer Nivron for help with this.\n",
    "    \n",
    "    '''\n",
    "    proportion_labelled = x_label.shape[0]/(x_label.shape[0] + x_unlabel.shape[0])\n",
    "    \n",
    "    shape_label = x_label.shape[0]\n",
    "    label_per_batch = int(np.ceil(proportion_labelled*batch_s))\n",
    "    batch_idx_la = np.random.choice(list(range(shape_label)), label_per_batch)\n",
    "    batch_x_la = (x_label.iloc[batch_idx_la, :])\n",
    "    batch_y_la = (y_label[batch_idx_la,:])\n",
    "\n",
    "    \n",
    "    shape_unlabel = x_unlabel.shape[0]\n",
    "    unlabel_per_batch = batch_s - label_per_batch\n",
    "    batch_idx_un = np.random.choice(list(range(shape_unlabel)), unlabel_per_batch)\n",
    "    batch_x_un = (x_unlabel.iloc[batch_idx_un, :])\n",
    "    \n",
    "    del batch_idx_la,batch_idx_un\n",
    "            \n",
    "    return batch_x_la, batch_y_la, batch_x_un\n",
    "\n",
    "\n",
    "def progress_bar(iteration, total, size=30):\n",
    "    \"\"\"Progress bar for training\"\"\"\n",
    "    running = iteration < total\n",
    "    c = \">\" if running else \"=\"\n",
    "    p = (size - 1) * iteration // total\n",
    "    fmt = \"{{:-{}d}}/{{}} [{{}}]\".format(len(str(total)))\n",
    "    params = [iteration, total, \"=\" * p + c + \".\" * (size - p - 1)]\n",
    "    return fmt.format(*params)\n",
    "\n",
    "def print_status_bar(iteration, total, loss, metrics=None, size=30):\n",
    "    \"\"\"Status bar for training\"\"\"\n",
    "    metrics = \" - \".join([\"Loss for batch: {:.4f}\".format(loss)])\n",
    "    end = \"\" if iteration < total else \"\\n\"\n",
    "    print(\"\\r{} - {}\".format(progress_bar(iteration, total), metrics), end=end)\n",
    "    \n",
    "def print_status_bar_epoch(iteration, total, training_loss_for_epoch,val_loss, metrics=None, size=30):\n",
    "    \"\"\"Status bar for training (end of epoch)\"\"\"\n",
    "    metrics = \" - \".join(\n",
    "        [\"trainLoss: {:.4f}  Val_loss: {:.4f} \".format(\n",
    "            training_loss_for_epoch,val_loss)]\n",
    "    )\n",
    "    end = \"\" if iteration < total else \"\\n\"\n",
    "    print(\"\\r{} - {}\".format(progress_bar(iteration, total), metrics), end=end)\n",
    "    \n",
    "    \n",
    "def list_average(list_of_loss):\n",
    "    return sum(list_of_loss)/len(list_of_loss)\n",
    "\n",
    "\n",
    "def y_pred_loss(y_in):\n",
    "    \"\"\"Calculates loss and true y distribution given some y data.\n",
    "    \n",
    "    When the model calculates this it does it in batches (unlike this function which can take the whole data in).\n",
    "    \n",
    "    Therefore the model's learned distribution will probably not be as good as what is learnt when using the whole\n",
    "    dataset. But that is one of the things that happens if we use mini-batch gradient descent.\n",
    "    \n",
    "    \"\"\"\n",
    "    y_distribution = (K.sum(y_in,axis=0) / len(y_in))\n",
    "    loss = tf.reduce_mean(keras.losses.categorical_crossentropy(y_in,y_distribution))\n",
    "    return loss,y_distribution \n",
    "\n",
    "\n",
    "def rounded_accuracy(y_true,y_pred):\n",
    "    \"\"\"\n",
    "    Calculates accuracy of classification predictions.\n",
    "    \n",
    "    For the 10D p vector which is y_pred, it sets the highest number to 1 and the rest to 0.\n",
    "    \n",
    "    It then computes accuracy.\n",
    "    \"\"\"\n",
    "    from sklearn.metrics import accuracy_score\n",
    "    b = np.zeros_like(y_pred)\n",
    "    b[np.arange(len(y_pred)),y_pred.argmax(1)] = 1\n",
    "    return accuracy_score(y_true,b)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model ##"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Custom components ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Sampling(keras.layers.Layer):\n",
    "    \"\"\"reparameterization trick\"\"\"\n",
    "    def call(self, inputs):\n",
    "        mean, log_var = inputs\n",
    "        return K.random_normal(tf.shape(log_var)) * K.exp(log_var/2) + mean\n",
    "     \n",
    "    \n",
    "    \n",
    "class y_dist(keras.layers.Layer):\n",
    "    \"\"\"\n",
    "    Custom layer that is used to learn the parameters, p, of the distribution over y.\n",
    "    \n",
    "    Outputs a loss and p. The loss is used for training. The loss is the categorical cross entropy loss between \n",
    "    p and every y sample. The mean of this is then taken to provide a per batch loss. \n",
    "    \n",
    "    Shapes are configured for a 10D y. Change if you want to use different number of categories.\n",
    "    \n",
    "    \"\"\"\n",
    "    def __init__(self,**kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        \n",
    "    def build(self,batch_input_shape):\n",
    "        self.q = self.add_weight(name=\"q\",shape=[1,9],initializer=\"uniform\",trainable=True)\n",
    "        super().build(batch_input_shape)\n",
    "    \n",
    "    def call(self,X):\n",
    "        concatenated = tf.concat([self.q,tf.constant(np.array(0.0).reshape(1,-1),dtype=\"float32\")],axis=-1)\n",
    "        p = K.exp(concatenated)\n",
    "        p = tf.math.divide(p,K.sum(p))\n",
    "        loss = keras.losses.categorical_crossentropy(X,p)\n",
    "        loss = tf.reduce_mean(loss)\n",
    "        return loss,p \n",
    "    \n",
    "    def compute_output_shape(self,batch_input_shape):\n",
    "        return tf.TensorShape(10)\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "class FullModel(keras.models.Model):\n",
    "    \"\"\"\n",
    "    This is the full model. This is used for training purposes.\n",
    "    \n",
    "    It requires an encoder, decoder, classifier and y_distribution model to be already defined (as can be done with \n",
    "    the build_model function).\n",
    "    \n",
    "    It returns the nloglik i.e. the loss. \n",
    "    \n",
    "    This loss can then be used in gradient descent and be minimised wrt parameters (of the four component models).\n",
    "    \n",
    "    At test time, you will call which of the component models you want to use (as opposed to trying to \"call\" this \n",
    "    FullModel which you won't want to do as its purpose is just to calculate the nloglik for training).\n",
    "    \n",
    "    \"\"\"\n",
    "    def __init__(self,N_parameter,beta,variational_encoder,variational_decoder,classifier,y_distribution,**kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.encoder = variational_encoder\n",
    "        self.decoder = variational_decoder\n",
    "        self.classifier = classifier  \n",
    "        self.y_distribution = y_distribution\n",
    "        self.N = N_parameter\n",
    "        self.beta = beta\n",
    "    def call(self,inputs):\n",
    "        \"\"\"Inputs is a list, as such:\n",
    "            inputs[0] is labelled X \n",
    "            inputs[1] is labelled y \n",
    "            inputs[2] is unlabelled X\"\"\"\n",
    "        \n",
    "        X_labelled = inputs[0]\n",
    "        y_labelled = inputs[1]\n",
    "        X_unlabelled = inputs[2]\n",
    "        \n",
    "        ############### LABELLED CASE #################\n",
    "        \n",
    "        codings_mean,codings_log_var,codings = self.encoder([X_labelled,y_labelled])\n",
    "        y_pred_label = self.classifier(X_labelled)\n",
    "        reconstructions = self.decoder([codings,y_labelled])\n",
    "\n",
    "        #LOSSES#\n",
    "        recon_loss = labelled_loss_reconstruction(codings_log_var=codings_log_var,codings_mean=codings_mean,\n",
    "                                x=X_labelled,x_decoded_mean=reconstructions,beta=self.beta)\n",
    "        cls_loss = labelled_cls_loss(y=y_labelled,y_pred=y_pred_label,N=self.N)\n",
    "        y_dist_loss = self.y_distribution(y_labelled)[0]\n",
    "        labelled_loss = recon_loss + cls_loss + y_dist_loss\n",
    "\n",
    "        ############### UNLABELLED CASE #################\n",
    "       \n",
    "        y_pred_unlabel = self.classifier(X_unlabelled)\n",
    "        codings_mean,codings_log_var,codings = self.encoder([X_unlabelled,y_pred_unlabel])\n",
    "        reconstructions_un = self.decoder([codings,y_pred_unlabel])\n",
    "        \n",
    "        #LOSSES#\n",
    "        \n",
    "        unlabelled_recon_loss = unlabelled_loss_reconstruction(codings_log_var=codings_log_var,\n",
    "                            codings_mean= codings_mean,y_pred=y_pred_unlabel,\n",
    "                               x=X_unlabelled,x_decoded_mean=reconstructions_un,beta=self.beta)\n",
    "\n",
    "        y_dist_loss = self.y_distribution(y_pred_unlabel)[0]\n",
    "        unlabelled_loss = unlabelled_recon_loss + y_dist_loss\n",
    "        \n",
    "        ############### ALL LOSSES #######################\n",
    "        \n",
    "        loss = labelled_loss + unlabelled_loss\n",
    "        return loss\n",
    "\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "\n",
    "def build_model(n_hidden=1, n_neurons=723,input_shape=input_shape,beta=1,n_hidden_classifier=1,\n",
    "              n_neurons_classifier=300,N=30,codings_size=50):\n",
    "    \n",
    "    \"\"\"\n",
    "    Builds deep generative model.\n",
    "    \n",
    "    Parameters specify the architecture. Architecture is such that encoder and decoder have same number of nodes and hidden\n",
    "    layers. Done for simplicity. Classifier has its own architecture.\n",
    "    \n",
    "    Returns encoder,decoder,y_distribution, classifier and overall model. These can be used downstream.\n",
    "    \n",
    "    e.g. variational_encoder,variational_decoder,classifier,y_distribution,model = build_model(n_hidden=1, n_neurons=723,input_shape=input_shape,beta=1,n_hidden_classifier=1,\n",
    "              n_neurons_classifier=300,N=30,codings_size=50)\n",
    "    \"\"\"\n",
    "       \n",
    "    ########## ENCODER ###############\n",
    "    \n",
    "    x_in = keras.layers.Input(shape=[input_shape])\n",
    "    y_in = keras.layers.Input(shape=[10])\n",
    "    z = keras.layers.concatenate([x_in,y_in])\n",
    "    for layer in range(n_hidden):\n",
    "        z = keras.layers.Dense(n_neurons,activation=\"elu\",kernel_initializer=\"he_normal\")(z)\n",
    "        z = keras.layers.Dropout(0.3)(z)\n",
    "\n",
    "    codings_mean = keras.layers.Dense(codings_size)(z)\n",
    "    codings_log_var = keras.layers.Dense(codings_size)(z)\n",
    "    codings = Sampling()([codings_mean, codings_log_var])\n",
    "    variational_encoder = keras.models.Model(\n",
    "        inputs=[x_in,y_in], outputs=[codings_mean, codings_log_var, codings])\n",
    "    \n",
    "    \n",
    "    ########## DECODER ###############\n",
    "\n",
    "    latent = keras.layers.Input(shape=[codings_size])\n",
    "    l_merged = keras.layers.concatenate([latent,y_in])\n",
    "    x = l_merged\n",
    "    for layer in range(n_hidden):\n",
    "        x = keras.layers.Dense(n_neurons, activation=\"elu\",kernel_initializer=\"he_normal\")(x)\n",
    "        x = keras.layers.Dropout(0.3)(x)\n",
    "    x_out = keras.layers.Dense(input_shape,activation=\"sigmoid\")(x) \n",
    "    variational_decoder = keras.models.Model(inputs=[latent,y_in], outputs=[x_out])\n",
    "    \n",
    "    \n",
    "    ########### CLASSIFIER ############\n",
    "    \n",
    "    y_classifier = x_in\n",
    "    for layer in range(n_hidden_classifier):\n",
    "        y_classifier = keras.layers.Dense(n_neurons_classifier, activation=\"elu\",kernel_initializer=\"he_normal\")(y_classifier)\n",
    "        y_classifier = keras.layers.Dropout(rate=0.3)(y_classifier)\n",
    "    y_pred = keras.layers.Dense(10,activation=\"softmax\")(y_classifier) \n",
    "    classifier = keras.models.Model(inputs=[x_in], outputs=[y_pred])\n",
    "    \n",
    "    \n",
    "    ############ Y DISTRIBUTION #############\n",
    "    \n",
    "    loss,p = y_dist()(y_in)\n",
    "    y_distribution = keras.models.Model(inputs=[y_in],outputs=[loss,p])\n",
    "    \n",
    "    \n",
    "    ########## FULL MODEL #############\n",
    "    \n",
    "    model = FullModel(N_parameter=N,beta=beta,variational_encoder=variational_encoder,\n",
    "                  variational_decoder=variational_decoder,classifier=classifier,y_distribution=y_distribution)\n",
    "    \n",
    "    return variational_encoder,variational_decoder,classifier,y_distribution,model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loss functions ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def custom_mse(x,x_decoded_mean):\n",
    "    \"\"\"returns column of squared errors. Length of column is number of samples.\"\"\"\n",
    "    diff = (x-x_decoded_mean)**2\n",
    "    return K.sum(diff,axis=-1) /2 \n",
    "\n",
    "\n",
    "def labelled_loss_reconstruction(codings_log_var,codings_mean,x, x_decoded_mean,beta=1):\n",
    "    \"\"\"Labelled data. This is the labelled loss.\"\"\"\n",
    "    recon_loss = custom_mse(x, x_decoded_mean)\n",
    "    kl_loss = - 0.5 * K.sum(1 + codings_log_var - K.square(codings_mean) - K.exp(codings_log_var), axis=-1)\n",
    "    #kl loss gives vector of one value for each sample in batch \n",
    "    return K.mean(recon_loss + beta*kl_loss)\n",
    "\n",
    "def unlabelled_loss_reconstruction(codings_log_var,codings_mean,y_pred,x,x_decoded_mean,beta=1):\n",
    "    \"\"\"Unlabelled data. This is the loss for unlabelled data. Reconstruction loss, KL loss and entropy\n",
    "    are the included terms.\"\"\"\n",
    "    recon_loss = custom_mse(x,x_decoded_mean)\n",
    "    kl_loss = - 0.5 * K.sum(1 + codings_log_var - K.square(codings_mean) - K.exp(codings_log_var), axis=-1)\n",
    "    entropy = keras.losses.categorical_crossentropy(y_pred,y_pred)\n",
    "    loss = K.mean(beta*kl_loss + recon_loss)\n",
    "    #need to check below. We are summing over y, but we are assuming that the loss term is independent of y\n",
    "    #which is not the case. Should update this for better model https://github.com/bjlkeng/sandbox/issues/3\n",
    "    return K.mean(K.sum(y_pred*loss,axis=-1)) - K.mean(entropy) #note the sign\n",
    "\n",
    "\n",
    "def dummy_loss(y,ypred):\n",
    "    \"\"\"This is a dummy loss that returns a value of zero. It is here as keras requires a loss term for each output.\n",
    "        The regression_loss_for_labelled_y gives the loss which depends on the log var and mean, so we don't need another\n",
    "        loss. But keras wants us to give separate losses for each. To keep keras happy, we use the dummy loss as a placeholder.\"\"\"\n",
    "    return 0.0\n",
    "\n",
    "def labelled_cls_loss(y, y_pred,N=383):\n",
    "    alpha = 0.1*N\n",
    "    cat_xent_loss = keras.losses.categorical_crossentropy(y, y_pred)\n",
    "    return alpha*K.mean(cat_xent_loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training functions ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "@tf.function\n",
    "def train_step(inputs):\n",
    "    \"\"\"Decorated train_step function which applies a gradient update to the parameters\"\"\"\n",
    "    with tf.GradientTape() as tape:\n",
    "        loss = model(inputs,training=True)\n",
    "        loss = tf.add_n([loss] + model.losses)\n",
    "    gradients = tape.gradient(loss, model.trainable_variables)\n",
    "    optimizer.apply_gradients(zip(gradients, model.trainable_variables))\n",
    "    return loss\n",
    "\n",
    "def fit_model(X_train_la, y_train_la, X_train_un,epochs,X_valid_la, y_valid_la,\n",
    "              patience,variational_encoder,variational_decoder,\n",
    "             classifier,y_distribution,model,Sampling=Sampling,y_dist=y_dist,batch_size=32,learning_rate=0.001,codings_size=50,\n",
    "             valid_set=True):\n",
    "\n",
    "    \"\"\"\n",
    "    Fits a single model. Gets the validation loss too if valid set exists. \n",
    "    And includes a version of early stopping, given by the patience.\n",
    "    Progress bars are shown too.\n",
    "    Number of epochs are specified by the parameter epochs.\n",
    "    \n",
    "    Need to pass in all the custom components. Maybe could put them in a dictionary for cleanliness.\n",
    "    \n",
    "    Valid set is True or False depending if you have one. If you don't, the model at the end of training is saved.\n",
    "    You must still pass in dummy valid sets even if valid_set=False.\n",
    "    \n",
    "    Returns list of training loss, and the minimum validation loss. It also saves the best encoder, decoder and\n",
    "    regressor so they can be used. \n",
    "    \n",
    "    e.g. usage fit_model(X_train_omics_labelled, train_set_labelled_y, X_train_omics_unlabelled,50,X_valid_omics, valid_set_labelled_y,\n",
    "              10,variational_encoder=variational_encoder,variational_decoder=variational_decoder,\n",
    "             classifier=classifier,y_distribution=y_distribution,model=model,\n",
    "          Sampling=Sampling,y_dist=y_dist,batch_size=32,learning_rate=0.001,codings_size=50,valid_set=True)\n",
    "    \"\"\"\n",
    "    if valid_set is True:\n",
    "    \n",
    "        start = time.time()\n",
    "        history = []\n",
    "        K.clear_session()\n",
    "\n",
    "        @tf.function\n",
    "        def train_step(inputs):\n",
    "            with tf.GradientTape() as tape:\n",
    "                loss = model(inputs,training=True)\n",
    "                loss = tf.add_n([loss] + model.losses)\n",
    "            gradients = tape.gradient(loss, model.trainable_variables)\n",
    "            optimizer.apply_gradients(zip(gradients, model.trainable_variables))\n",
    "            return loss\n",
    "\n",
    "        validation_loss = []\n",
    "        optimizer=keras.optimizers.Adam(learning_rate=learning_rate)\n",
    "        batch_loss = []\n",
    "        batches_per_epoch = int(np.floor((X_train_la.shape[0] + X_train_un.shape[0])/batch_size))\n",
    "\n",
    "        for epoch in range(epochs):\n",
    "\n",
    "                print(\"Epoch {}/{}\".format(epoch,epochs))\n",
    "\n",
    "                for i in range(batches_per_epoch):\n",
    "\n",
    "                    batch_x_la, batch_y_la, batch_x_un= create_batch(\n",
    "                        X_train_la, y_train_la, X_train_un,batch_size)\n",
    "\n",
    "                    inputs = [batch_x_la.to_numpy(),batch_y_la,batch_x_un.to_numpy()]\n",
    "                    loss = train_step(inputs)\n",
    "                    batch_loss.append(loss)\n",
    "                    average_batch_loss = list_average(batch_loss)\n",
    "                    print_status_bar(i*batch_size,X_train_la.shape[0] + X_train_un.shape[0],average_batch_loss)\n",
    "\n",
    "                training_loss_for_epoch = list_average(batch_loss)\n",
    "                batch_loss = []\n",
    "                history.append(training_loss_for_epoch)\n",
    "                val_loss = -validation_log_lik_sampling(y_valid_la,X_valid_la.to_numpy(),variational_decoder=variational_decoder,codings_size=codings_size)\n",
    "\n",
    "                validation_loss.append(val_loss)\n",
    "                print_status_bar_epoch(X_train_la.shape[0] + X_train_un.shape[0]\n",
    "                                 ,(X_train_la.shape[0] + X_train_un.shape[0]),training_loss_for_epoch,val_loss )\n",
    "\n",
    "                #callback for early stopping\n",
    "                if epoch <= patience - 1:\n",
    "\n",
    "                    if epoch == 0:\n",
    "\n",
    "                        variational_encoder.save(\"variational_encoder.h5\")\n",
    "                        variational_decoder.save(\"variational_decoder.h5\")\n",
    "                        classifier.save(\"classifier.h5\")\n",
    "                        y_distribution.save(\"y_distribution.h5\")\n",
    "\n",
    "                    else:\n",
    "                        if all(val_loss<i for i in validation_loss[:-1]) is True:\n",
    "                            variational_encoder.save(\"variational_encoder.h5\")\n",
    "                            variational_decoder.save(\"variational_decoder.h5\")\n",
    "                            classifier.save(\"classifier.h5\")\n",
    "                            y_distribution.save(\"y_distribution.h5\")\n",
    "                #this statement means at least a model is saved. Because if the best model was before epoch > patience-1,\n",
    "                #then the statement below won't save any model, which is undesirable as we need to load a model. \n",
    "\n",
    "                if epoch > patience - 1:\n",
    "\n",
    "                    latest_val_loss = validation_loss[-patience:]\n",
    "                    if all(val_loss<i for i in latest_val_loss[:-2]) is True:\n",
    "                        variational_encoder.save(\"variational_encoder.h5\")\n",
    "                        variational_decoder.save(\"variational_decoder.h5\")\n",
    "                        classifier.save(\"classifier.h5\")\n",
    "                        y_distribution.save(\"y_distribution.h5\")\n",
    "                    if all(i>latest_val_loss[0] for i in latest_val_loss[1:]) is True:\n",
    "                        break     \n",
    "\n",
    "        #load best model#\n",
    "        variational_encoder = keras.models.load_model(\"variational_encoder.h5\", custom_objects={\n",
    "           \"Sampling\": Sampling\n",
    "        })\n",
    "        variational_decoder = keras.models.load_model(\"variational_decoder.h5\")\n",
    "        classifier = keras.models.load_model(\"classifier.h5\")     \n",
    "        y_distribution = keras.models.load_model(\"y_distribution.h5\", custom_objects={\n",
    "           \"y_dist\": y_dist\n",
    "        })    \n",
    "\n",
    "        done = time.time()\n",
    "        elapsed = done-start\n",
    "        print(\"Elapsed/s: \",elapsed)\n",
    "        print(\"Final training loss: \",training_loss_for_epoch)\n",
    "        print(\"best val loss: \", min(validation_loss))\n",
    "        \n",
    "        return history, min(validation_loss)\n",
    "\n",
    "        \n",
    "    else:\n",
    "        \n",
    "        start = time.time()\n",
    "        history = []\n",
    "        K.clear_session()\n",
    "\n",
    "        @tf.function\n",
    "        def train_step(inputs):\n",
    "            with tf.GradientTape() as tape:\n",
    "                loss = model(inputs,training=True)\n",
    "                loss = tf.add_n([loss] + model.losses)\n",
    "            gradients = tape.gradient(loss, model.trainable_variables)\n",
    "            optimizer.apply_gradients(zip(gradients, model.trainable_variables))\n",
    "            return loss\n",
    "\n",
    "        optimizer=keras.optimizers.Adam(learning_rate=learning_rate)\n",
    "        batch_loss = []\n",
    "        batches_per_epoch = int(np.floor((X_train_la.shape[0] + X_train_un.shape[0])/batch_size))        \n",
    "        val_loss = 0\n",
    "\n",
    "        for epoch in range(epochs):\n",
    "\n",
    "                print(\"Epoch {}/{}\".format(epoch,epochs))\n",
    "                for i in range(batches_per_epoch):\n",
    "\n",
    "                    batch_x_la, batch_y_la, batch_x_un= create_batch(\n",
    "                        X_train_la, y_train_la, X_train_un,batch_size)\n",
    "\n",
    "                    inputs = [batch_x_la.to_numpy(),batch_y_la,batch_x_un.to_numpy()]\n",
    "                    loss = train_step(inputs)\n",
    "                    batch_loss.append(loss)\n",
    "                    average_batch_loss = list_average(batch_loss)\n",
    "                    print_status_bar(i*batch_size,X_train_la.shape[0] + X_train_un.shape[0],average_batch_loss)\n",
    "\n",
    "                training_loss_for_epoch = list_average(batch_loss)\n",
    "                batch_loss = []\n",
    "                history.append(training_loss_for_epoch)\n",
    "                print_status_bar_epoch(X_train_la.shape[0] + X_train_un.shape[0]\n",
    "                                 ,(X_train_la.shape[0] + X_train_un.shape[0]),training_loss_for_epoch,val_loss )\n",
    "        \n",
    "\n",
    "        variational_encoder.save(\"variational_encoder.h5\")\n",
    "        variational_decoder.save(\"variational_decoder.h5\")\n",
    "        classifier.save(\"classifier.h5\")\n",
    "        y_distribution.save(\"y_distribution.h5\")\n",
    "        \n",
    "        #load best model#\n",
    "        variational_encoder = keras.models.load_model(\"variational_encoder.h5\", custom_objects={\n",
    "           \"Sampling\": Sampling\n",
    "        })\n",
    "        variational_decoder = keras.models.load_model(\"variational_decoder.h5\")\n",
    "        classifier = keras.models.load_model(\"classifier.h5\")     \n",
    "        y_distribution = keras.models.load_model(\"y_distribution.h5\", custom_objects={\n",
    "           \"y_dist\": y_dist\n",
    "        })    \n",
    "\n",
    "        done = time.time()\n",
    "        elapsed = done-start\n",
    "        print(\"Elapsed/s: \",elapsed)\n",
    "        print(\"Final training loss: \",training_loss_for_epoch)\n",
    "        \n",
    "    \n",
    "        return history\n",
    "\n",
    "\n",
    "def fit_model_search(X_train_la, y_train_la, X_train_un,epochs,X_valid_la, y_valid_la,\n",
    "              patience,variational_encoder,variational_decoder,\n",
    "             classifier,y_distribution,model,Sampling=Sampling,y_dist=y_dist,batch_size=32,learning_rate=0.001,\n",
    "                    codings_size=50):\n",
    "\n",
    "    \"\"\"\n",
    "    Use for hyperparameter search. \n",
    "    \n",
    "    Fits the model. Gets the validation loss too. And includes a version of early stopping, given by the patience.\n",
    "    Progress bars are shown too.\n",
    "    Number of epochs are specified by the parameter epochs.\n",
    "    \n",
    "    Need to pass in all the custom components. Maybe could put them in a dictionary for cleanliness.\n",
    "    \n",
    "    Returns list of training loss, and the minimum validation loss. It also saves the best encoder, decoder and\n",
    "    regressor so they can be used. \n",
    "    \n",
    "    \"\"\"\n",
    "    \n",
    "    start = time.time()\n",
    "    history = []\n",
    "    \n",
    "        \n",
    "    @tf.function\n",
    "    def train_step(inputs):\n",
    "        with tf.GradientTape() as tape:\n",
    "            loss = model(inputs,training=True)\n",
    "            loss = tf.add_n([loss] + model.losses)\n",
    "        gradients = tape.gradient(loss, model.trainable_variables)\n",
    "        optimizer.apply_gradients(zip(gradients, model.trainable_variables))\n",
    "        return loss\n",
    "    \n",
    "    validation_loss = []\n",
    "    optimizer=keras.optimizers.Adam(learning_rate=learning_rate)\n",
    "\n",
    "    batch_loss = []\n",
    "    \n",
    "    batches_per_epoch = int(np.floor((X_train_la.shape[0] + X_train_un.shape[0])/batch_size))\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "            \n",
    "            print(\"Epoch {}/{}\".format(epoch,epochs))\n",
    "            \n",
    "            for i in range(batches_per_epoch):\n",
    "                \n",
    "                batch_x_la, batch_y_la, batch_x_un= create_batch(\n",
    "                    X_train_la, y_train_la, X_train_un,batch_size)\n",
    "\n",
    "                inputs = [batch_x_la.to_numpy(),batch_y_la,batch_x_un.to_numpy()]\n",
    "                loss = train_step(inputs)\n",
    "\n",
    "                batch_loss.append(loss)\n",
    "                \n",
    "                average_batch_loss = list_average(batch_loss)\n",
    "                \n",
    "                print_status_bar(i*batch_size,X_train_la.shape[0] + X_train_un.shape[0],average_batch_loss)\n",
    "            \n",
    "            training_loss_for_epoch = list_average(batch_loss)\n",
    "\n",
    "            batch_loss = []\n",
    "                \n",
    "            history.append(training_loss_for_epoch)\n",
    "            \n",
    "            val_loss = -validation_log_lik_sampling(y_valid_la,X_valid_la.to_numpy(),variational_decoder=variational_decoder,codings_size=codings_size)\n",
    "            \n",
    "            validation_loss.append(val_loss)\n",
    "            \n",
    "            print_status_bar_epoch(X_train_la.shape[0] + X_train_un.shape[0]\n",
    "                             ,(X_train_la.shape[0] + X_train_un.shape[0]),training_loss_for_epoch,val_loss )\n",
    "            \n",
    "            #callback for early stopping\n",
    "            \n",
    "            if epoch <= patience - 1:\n",
    "                \n",
    "                if epoch == 0:\n",
    "                \n",
    "                    variational_encoder.save(\"variational_encoder_intermediate.h5\")\n",
    "                    variational_decoder.save(\"variational_decoder_intermediate.h5\")\n",
    "                    classifier.save(\"classifier_intermediate.h5\")\n",
    "                    y_distribution.save(\"y_distribution_intermediate.h5\")\n",
    "                    \n",
    "                else:\n",
    "                    if all(val_loss<i for i in validation_loss[:-1]) is True:\n",
    "                        variational_encoder.save(\"variational_encoder_intermediate.h5\")\n",
    "                        variational_decoder.save(\"variational_decoder_intermediate.h5\")\n",
    "                        classifier.save(\"classifier_intermediate.h5\")\n",
    "                        y_distribution.save(\"y_distribution_intermediate.h5\")\n",
    "            #this statement means at least a model is saved. Because if the best model was before epoch > patience-1,\n",
    "            #then the statement below won't save any model, which is undesirable as we need to load a model. \n",
    "            \n",
    "            if epoch > patience - 1:\n",
    "                                \n",
    "                latest_val_loss = validation_loss[-patience:]\n",
    "                if all(val_loss<i for i in latest_val_loss[:-1]) is True:\n",
    "                    variational_encoder.save(\"variational_encoder_intermediate.h5\")\n",
    "                    variational_decoder.save(\"variational_decoder_intermediate.h5\")\n",
    "                    classifier.save(\"classifier_intermediate.h5\")\n",
    "                    y_distribution.save(\"y_distribution_intermediate.h5\")\n",
    "                if all(i>latest_val_loss[0] for i in latest_val_loss[1:]) is True:\n",
    "                    break     \n",
    "    \n",
    "    #load best model#\n",
    "    variational_encoder = keras.models.load_model(\"variational_encoder_intermediate.h5\", custom_objects={\n",
    "       \"Sampling\": Sampling\n",
    "    })\n",
    "    variational_decoder = keras.models.load_model(\"variational_decoder_intermediate.h5\")\n",
    "    classifier = keras.models.load_model(\"classifier_intermediate.h5\")     \n",
    "    y_distribution = keras.models.load_model(\"y_distribution_intermediate.h5\", custom_objects={\n",
    "       \"y_dist\": y_dist\n",
    "    })    \n",
    "                \n",
    "    done = time.time()\n",
    "    elapsed = done-start\n",
    "    print(\"Elapsed/s: \",elapsed)\n",
    "    print(\"Final training loss: \",training_loss_for_epoch)\n",
    "    print(\"best val loss: \", min(validation_loss))\n",
    "    \n",
    "    return history, min(validation_loss)\n",
    "\n",
    "def hyperparameter_search(param_distribs,epochs,patience,n_iter,X_train_la=X_train_omics_labelled, \n",
    "                          y_train_la=train_set_labelled_y, X_train_un=X_train_omics_unlabelled,\n",
    "                          X_valid_la=X_valid_omics, y_valid_la=valid_set_labelled_y):\n",
    "    \n",
    "    \"\"\"\n",
    "    Performs hyperparameter, random search. Assesses performance by determining the score on the validation set. \n",
    "    \n",
    "    Saves best models (encoder, decoder and regressor) and returns these. These can then be used downstream.\n",
    "    \n",
    "    Also returns dictionary of the search results.\n",
    "    \n",
    "    Param_distribs of the form: \n",
    "            param_distribs = {\n",
    "            \"n_hidden\": [1],\n",
    "            \"n_hidden_classifier\": [1],\n",
    "            \"beta\": [1],\n",
    "            \"n_neurons\": randint.rvs(50,1000-49,size=20,random_state=random_state).tolist(),\n",
    "           \"n_neurons_classifier\": randint.rvs(49,1000-49,size=20,random_state=random_state).tolist(),\n",
    "            \"codings_size\": randint.rvs(50,290-50,size=30,random_state=random_state).tolist(),\n",
    "            \"N\" :randint.rvs().tolist(),\n",
    "            \"learning_rate\" : ....\n",
    "            #\"codings_size\": [50]}\n",
    "            \n",
    "    There must be a value for every parameter. If you know the value you want to use, set it in the param_distribs\n",
    "    dictionary.\n",
    "    \n",
    "    Patience must be less than the number of epochs.\n",
    "    \n",
    "    e.g. result,variational_encoder,variational_decoder,classifier,y_distribution =\n",
    "            hyperparameter_search(param_distribs,500,10,n_iter=10)\n",
    "\n",
    "            \n",
    "    \"\"\"\n",
    "\n",
    "    np.random.seed(42) #needs to be here so that everything that follows is consistent\n",
    "\n",
    "    min_val_loss = []\n",
    "    master = {}\n",
    "\n",
    "    for i in range(n_iter): \n",
    "        K.clear_session()\n",
    "        master[i] = {}\n",
    "        master[i][\"parameters\"] = {}\n",
    "        \n",
    "        N= np.random.choice(param_distribs[\"N\"])\n",
    "        learning_rate= np.random.choice(param_distribs[\"learning_rate\"])\n",
    "        beta= np.random.choice(param_distribs[\"beta\"])\n",
    "        n_neurons =np.random.choice(param_distribs[\"n_neurons\"]) \n",
    "        n_neurons_classifier =np.random.choice(param_distribs[\"n_neurons_classifier\"]) \n",
    "        n_hidden  =np.random.choice(param_distribs[\"n_hidden\"]) \n",
    "        n_hidden_classifier  =np.random.choice(param_distribs[\"n_hidden_classifier\"]) \n",
    "        codings_size =np.random.choice(param_distribs[\"codings_size\"]) \n",
    "       \n",
    "        master[i][\"parameters\"][\"N\"] = N\n",
    "        master[i][\"parameters\"][\"learning_rate\"] = learning_rate\n",
    "        master[i][\"parameters\"][\"beta\"] = beta\n",
    "        master[i][\"parameters\"][\"n_neurons\"] = n_neurons\n",
    "        master[i][\"parameters\"][\"n_neurons_classifier\"] = n_neurons_classifier\n",
    "        master[i][\"parameters\"][\"n_hidden\"] = n_hidden\n",
    "        master[i][\"parameters\"][\"n_hidden_classifier\"] = n_hidden_classifier\n",
    "        master[i][\"parameters\"][\"codings_size\"] = codings_size\n",
    "\n",
    "        \n",
    "        variational_encoder,variational_decoder,classifier,y_distribution,model = build_model(n_hidden=n_hidden,       \n",
    "                                       n_neurons=n_neurons,beta=beta,n_hidden_classifier=n_hidden_classifier,\n",
    "                                        n_neurons_classifier=n_neurons_classifier,N=N,codings_size=codings_size)\n",
    "        \n",
    "                \n",
    "        history,val_loss = fit_model_search(X_train_la=X_train_la, y_train_la=y_train_la, \n",
    "                                 X_train_un=X_train_un, epochs=epochs,X_valid_la=X_valid_la, \n",
    "                                 y_valid_la=y_valid_la,patience=patience,variational_encoder=variational_encoder,\n",
    "                                variational_decoder=variational_decoder, classifier=classifier,\n",
    "                                y_distribution=y_distribution,model=model,Sampling=Sampling,y_dist=y_dist,\n",
    "                                            batch_size=32,learning_rate=learning_rate,codings_size=codings_size)\n",
    "        \n",
    "        \n",
    "\n",
    "        master[i][\"val_loss\"] = val_loss\n",
    "        min_val_loss.append(val_loss)\n",
    "\n",
    "        #If val loss is lowest, save this model. \n",
    "        if val_loss <=  min(min_val_loss):\n",
    "            os.rename(\"variational_encoder_intermediate.h5\",\"variational_encoder.h5\")\n",
    "            os.rename(\"variational_decoder_intermediate.h5\",\"variational_decoder.h5\")\n",
    "            os.rename(\"classifier_intermediate.h5\",\"classifier.h5\")\n",
    "            os.rename(\"y_distribution_intermediate.h5\",\"y_distribution.h5\")\n",
    "\n",
    "        print(master)\n",
    "            \n",
    "    #load best model#\n",
    "    variational_encoder = keras.models.load_model(\"variational_encoder.h5\", custom_objects={\n",
    "       \"Sampling\": Sampling\n",
    "    })\n",
    "    variational_decoder = keras.models.load_model(\"variational_decoder.h5\")\n",
    "    classifier = keras.models.load_model(\"classifier.h5\")     \n",
    "    y_distribution = keras.models.load_model(\"y_distribution.h5\", custom_objects={\n",
    "       \"y_dist\": y_dist\n",
    "    })    \n",
    "\n",
    "    result = sorted(master.items(), key=lambda item: item[1][\"val_loss\"])\n",
    "    return result,variational_encoder,variational_decoder,classifier,y_distribution\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hyperparameter Search #"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "param_distribs = {\n",
    "            \"n_hidden\": [1,2],\n",
    "            \"n_hidden_classifier\": [1,2],\n",
    "            \"beta\": [15],\n",
    "            \"n_neurons\": randint.rvs(50,600-49,size=20,random_state=random_state).tolist(),\n",
    "           \"n_neurons_classifier\": randint.rvs(20,120-20,size=20,random_state=random_state).tolist(),\n",
    "            \"codings_size\": randint.rvs(20,290-20,size=30,random_state=random_state).tolist(),\n",
    "            \"N\" :[100,110,130,150],\n",
    "            \"learning_rate\" : [0.001,0.0005,0.0001],\n",
    "            #\"codings_size\": [120,60]\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "param_distribs = {\n",
    "            \"n_hidden\": [1],\n",
    "            \"n_hidden_classifier\": [1],\n",
    "            \"beta\": [15],\n",
    "            \"n_neurons\": [516],\n",
    "           \"n_neurons_classifier\": [43],\n",
    "            \"codings_size\": [208],\n",
    "            \"N\" :[100,110,130,150],\n",
    "            \"learning_rate\" : [0.001],}\n",
    "            #\"codings_size\": [120,60]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0/500\n",
      "WARNING:tensorflow:Layer full_model is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because it's dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "2240/2278 [============================>.] - Loss for batch: 433.6157WARNING:tensorflow:Layer concatenate_1 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because it's dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "2278/2278 [==============================] - trainLoss: 433.6157  Val_loss: 15.2801 \n",
      "Epoch 1/500\n",
      "2278/2278 [==============================] - trainLoss: 154.4327  Val_loss: 13.8787 \n",
      "Epoch 2/500\n",
      "2278/2278 [==============================] - trainLoss: 112.2991  Val_loss: 13.2245 \n",
      "Epoch 3/500\n",
      "2278/2278 [==============================] - trainLoss: 88.2233  Val_loss: 12.8448 \n",
      "Epoch 4/500\n",
      "2278/2278 [==============================] - trainLoss: 74.4862  Val_loss: 12.6283 \n",
      "Epoch 5/500\n",
      "2278/2278 [==============================] - trainLoss: 63.8666  Val_loss: 12.4408 \n",
      "Epoch 6/500\n",
      "2278/2278 [==============================] - trainLoss: 57.1571  Val_loss: 12.6342 \n",
      "Epoch 7/500\n",
      "2278/2278 [==============================] - trainLoss: 52.1941  Val_loss: 12.3814 \n",
      "Epoch 8/500\n",
      "2278/2278 [==============================] - trainLoss: 48.7142  Val_loss: 12.2437 \n",
      "Epoch 9/500\n",
      "2278/2278 [==============================] - trainLoss: 44.8916  Val_loss: 12.3390 \n",
      "Epoch 10/500\n",
      "2278/2278 [==============================] - trainLoss: 42.7060  Val_loss: 12.1064 \n",
      "Epoch 11/500\n",
      "2278/2278 [==============================] - trainLoss: 41.6861  Val_loss: 12.1602 \n",
      "Epoch 12/500\n",
      "2278/2278 [==============================] - trainLoss: 38.9756  Val_loss: 12.0634 \n",
      "Epoch 13/500\n",
      "2278/2278 [==============================] - trainLoss: 37.7439  Val_loss: 11.9858 \n",
      "Epoch 14/500\n",
      "2278/2278 [==============================] - trainLoss: 36.7191  Val_loss: 12.0565 \n",
      "Epoch 15/500\n",
      "2278/2278 [==============================] - trainLoss: 35.4027  Val_loss: 12.0508 \n",
      "Epoch 16/500\n",
      "2278/2278 [==============================] - trainLoss: 35.4723  Val_loss: 11.9674 \n",
      "Epoch 17/500\n",
      "2278/2278 [==============================] - trainLoss: 33.9986  Val_loss: 12.0685 \n",
      "Epoch 18/500\n",
      "2278/2278 [==============================] - trainLoss: 34.0924  Val_loss: 12.0381 \n",
      "Epoch 19/500\n",
      "2278/2278 [==============================] - trainLoss: 32.6508  Val_loss: 12.1968 \n",
      "Epoch 20/500\n",
      "2278/2278 [==============================] - trainLoss: 33.1892  Val_loss: 12.1030 \n",
      "Epoch 21/500\n",
      "2278/2278 [==============================] - trainLoss: 31.9145  Val_loss: 12.1504 \n",
      "Epoch 22/500\n",
      "2278/2278 [==============================] - trainLoss: 31.1418  Val_loss: 12.1170 \n",
      "Epoch 23/500\n",
      "2278/2278 [==============================] - trainLoss: 29.8696  Val_loss: 12.2058 \n",
      "Epoch 24/500\n",
      "2278/2278 [==============================] - trainLoss: 31.7601  Val_loss: 12.1308 \n",
      "Epoch 25/500\n",
      "2278/2278 [==============================] - trainLoss: 30.1516  Val_loss: 12.1926 \n",
      "WARNING:tensorflow:No training configuration found in save file: the model was *not* compiled. Compile it manually.\n",
      "WARNING:tensorflow:No training configuration found in save file: the model was *not* compiled. Compile it manually.\n",
      "WARNING:tensorflow:No training configuration found in save file: the model was *not* compiled. Compile it manually.\n",
      "WARNING:tensorflow:No training configuration found in save file: the model was *not* compiled. Compile it manually.\n",
      "Elapsed/s:  30.872134685516357\n",
      "Final training loss:  tf.Tensor(30.15157, shape=(), dtype=float32)\n",
      "best val loss:  tf.Tensor(11.967422, shape=(), dtype=float32)\n",
      "{0: {'val_loss': <tf.Tensor: shape=(), dtype=float32, numpy=11.967422>, 'parameters': {'N': 130, 'beta': 15, 'n_neurons': 137, 'n_hidden': 2, 'codings_size': 169, 'learning_rate': 0.001, 'n_neurons_classifier': 72, 'n_hidden_classifier': 1}}}\n",
      "Epoch 0/500\n",
      "WARNING:tensorflow:Layer full_model is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because it's dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "2240/2278 [============================>.] - Loss for batch: 377.5970WARNING:tensorflow:Layer concatenate_1 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because it's dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "2278/2278 [==============================] - trainLoss: 377.5970  Val_loss: 16.5350 \n",
      "Epoch 1/500\n",
      "2278/2278 [==============================] - trainLoss: 158.7141  Val_loss: 15.0122 \n",
      "Epoch 2/500\n",
      "2278/2278 [==============================] - trainLoss: 126.0174  Val_loss: 14.1657 \n",
      "Epoch 3/500\n",
      "2278/2278 [==============================] - trainLoss: 104.9065  Val_loss: 13.6242 \n",
      "Epoch 4/500\n",
      "2278/2278 [==============================] - trainLoss: 90.8725  Val_loss: 13.2989 \n",
      "Epoch 5/500\n",
      "2278/2278 [==============================] - trainLoss: 81.4029  Val_loss: 13.0985 \n",
      "Epoch 6/500\n",
      "2278/2278 [==============================] - trainLoss: 72.3539  Val_loss: 12.9016 \n",
      "Epoch 7/500\n",
      "2278/2278 [==============================] - trainLoss: 65.6839  Val_loss: 12.7527 \n",
      "Epoch 8/500\n",
      "2278/2278 [==============================] - trainLoss: 60.2721  Val_loss: 12.6304 \n",
      "Epoch 9/500\n",
      "2278/2278 [==============================] - trainLoss: 57.8043  Val_loss: 12.5987 \n",
      "Epoch 10/500\n",
      "2278/2278 [==============================] - trainLoss: 54.0192  Val_loss: 12.5701 \n",
      "Epoch 11/500\n",
      "2278/2278 [==============================] - trainLoss: 51.3805  Val_loss: 12.4607 \n",
      "Epoch 12/500\n",
      "2278/2278 [==============================] - trainLoss: 48.5078  Val_loss: 12.2417 \n",
      "Epoch 13/500\n",
      "2278/2278 [==============================] - trainLoss: 46.6733  Val_loss: 12.3037 \n",
      "Epoch 14/500\n",
      "2278/2278 [==============================] - trainLoss: 44.9236  Val_loss: 12.2375 \n",
      "Epoch 15/500\n",
      "2278/2278 [==============================] - trainLoss: 43.5082  Val_loss: 12.2580 \n",
      "Epoch 16/500\n",
      "2278/2278 [==============================] - trainLoss: 42.0397  Val_loss: 12.1604 \n",
      "Epoch 17/500\n",
      "2278/2278 [==============================] - trainLoss: 40.7087  Val_loss: 12.1405 \n",
      "Epoch 18/500\n",
      "2278/2278 [==============================] - trainLoss: 39.7077  Val_loss: 12.0857 \n",
      "Epoch 19/500\n",
      "2278/2278 [==============================] - trainLoss: 38.5510  Val_loss: 12.1524 \n",
      "Epoch 20/500\n",
      "2278/2278 [==============================] - trainLoss: 38.2763  Val_loss: 12.1425 \n",
      "Epoch 21/500\n",
      "2278/2278 [==============================] - trainLoss: 36.8564  Val_loss: 12.1832 \n",
      "Epoch 22/500\n",
      "2278/2278 [==============================] - trainLoss: 36.6088  Val_loss: 12.0771 \n",
      "Epoch 23/500\n",
      "2278/2278 [==============================] - trainLoss: 36.2964  Val_loss: 12.1925 \n",
      "Epoch 24/500\n",
      "2278/2278 [==============================] - trainLoss: 34.9989  Val_loss: 12.1675 \n",
      "Epoch 25/500\n",
      "2278/2278 [==============================] - trainLoss: 34.5589  Val_loss: 12.1153 \n",
      "Epoch 26/500\n",
      "2278/2278 [==============================] - trainLoss: 34.9552  Val_loss: 12.1089 \n",
      "Epoch 27/500\n",
      "2278/2278 [==============================] - trainLoss: 34.1821  Val_loss: 12.1232 \n",
      "Epoch 28/500\n",
      "2278/2278 [==============================] - trainLoss: 32.8714  Val_loss: 12.1003 \n",
      "Epoch 29/500\n",
      "2278/2278 [==============================] - trainLoss: 32.8015  Val_loss: 12.0850 \n",
      "Epoch 30/500\n",
      "2278/2278 [==============================] - trainLoss: 32.0179  Val_loss: 12.1928 \n",
      "Epoch 31/500\n",
      "2278/2278 [==============================] - trainLoss: 32.1480  Val_loss: 12.0747 \n",
      "Epoch 32/500\n",
      "2278/2278 [==============================] - trainLoss: 32.1476  Val_loss: 12.0763 \n",
      "Epoch 33/500\n",
      "2278/2278 [==============================] - trainLoss: 31.2812  Val_loss: 12.1562 \n",
      "Epoch 34/500\n",
      "2278/2278 [==============================] - trainLoss: 30.4982  Val_loss: 12.1227 \n",
      "Epoch 35/500\n",
      "2278/2278 [==============================] - trainLoss: 30.1080  Val_loss: 12.2549 \n",
      "Epoch 36/500\n",
      "2278/2278 [==============================] - trainLoss: 31.2110  Val_loss: 12.1241 \n",
      "Epoch 37/500\n",
      "2278/2278 [==============================] - trainLoss: 30.5553  Val_loss: 12.2145 \n",
      "Epoch 38/500\n",
      "2278/2278 [==============================] - trainLoss: 30.1855  Val_loss: 12.1584 \n",
      "Epoch 39/500\n",
      "2278/2278 [==============================] - trainLoss: 30.3831  Val_loss: 12.1839 \n",
      "Epoch 40/500\n",
      "2278/2278 [==============================] - trainLoss: 29.6890  Val_loss: 12.1041 \n",
      "WARNING:tensorflow:No training configuration found in save file: the model was *not* compiled. Compile it manually.\n",
      "WARNING:tensorflow:No training configuration found in save file: the model was *not* compiled. Compile it manually.\n",
      "WARNING:tensorflow:No training configuration found in save file: the model was *not* compiled. Compile it manually.\n",
      "WARNING:tensorflow:No training configuration found in save file: the model was *not* compiled. Compile it manually.\n",
      "Elapsed/s:  44.672207832336426\n",
      "Final training loss:  tf.Tensor(29.689003, shape=(), dtype=float32)\n",
      "best val loss:  tf.Tensor(12.074698, shape=(), dtype=float32)\n",
      "{0: {'val_loss': <tf.Tensor: shape=(), dtype=float32, numpy=11.967422>, 'parameters': {'N': 130, 'beta': 15, 'n_neurons': 137, 'n_hidden': 2, 'codings_size': 169, 'learning_rate': 0.001, 'n_neurons_classifier': 72, 'n_hidden_classifier': 1}}, 1: {'val_loss': <tf.Tensor: shape=(), dtype=float32, numpy=12.074698>, 'parameters': {'N': 130, 'beta': 15, 'n_neurons': 152, 'n_hidden': 2, 'codings_size': 72, 'learning_rate': 0.0005, 'n_neurons_classifier': 57, 'n_hidden_classifier': 1}}}\n",
      "Epoch 0/500\n",
      "WARNING:tensorflow:Layer full_model is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because it's dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "2240/2278 [============================>.] - Loss for batch: 382.7128WARNING:tensorflow:Layer concatenate_1 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because it's dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "2278/2278 [==============================] - trainLoss: 382.7128  Val_loss: 16.5226 \n",
      "Epoch 1/500\n",
      "2278/2278 [==============================] - trainLoss: 154.6938  Val_loss: 14.8613 \n",
      "Epoch 2/500\n",
      "2278/2278 [==============================] - trainLoss: 125.8722  Val_loss: 13.9574 \n",
      "Epoch 3/500\n",
      "2278/2278 [==============================] - trainLoss: 106.0317  Val_loss: 13.3556 \n",
      "Epoch 4/500\n",
      "2278/2278 [==============================] - trainLoss: 90.1557  Val_loss: 13.1315 \n",
      "Epoch 5/500\n",
      "2278/2278 [==============================] - trainLoss: 79.9312  Val_loss: 13.0445 \n",
      "Epoch 6/500\n",
      "2278/2278 [==============================] - trainLoss: 73.7506  Val_loss: 12.7651 \n",
      "Epoch 7/500\n",
      "2278/2278 [==============================] - trainLoss: 66.1970  Val_loss: 12.6697 \n",
      "Epoch 8/500\n",
      "2278/2278 [==============================] - trainLoss: 61.7697  Val_loss: 12.6394 \n",
      "Epoch 9/500\n",
      "2278/2278 [==============================] - trainLoss: 59.2923  Val_loss: 12.4787 \n",
      "Epoch 10/500\n",
      "2278/2278 [==============================] - trainLoss: 54.0045  Val_loss: 12.4248 \n",
      "Epoch 11/500\n",
      "2278/2278 [==============================] - trainLoss: 52.1721  Val_loss: 12.3140 \n",
      "Epoch 12/500\n",
      "2278/2278 [==============================] - trainLoss: 50.3699  Val_loss: 12.3076 \n",
      "Epoch 13/500\n",
      "2278/2278 [==============================] - trainLoss: 47.0992  Val_loss: 12.1893 \n",
      "Epoch 14/500\n",
      "2278/2278 [==============================] - trainLoss: 45.6689  Val_loss: 12.1845 \n",
      "Epoch 15/500\n",
      "2278/2278 [==============================] - trainLoss: 44.5231  Val_loss: 12.2031 \n",
      "Epoch 16/500\n",
      "2278/2278 [==============================] - trainLoss: 41.5653  Val_loss: 12.0293 \n",
      "Epoch 17/500\n",
      "2278/2278 [==============================] - trainLoss: 41.0773  Val_loss: 12.0052 \n",
      "Epoch 18/500\n",
      "2278/2278 [==============================] - trainLoss: 40.4186  Val_loss: 11.9456 \n",
      "Epoch 19/500\n",
      "2278/2278 [==============================] - trainLoss: 39.7336  Val_loss: 12.0845 \n",
      "Epoch 20/500\n",
      "2278/2278 [==============================] - trainLoss: 38.5542  Val_loss: 12.0849 \n",
      "Epoch 21/500\n",
      "2278/2278 [==============================] - trainLoss: 38.2715  Val_loss: 12.0504 \n",
      "Epoch 22/500\n",
      "2278/2278 [==============================] - trainLoss: 37.4588  Val_loss: 12.0043 \n",
      "Epoch 23/500\n",
      "2278/2278 [==============================] - trainLoss: 36.7127  Val_loss: 12.0227 \n",
      "Epoch 24/500\n",
      "2278/2278 [==============================] - trainLoss: 35.9847  Val_loss: 12.0358 \n",
      "Epoch 25/500\n",
      "2278/2278 [==============================] - trainLoss: 35.2575  Val_loss: 11.9776 \n",
      "Epoch 26/500\n",
      "2278/2278 [==============================] - trainLoss: 34.0259  Val_loss: 12.0222 \n",
      "Epoch 27/500\n",
      "2278/2278 [==============================] - trainLoss: 34.1812  Val_loss: 12.0250 \n",
      "WARNING:tensorflow:No training configuration found in save file: the model was *not* compiled. Compile it manually.\n",
      "WARNING:tensorflow:No training configuration found in save file: the model was *not* compiled. Compile it manually.\n",
      "WARNING:tensorflow:No training configuration found in save file: the model was *not* compiled. Compile it manually.\n",
      "WARNING:tensorflow:No training configuration found in save file: the model was *not* compiled. Compile it manually.\n",
      "Elapsed/s:  29.869322538375854\n",
      "Final training loss:  tf.Tensor(34.18119, shape=(), dtype=float32)\n",
      "best val loss:  tf.Tensor(11.945568, shape=(), dtype=float32)\n",
      "{0: {'val_loss': <tf.Tensor: shape=(), dtype=float32, numpy=11.967422>, 'parameters': {'N': 130, 'beta': 15, 'n_neurons': 137, 'n_hidden': 2, 'codings_size': 169, 'learning_rate': 0.001, 'n_neurons_classifier': 72, 'n_hidden_classifier': 1}}, 1: {'val_loss': <tf.Tensor: shape=(), dtype=float32, numpy=12.074698>, 'parameters': {'N': 130, 'beta': 15, 'n_neurons': 152, 'n_hidden': 2, 'codings_size': 72, 'learning_rate': 0.0005, 'n_neurons_classifier': 57, 'n_hidden_classifier': 1}}, 2: {'val_loss': <tf.Tensor: shape=(), dtype=float32, numpy=11.945568>, 'parameters': {'N': 130, 'beta': 15, 'n_neurons': 149, 'n_hidden': 2, 'codings_size': 72, 'learning_rate': 0.0005, 'n_neurons_classifier': 52, 'n_hidden_classifier': 1}}}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0/500\n",
      "WARNING:tensorflow:Layer full_model is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because it's dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "2240/2278 [============================>.] - Loss for batch: 231.8085WARNING:tensorflow:Layer concatenate_1 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because it's dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "2278/2278 [==============================] - trainLoss: 231.8085  Val_loss: 15.9573 \n",
      "Epoch 1/500\n",
      "2278/2278 [==============================] - trainLoss: 106.7847  Val_loss: 14.7248 \n",
      "Epoch 2/500\n",
      "2278/2278 [==============================] - trainLoss: 83.1892  Val_loss: 13.8776 \n",
      "Epoch 3/500\n",
      "2278/2278 [==============================] - trainLoss: 69.8050  Val_loss: 13.2662 \n",
      "Epoch 4/500\n",
      "2278/2278 [==============================] - trainLoss: 60.8868  Val_loss: 12.8962 \n",
      "Epoch 5/500\n",
      "2278/2278 [==============================] - trainLoss: 55.4171  Val_loss: 12.7554 \n",
      "Epoch 6/500\n",
      "2278/2278 [==============================] - trainLoss: 50.6817  Val_loss: 12.5796 \n",
      "Epoch 7/500\n",
      "2278/2278 [==============================] - trainLoss: 47.2517  Val_loss: 12.4357 \n",
      "Epoch 8/500\n",
      "2278/2278 [==============================] - trainLoss: 43.9346  Val_loss: 12.2519 \n",
      "Epoch 9/500\n",
      "2278/2278 [==============================] - trainLoss: 41.7069  Val_loss: 12.2119 \n",
      "Epoch 10/500\n",
      "2278/2278 [==============================] - trainLoss: 39.5025  Val_loss: 12.1452 \n",
      "Epoch 11/500\n",
      "2278/2278 [==============================] - trainLoss: 39.1798  Val_loss: 12.1235 \n",
      "Epoch 12/500\n",
      "2278/2278 [==============================] - trainLoss: 38.1988  Val_loss: 12.0283 \n",
      "Epoch 13/500\n",
      "2278/2278 [==============================] - trainLoss: 36.0725  Val_loss: 12.0741 \n",
      "Epoch 14/500\n",
      "2278/2278 [==============================] - trainLoss: 35.0856  Val_loss: 12.0772 \n",
      "Epoch 15/500\n",
      "2278/2278 [==============================] - trainLoss: 35.2570  Val_loss: 12.1111 \n",
      "Epoch 16/500\n",
      "2278/2278 [==============================] - trainLoss: 34.6928  Val_loss: 12.0235 \n",
      "Epoch 17/500\n",
      "2278/2278 [==============================] - trainLoss: 34.6148  Val_loss: 12.0630 \n",
      "Epoch 18/500\n",
      "2278/2278 [==============================] - trainLoss: 31.9964  Val_loss: 12.1333 \n",
      "Epoch 19/500\n",
      "2278/2278 [==============================] - trainLoss: 33.0322  Val_loss: 12.1414 \n",
      "Epoch 20/500\n",
      "2278/2278 [==============================] - trainLoss: 31.7791  Val_loss: 12.1718 \n",
      "Epoch 21/500\n",
      "2278/2278 [==============================] - trainLoss: 31.6881  Val_loss: 12.0641 \n",
      "Epoch 22/500\n",
      "2278/2278 [==============================] - trainLoss: 32.1400  Val_loss: 12.1065 \n",
      "Epoch 23/500\n",
      "2278/2278 [==============================] - trainLoss: 32.2341  Val_loss: 12.1251 \n",
      "Epoch 24/500\n",
      "2278/2278 [==============================] - trainLoss: 30.8383  Val_loss: 12.0680 \n",
      "Epoch 25/500\n",
      "2278/2278 [==============================] - trainLoss: 30.8597  Val_loss: 11.9284 \n",
      "Epoch 26/500\n",
      "2278/2278 [==============================] - trainLoss: 30.5728  Val_loss: 12.1566 \n",
      "Epoch 27/500\n",
      "2278/2278 [==============================] - trainLoss: 30.2211  Val_loss: 12.0482 \n",
      "Epoch 28/500\n",
      "2278/2278 [==============================] - trainLoss: 30.2865  Val_loss: 12.1512 \n",
      "Epoch 29/500\n",
      "2278/2278 [==============================] - trainLoss: 28.8690  Val_loss: 12.1569 \n",
      "Epoch 30/500\n",
      "2278/2278 [==============================] - trainLoss: 29.1595  Val_loss: 12.0745 \n",
      "Epoch 31/500\n",
      "2278/2278 [==============================] - trainLoss: 29.4283  Val_loss: 12.0242 \n",
      "Epoch 32/500\n",
      "2278/2278 [==============================] - trainLoss: 28.7211  Val_loss: 12.0918 \n",
      "Epoch 33/500\n",
      "2278/2278 [==============================] - trainLoss: 29.6153  Val_loss: 12.1623 \n",
      "Epoch 34/500\n",
      "2278/2278 [==============================] - trainLoss: 28.4450  Val_loss: 12.0329 \n",
      "WARNING:tensorflow:No training configuration found in save file: the model was *not* compiled. Compile it manually.\n",
      "WARNING:tensorflow:No training configuration found in save file: the model was *not* compiled. Compile it manually.\n",
      "WARNING:tensorflow:No training configuration found in save file: the model was *not* compiled. Compile it manually.\n",
      "WARNING:tensorflow:No training configuration found in save file: the model was *not* compiled. Compile it manually.\n",
      "Elapsed/s:  37.312403202056885\n",
      "Final training loss:  tf.Tensor(28.444973, shape=(), dtype=float32)\n",
      "best val loss:  tf.Tensor(11.928445, shape=(), dtype=float32)\n",
      "{0: {'val_loss': <tf.Tensor: shape=(), dtype=float32, numpy=11.967422>, 'parameters': {'N': 130, 'beta': 15, 'n_neurons': 137, 'n_hidden': 2, 'codings_size': 169, 'learning_rate': 0.001, 'n_neurons_classifier': 72, 'n_hidden_classifier': 1}}, 1: {'val_loss': <tf.Tensor: shape=(), dtype=float32, numpy=12.074698>, 'parameters': {'N': 130, 'beta': 15, 'n_neurons': 152, 'n_hidden': 2, 'codings_size': 72, 'learning_rate': 0.0005, 'n_neurons_classifier': 57, 'n_hidden_classifier': 1}}, 2: {'val_loss': <tf.Tensor: shape=(), dtype=float32, numpy=11.945568>, 'parameters': {'N': 130, 'beta': 15, 'n_neurons': 149, 'n_hidden': 2, 'codings_size': 72, 'learning_rate': 0.0005, 'n_neurons_classifier': 52, 'n_hidden_classifier': 1}}, 3: {'val_loss': <tf.Tensor: shape=(), dtype=float32, numpy=11.928445>, 'parameters': {'N': 130, 'beta': 15, 'n_neurons': 149, 'n_hidden': 1, 'codings_size': 211, 'learning_rate': 0.001, 'n_neurons_classifier': 49, 'n_hidden_classifier': 2}}}\n",
      "Epoch 0/500\n",
      "WARNING:tensorflow:Layer full_model is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because it's dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "2240/2278 [============================>.] - Loss for batch: 274.4808WARNING:tensorflow:Layer concatenate_1 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because it's dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "2278/2278 [==============================] - trainLoss: 274.4808  Val_loss: 17.3755 \n",
      "Epoch 1/500\n",
      "2278/2278 [==============================] - trainLoss: 107.8470  Val_loss: 15.1740 \n",
      "Epoch 2/500\n",
      "2278/2278 [==============================] - trainLoss: 91.5400  Val_loss: 14.6098 \n",
      "Epoch 3/500\n",
      "2278/2278 [==============================] - trainLoss: 80.9309  Val_loss: 14.2354 \n",
      "Epoch 4/500\n",
      "2278/2278 [==============================] - trainLoss: 72.9152  Val_loss: 13.9054 \n",
      "Epoch 5/500\n",
      "2278/2278 [==============================] - trainLoss: 66.4644  Val_loss: 13.5493 \n",
      "Epoch 6/500\n",
      "2278/2278 [==============================] - trainLoss: 61.4245  Val_loss: 13.3498 \n",
      "Epoch 7/500\n",
      "2278/2278 [==============================] - trainLoss: 57.1256  Val_loss: 13.1582 \n",
      "Epoch 8/500\n",
      "2278/2278 [==============================] - trainLoss: 53.4127  Val_loss: 12.9847 \n",
      "Epoch 9/500\n",
      "2278/2278 [==============================] - trainLoss: 50.3646  Val_loss: 12.8725 \n",
      "Epoch 10/500\n",
      "2278/2278 [==============================] - trainLoss: 49.1161  Val_loss: 12.6954 \n",
      "Epoch 11/500\n",
      "2278/2278 [==============================] - trainLoss: 46.1403  Val_loss: 12.5536 \n",
      "Epoch 12/500\n",
      "2278/2278 [==============================] - trainLoss: 44.7812  Val_loss: 12.5484 \n",
      "Epoch 13/500\n",
      "2278/2278 [==============================] - trainLoss: 42.8427  Val_loss: 12.5017 \n",
      "Epoch 14/500\n",
      "2278/2278 [==============================] - trainLoss: 41.3152  Val_loss: 12.3751 \n",
      "Epoch 15/500\n",
      "2278/2278 [==============================] - trainLoss: 39.7553  Val_loss: 12.3498 \n",
      "Epoch 16/500\n",
      "2278/2278 [==============================] - trainLoss: 38.5655  Val_loss: 12.3028 \n",
      "Epoch 17/500\n",
      "2278/2278 [==============================] - trainLoss: 38.2573  Val_loss: 12.2511 \n",
      "Epoch 18/500\n",
      "2278/2278 [==============================] - trainLoss: 36.7917  Val_loss: 12.1655 \n",
      "Epoch 19/500\n",
      "2278/2278 [==============================] - trainLoss: 35.5181  Val_loss: 12.1811 \n",
      "Epoch 20/500\n",
      "2278/2278 [==============================] - trainLoss: 34.7124  Val_loss: 12.1390 \n",
      "Epoch 21/500\n",
      "2278/2278 [==============================] - trainLoss: 34.2305  Val_loss: 12.0926 \n",
      "Epoch 22/500\n",
      "2278/2278 [==============================] - trainLoss: 33.5107  Val_loss: 12.1290 \n",
      "Epoch 23/500\n",
      "2278/2278 [==============================] - trainLoss: 32.0933  Val_loss: 12.0586 \n",
      "Epoch 24/500\n",
      "2278/2278 [==============================] - trainLoss: 32.2747  Val_loss: 12.0548 \n",
      "Epoch 25/500\n",
      "2278/2278 [==============================] - trainLoss: 31.8828  Val_loss: 12.0455 \n",
      "Epoch 26/500\n",
      "2278/2278 [==============================] - trainLoss: 30.8719  Val_loss: 12.0384 \n",
      "Epoch 27/500\n",
      "2278/2278 [==============================] - trainLoss: 30.4783  Val_loss: 12.0903 \n",
      "Epoch 28/500\n",
      "2278/2278 [==============================] - trainLoss: 30.9360  Val_loss: 12.1493 \n",
      "Epoch 29/500\n",
      "2278/2278 [==============================] - trainLoss: 29.5241  Val_loss: 12.1324 \n",
      "Epoch 30/500\n",
      "2278/2278 [==============================] - trainLoss: 29.4625  Val_loss: 12.1151 \n",
      "Epoch 31/500\n",
      "2278/2278 [==============================] - trainLoss: 29.4284  Val_loss: 12.1049 \n",
      "Epoch 32/500\n",
      "2278/2278 [==============================] - trainLoss: 29.1831  Val_loss: 12.1102 \n",
      "Epoch 33/500\n",
      "2278/2278 [==============================] - trainLoss: 28.4380  Val_loss: 12.0810 \n",
      "Epoch 34/500\n",
      "2278/2278 [==============================] - trainLoss: 28.1572  Val_loss: 12.1007 \n",
      "Epoch 35/500\n",
      "2278/2278 [==============================] - trainLoss: 27.5643  Val_loss: 12.0884 \n",
      "WARNING:tensorflow:No training configuration found in save file: the model was *not* compiled. Compile it manually.\n",
      "WARNING:tensorflow:No training configuration found in save file: the model was *not* compiled. Compile it manually.\n",
      "WARNING:tensorflow:No training configuration found in save file: the model was *not* compiled. Compile it manually.\n",
      "WARNING:tensorflow:No training configuration found in save file: the model was *not* compiled. Compile it manually.\n",
      "Elapsed/s:  36.09855031967163\n",
      "Final training loss:  tf.Tensor(27.564262, shape=(), dtype=float32)\n",
      "best val loss:  tf.Tensor(12.038363, shape=(), dtype=float32)\n",
      "{0: {'val_loss': <tf.Tensor: shape=(), dtype=float32, numpy=11.967422>, 'parameters': {'N': 130, 'beta': 15, 'n_neurons': 137, 'n_hidden': 2, 'codings_size': 169, 'learning_rate': 0.001, 'n_neurons_classifier': 72, 'n_hidden_classifier': 1}}, 1: {'val_loss': <tf.Tensor: shape=(), dtype=float32, numpy=12.074698>, 'parameters': {'N': 130, 'beta': 15, 'n_neurons': 152, 'n_hidden': 2, 'codings_size': 72, 'learning_rate': 0.0005, 'n_neurons_classifier': 57, 'n_hidden_classifier': 1}}, 2: {'val_loss': <tf.Tensor: shape=(), dtype=float32, numpy=11.945568>, 'parameters': {'N': 130, 'beta': 15, 'n_neurons': 149, 'n_hidden': 2, 'codings_size': 72, 'learning_rate': 0.0005, 'n_neurons_classifier': 52, 'n_hidden_classifier': 1}}, 3: {'val_loss': <tf.Tensor: shape=(), dtype=float32, numpy=11.928445>, 'parameters': {'N': 130, 'beta': 15, 'n_neurons': 149, 'n_hidden': 1, 'codings_size': 211, 'learning_rate': 0.001, 'n_neurons_classifier': 49, 'n_hidden_classifier': 2}}, 4: {'val_loss': <tf.Tensor: shape=(), dtype=float32, numpy=12.038363>, 'parameters': {'N': 100, 'beta': 15, 'n_neurons': 149, 'n_hidden': 1, 'codings_size': 107, 'learning_rate': 0.0005, 'n_neurons_classifier': 21, 'n_hidden_classifier': 2}}}\n",
      "WARNING:tensorflow:No training configuration found in save file: the model was *not* compiled. Compile it manually.\n",
      "WARNING:tensorflow:No training configuration found in save file: the model was *not* compiled. Compile it manually.\n",
      "WARNING:tensorflow:No training configuration found in save file: the model was *not* compiled. Compile it manually.\n",
      "WARNING:tensorflow:No training configuration found in save file: the model was *not* compiled. Compile it manually.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[(3,\n",
       "  {'parameters': {'N': 130,\n",
       "    'beta': 15,\n",
       "    'codings_size': 211,\n",
       "    'learning_rate': 0.001,\n",
       "    'n_hidden': 1,\n",
       "    'n_hidden_classifier': 2,\n",
       "    'n_neurons': 149,\n",
       "    'n_neurons_classifier': 49},\n",
       "   'val_loss': <tf.Tensor: shape=(), dtype=float32, numpy=11.928445>}),\n",
       " (2,\n",
       "  {'parameters': {'N': 130,\n",
       "    'beta': 15,\n",
       "    'codings_size': 72,\n",
       "    'learning_rate': 0.0005,\n",
       "    'n_hidden': 2,\n",
       "    'n_hidden_classifier': 1,\n",
       "    'n_neurons': 149,\n",
       "    'n_neurons_classifier': 52},\n",
       "   'val_loss': <tf.Tensor: shape=(), dtype=float32, numpy=11.945568>}),\n",
       " (0,\n",
       "  {'parameters': {'N': 130,\n",
       "    'beta': 15,\n",
       "    'codings_size': 169,\n",
       "    'learning_rate': 0.001,\n",
       "    'n_hidden': 2,\n",
       "    'n_hidden_classifier': 1,\n",
       "    'n_neurons': 137,\n",
       "    'n_neurons_classifier': 72},\n",
       "   'val_loss': <tf.Tensor: shape=(), dtype=float32, numpy=11.967422>}),\n",
       " (4,\n",
       "  {'parameters': {'N': 100,\n",
       "    'beta': 15,\n",
       "    'codings_size': 107,\n",
       "    'learning_rate': 0.0005,\n",
       "    'n_hidden': 1,\n",
       "    'n_hidden_classifier': 2,\n",
       "    'n_neurons': 149,\n",
       "    'n_neurons_classifier': 21},\n",
       "   'val_loss': <tf.Tensor: shape=(), dtype=float32, numpy=12.038363>}),\n",
       " (1,\n",
       "  {'parameters': {'N': 130,\n",
       "    'beta': 15,\n",
       "    'codings_size': 72,\n",
       "    'learning_rate': 0.0005,\n",
       "    'n_hidden': 2,\n",
       "    'n_hidden_classifier': 1,\n",
       "    'n_neurons': 152,\n",
       "    'n_neurons_classifier': 57},\n",
       "   'val_loss': <tf.Tensor: shape=(), dtype=float32, numpy=12.074698>})]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result,variational_encoder,variational_decoder,classifier,y_distribution = hyperparameter_search(param_distribs=param_distribs,\n",
    "                                                                        epochs=500,patience=10,n_iter=5)\n",
    "result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Best KL model has val nloglik of 11.925144\n",
    "\n",
    "  {'parameters': {'N': 100,\n",
    "    'beta': 15,\n",
    "    'codings_size': 122,\n",
    "    'learning_rate': 0.0005,\n",
    "    'n_hidden': 2,\n",
    "    'n_hidden_classifier': 1,\n",
    "    'n_neurons': 152,\n",
    "    'n_neurons_classifier': 21},\n",
    "   'val_loss': <tf.Tensor: shape=(), dtype=float32, numpy=11.928445>}),"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Single run # "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note: we don't train on train+val set etc. as the N parameter is based on the balance between number of labelled vs unlabelled points. If you did want to train on all data, then you'd just train on everything and change the N parameter appropriately. Though you may want to check that the N is related to the ratio and not anything else."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "variational_encoder,variational_decoder,classifier,y_distribution,model = build_model(n_hidden=2, n_neurons=152,input_shape=input_shape,beta=15,n_hidden_classifier=1,\n",
    "              n_neurons_classifier=21,N=100,codings_size=122)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0/100\n",
      "WARNING:tensorflow:Layer full_model is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because it's dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "2240/2278 [============================>.] - Loss for batch: 536.6541WARNING:tensorflow:Layer concatenate_1 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because it's dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "2278/2278 [==============================] - trainLoss: 536.6541  Val_loss: 17.0475 \n",
      "Epoch 1/100\n",
      "2278/2278 [==============================] - trainLoss: 192.8434  Val_loss: 15.3753 \n",
      "Epoch 2/100\n",
      "2278/2278 [==============================] - trainLoss: 149.8948  Val_loss: 14.3035 \n",
      "Epoch 3/100\n",
      "2278/2278 [==============================] - trainLoss: 124.6395  Val_loss: 13.7487 \n",
      "Epoch 4/100\n",
      "2278/2278 [==============================] - trainLoss: 104.5209  Val_loss: 13.4099 \n",
      "Epoch 5/100\n",
      "2278/2278 [==============================] - trainLoss: 92.4024  Val_loss: 13.1206 \n",
      "Epoch 6/100\n",
      "2278/2278 [==============================] - trainLoss: 81.9308  Val_loss: 12.9337 \n",
      "Epoch 7/100\n",
      "2278/2278 [==============================] - trainLoss: 74.5147  Val_loss: 12.9178 \n",
      "Epoch 8/100\n",
      "2278/2278 [==============================] - trainLoss: 68.2405  Val_loss: 12.7821 \n",
      "Epoch 9/100\n",
      "2278/2278 [==============================] - trainLoss: 61.0302  Val_loss: 12.7209 \n",
      "Epoch 10/100\n",
      "2278/2278 [==============================] - trainLoss: 58.3886  Val_loss: 12.5880 \n",
      "Epoch 11/100\n",
      "2278/2278 [==============================] - trainLoss: 53.5902  Val_loss: 12.5341 \n",
      "Epoch 12/100\n",
      "2278/2278 [==============================] - trainLoss: 50.3206  Val_loss: 12.4671 \n",
      "Epoch 13/100\n",
      "2278/2278 [==============================] - trainLoss: 48.1513  Val_loss: 12.5240 \n",
      "Epoch 14/100\n",
      "2278/2278 [==============================] - trainLoss: 46.1768  Val_loss: 12.2714 \n",
      "Epoch 15/100\n",
      "2278/2278 [==============================] - trainLoss: 43.7206  Val_loss: 12.3184 \n",
      "Epoch 16/100\n",
      "2278/2278 [==============================] - trainLoss: 42.0573  Val_loss: 12.2342 \n",
      "Epoch 17/100\n",
      "2278/2278 [==============================] - trainLoss: 40.3748  Val_loss: 12.1457 \n",
      "Epoch 18/100\n",
      "2278/2278 [==============================] - trainLoss: 38.8522  Val_loss: 12.2266 \n",
      "Epoch 19/100\n",
      "2278/2278 [==============================] - trainLoss: 38.2167  Val_loss: 12.2893 \n",
      "Epoch 20/100\n",
      "2278/2278 [==============================] - trainLoss: 37.0453  Val_loss: 12.1212 \n",
      "Epoch 21/100\n",
      "2278/2278 [==============================] - trainLoss: 35.7212  Val_loss: 12.1501 \n",
      "Epoch 22/100\n",
      "2278/2278 [==============================] - trainLoss: 34.7321  Val_loss: 12.1025 \n",
      "Epoch 23/100\n",
      "2278/2278 [==============================] - trainLoss: 33.3947  Val_loss: 12.0889 \n",
      "Epoch 24/100\n",
      "2278/2278 [==============================] - trainLoss: 32.6856  Val_loss: 12.0580 \n",
      "Epoch 25/100\n",
      "2278/2278 [==============================] - trainLoss: 31.8856  Val_loss: 11.9275 \n",
      "Epoch 26/100\n",
      "2278/2278 [==============================] - trainLoss: 31.7996  Val_loss: 11.9765 \n",
      "Epoch 27/100\n",
      "2278/2278 [==============================] - trainLoss: 31.7431  Val_loss: 12.0763 \n",
      "Epoch 28/100\n",
      "2278/2278 [==============================] - trainLoss: 31.3949  Val_loss: 12.0738 \n",
      "Epoch 29/100\n",
      "2278/2278 [==============================] - trainLoss: 29.9813  Val_loss: 12.0808 \n",
      "Epoch 30/100\n",
      "2278/2278 [==============================] - trainLoss: 30.6935  Val_loss: 12.1366 \n",
      "Epoch 31/100\n",
      "2278/2278 [==============================] - trainLoss: 30.0371  Val_loss: 11.9726 \n",
      "Epoch 32/100\n",
      "2278/2278 [==============================] - trainLoss: 29.0929  Val_loss: 12.0162 \n",
      "Epoch 33/100\n",
      "2278/2278 [==============================] - trainLoss: 29.2925  Val_loss: 11.9871 \n",
      "Epoch 34/100\n",
      "2278/2278 [==============================] - trainLoss: 28.6760  Val_loss: 12.0712 \n",
      "WARNING:tensorflow:No training configuration found in save file: the model was *not* compiled. Compile it manually.\n",
      "WARNING:tensorflow:No training configuration found in save file: the model was *not* compiled. Compile it manually.\n",
      "WARNING:tensorflow:No training configuration found in save file: the model was *not* compiled. Compile it manually.\n",
      "WARNING:tensorflow:No training configuration found in save file: the model was *not* compiled. Compile it manually.\n",
      "Elapsed/s:  40.37829327583313\n",
      "Final training loss:  tf.Tensor(28.675982, shape=(), dtype=float32)\n",
      "best val loss:  tf.Tensor(11.92746, shape=(), dtype=float32)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "([<tf.Tensor: shape=(), dtype=float32, numpy=536.65405>,\n",
       "  <tf.Tensor: shape=(), dtype=float32, numpy=192.84341>,\n",
       "  <tf.Tensor: shape=(), dtype=float32, numpy=149.89479>,\n",
       "  <tf.Tensor: shape=(), dtype=float32, numpy=124.63955>,\n",
       "  <tf.Tensor: shape=(), dtype=float32, numpy=104.52094>,\n",
       "  <tf.Tensor: shape=(), dtype=float32, numpy=92.40239>,\n",
       "  <tf.Tensor: shape=(), dtype=float32, numpy=81.930756>,\n",
       "  <tf.Tensor: shape=(), dtype=float32, numpy=74.514656>,\n",
       "  <tf.Tensor: shape=(), dtype=float32, numpy=68.24049>,\n",
       "  <tf.Tensor: shape=(), dtype=float32, numpy=61.030224>,\n",
       "  <tf.Tensor: shape=(), dtype=float32, numpy=58.388588>,\n",
       "  <tf.Tensor: shape=(), dtype=float32, numpy=53.590218>,\n",
       "  <tf.Tensor: shape=(), dtype=float32, numpy=50.32064>,\n",
       "  <tf.Tensor: shape=(), dtype=float32, numpy=48.151314>,\n",
       "  <tf.Tensor: shape=(), dtype=float32, numpy=46.176765>,\n",
       "  <tf.Tensor: shape=(), dtype=float32, numpy=43.72063>,\n",
       "  <tf.Tensor: shape=(), dtype=float32, numpy=42.05734>,\n",
       "  <tf.Tensor: shape=(), dtype=float32, numpy=40.374775>,\n",
       "  <tf.Tensor: shape=(), dtype=float32, numpy=38.85224>,\n",
       "  <tf.Tensor: shape=(), dtype=float32, numpy=38.216663>,\n",
       "  <tf.Tensor: shape=(), dtype=float32, numpy=37.04531>,\n",
       "  <tf.Tensor: shape=(), dtype=float32, numpy=35.72121>,\n",
       "  <tf.Tensor: shape=(), dtype=float32, numpy=34.732063>,\n",
       "  <tf.Tensor: shape=(), dtype=float32, numpy=33.39469>,\n",
       "  <tf.Tensor: shape=(), dtype=float32, numpy=32.685566>,\n",
       "  <tf.Tensor: shape=(), dtype=float32, numpy=31.885553>,\n",
       "  <tf.Tensor: shape=(), dtype=float32, numpy=31.799637>,\n",
       "  <tf.Tensor: shape=(), dtype=float32, numpy=31.74312>,\n",
       "  <tf.Tensor: shape=(), dtype=float32, numpy=31.39493>,\n",
       "  <tf.Tensor: shape=(), dtype=float32, numpy=29.981253>,\n",
       "  <tf.Tensor: shape=(), dtype=float32, numpy=30.693455>,\n",
       "  <tf.Tensor: shape=(), dtype=float32, numpy=30.037127>,\n",
       "  <tf.Tensor: shape=(), dtype=float32, numpy=29.092922>,\n",
       "  <tf.Tensor: shape=(), dtype=float32, numpy=29.292507>,\n",
       "  <tf.Tensor: shape=(), dtype=float32, numpy=28.675982>],\n",
       " <tf.Tensor: shape=(), dtype=float32, numpy=11.92746>)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fit_model(X_train_omics_labelled, train_set_labelled_y, X_train_omics_unlabelled,100,X_valid_omics, valid_set_labelled_y,\n",
    "              10,variational_encoder=variational_encoder,variational_decoder=variational_decoder,\n",
    "             classifier=classifier,y_distribution=y_distribution,model=model,\n",
    "          Sampling=Sampling,y_dist=y_dist,batch_size=32,learning_rate=0.0005,valid_set=True,codings_size=122)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test_nlog_lik = tf.Tensor(12.677079, shape=(), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "test_nlog_lik = -validation_log_lik_sampling(test_set_labelled_y,X_test_omics.to_numpy(),\n",
    "                                    variational_decoder=variational_decoder,codings_size=122,samples=2000)\n",
    "print(\"test_nlog_lik = \" + str(test_nlog_lik))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3.9% better than just labelled data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
