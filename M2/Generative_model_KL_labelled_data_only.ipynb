{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "assert sys.version_info >= (3, 5)\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "assert tf.__version__ >= \"2.0\"\n",
    "import numpy as np\n",
    "import time\n",
    "K = keras.backend\n",
    "import pandas as pd\n",
    "import math\n",
    "import os\n",
    "from sklearn.metrics import r2_score\n",
    "from sklearn.metrics import mean_squared_error as mse\n",
    "from scipy.stats import uniform,randint\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import scipy.stats\n",
    "from scipy.stats import norm\n",
    "\n",
    "\n",
    "\n",
    "tf.random.set_seed(42)\n",
    "np.random.seed(42)\n",
    "random_state=42\n",
    "\n",
    "save_path = 'save_path'\n",
    "os.chdir(save_path)\n",
    "\n",
    "X_train_omics_labelled = pd.read_csv(\"X_train_omics_labelled.csv\",index_col=0)\n",
    "X_test_omics= pd.read_csv(\"X_test_omics.csv\",index_col=0)\n",
    "X_valid_omics= pd.read_csv(\"X_valid_omics.csv\",index_col=0)\n",
    "features = np.load(\"feature_selection.npy\",allow_pickle=True)\n",
    "\n",
    "train_set_labelled_y= pd.read_csv(\"train_set_labelled_y.csv\",index_col=0)\n",
    "test_set_labelled_y= pd.read_csv(\"test_set_labelled_y.csv\",index_col=0)\n",
    "valid_set_labelled_y= pd.read_csv(\"valid_set_labelled_y.csv\",index_col=0)\n",
    "\n",
    "X_train_omics_labelled = X_train_omics_labelled[features]\n",
    "X_test_omics = X_test_omics[features]\n",
    "X_valid_omics = X_valid_omics[features]\n",
    "\n",
    "train_set_labelled_c= pd.read_csv(\"train_set_labelled_c.csv\",index_col=0)\n",
    "test_set_labelled_c= pd.read_csv(\"test_set_labelled_c.csv\",index_col=0)\n",
    "valid_set_labelled_c= pd.read_csv(\"valid_set_labelled_c.csv\",index_col=0)\n",
    "\n",
    "#bin y \n",
    "from sklearn.preprocessing import KBinsDiscretizer\n",
    "binner = KBinsDiscretizer(n_bins=10,encode=\"onehot-dense\",strategy=\"uniform\")\n",
    "train_set_labelled_y = binner.fit_transform(train_set_labelled_y)\n",
    "valid_set_labelled_y = binner.transform(valid_set_labelled_y)\n",
    "test_set_labelled_y = binner.transform(test_set_labelled_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_shape = X_train_omics_labelled.shape[1]\n",
    "\n",
    "save_model_path = 'save_model_path'\n",
    "os.chdir(save_model_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Custom parts # "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Helpful functions ## "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def validation_log_lik_sampling(y_val,x_val,variational_decoder,codings_size,samples=200):\n",
    "    \"\"\"\n",
    "    Samples a value of z for the expectation, and calculates something proportional to loglikelihood.\n",
    "    \n",
    "    The more samples of z, the better the MC approximation to loglik.\n",
    "    \n",
    "    This is how we do our evaluation on the validation and also test set. \n",
    "    \n",
    "    We look at the ability to generate x given y i.e. loglik(x|y)\"\"\"\n",
    "    \n",
    "    x_val_len = len(x_val)\n",
    "    expectation = 0\n",
    "    for i in range(samples):\n",
    "        z = np.random.normal(loc=0,scale=1,size=codings_size*x_val_len).reshape(x_val_len,codings_size)\n",
    "        x_pred = variational_decoder([z,y_val])\n",
    "        diff = (x_val-x_pred)**2\n",
    "        pdf = K.sum(diff,axis=-1)\n",
    "        pdf = K.exp(-pdf)\n",
    "        expectation += pdf \n",
    "    expectation = expectation / samples\n",
    "    lik = tf.math.log(expectation)\n",
    "    lik = K.mean(lik)    \n",
    "    return lik\n",
    "\n",
    "\n",
    "\n",
    "def create_batch_label(x_label, y_label, batch_s=32):\n",
    "    '''\n",
    "    Creates batches of labelled and unlabelled data. The total number of points in both batches is equal to batch_s. \n",
    "    \n",
    "    Thanks to Omer for help\n",
    "    '''\n",
    "    proportion_labelled = x_label.shape[0]/(x_label.shape[0])\n",
    "    \n",
    "    shape_label = x_label.shape[0]\n",
    "    label_per_batch = int(np.ceil(proportion_labelled*batch_s))\n",
    "    batch_idx_la = np.random.choice(list(range(shape_label)), label_per_batch)\n",
    "    batch_x_la = (x_label.iloc[batch_idx_la, :])\n",
    "    batch_y_la = (y_label[batch_idx_la,:])\n",
    "    \n",
    "    del batch_idx_la\n",
    "            \n",
    "    return batch_x_la, batch_y_la\n",
    "\n",
    "\n",
    "def progress_bar(iteration, total, size=30):\n",
    "    \"\"\"Progress bar for training\"\"\"\n",
    "    running = iteration < total\n",
    "    c = \">\" if running else \"=\"\n",
    "    p = (size - 1) * iteration // total\n",
    "    fmt = \"{{:-{}d}}/{{}} [{{}}]\".format(len(str(total)))\n",
    "    params = [iteration, total, \"=\" * p + c + \".\" * (size - p - 1)]\n",
    "    return fmt.format(*params)\n",
    "\n",
    "def print_status_bar(iteration, total, loss, metrics=None, size=30):\n",
    "    \"\"\"Status bar for training\"\"\"\n",
    "    metrics = \" - \".join([\"Loss for batch: {:.4f}\".format(loss)])\n",
    "    end = \"\" if iteration < total else \"\\n\"\n",
    "    print(\"\\r{} - {}\".format(progress_bar(iteration, total), metrics), end=end)\n",
    "    \n",
    "def print_status_bar_epoch(iteration, total, training_loss_for_epoch,val_loss, metrics=None, size=30):\n",
    "    \"\"\"Status bar for training (end of epoch)\"\"\"\n",
    "    metrics = \" - \".join(\n",
    "        [\"trainLoss: {:.4f}  Val_loss: {:.4f} \".format(\n",
    "            training_loss_for_epoch,val_loss)]\n",
    "    )\n",
    "    end = \"\" if iteration < total else \"\\n\"\n",
    "    print(\"\\r{} - {}\".format(progress_bar(iteration, total), metrics), end=end)\n",
    "    \n",
    "    \n",
    "def list_average(list_of_loss):\n",
    "    return sum(list_of_loss)/len(list_of_loss)\n",
    "\n",
    "\n",
    "def y_pred_loss(y_in):\n",
    "    \"\"\"Calculates loss and true y distribution given some y data.\n",
    "    \n",
    "    When the model calculates this it does it in batches (unlike this function which can take the whole data in).\n",
    "    \n",
    "    Therefore the model's learned distribution will probably not be as good as what is learnt when using the whole\n",
    "    dataset. But that is one of the things that happens if we use mini-batch gradient descent.\n",
    "    \n",
    "    \"\"\"\n",
    "    y_distribution = (K.sum(y_in,axis=0) / len(y_in))\n",
    "    loss = tf.reduce_mean(keras.losses.categorical_crossentropy(y_in,y_distribution))\n",
    "    return loss,y_distribution \n",
    "\n",
    "\n",
    "def rounded_accuracy(y_true,y_pred):\n",
    "    \"\"\"\n",
    "    Calculates accuracy of classification predictions.\n",
    "    \n",
    "    For the 10D p vector which is y_pred, it sets the highest number to 1 and the rest to 0.\n",
    "    \n",
    "    It then computes accuracy.\n",
    "    \"\"\"\n",
    "    from sklearn.metrics import accuracy_score\n",
    "    b = np.zeros_like(y_pred)\n",
    "    b[np.arange(len(y_pred)),y_pred.argmax(1)] = 1\n",
    "    return accuracy_score(y_true,b)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model ##"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Custom components ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Sampling(keras.layers.Layer):\n",
    "    \"\"\"reparameterization trick\"\"\"\n",
    "    def call(self, inputs):\n",
    "        mean, log_var = inputs\n",
    "        return K.random_normal(tf.shape(log_var)) * K.exp(log_var/2) + mean\n",
    "    \n",
    "class y_dist(keras.layers.Layer):\n",
    "    \"\"\"\n",
    "    Custom layer that is used to learn the parameters, p, of the distribution over y.\n",
    "    \n",
    "    Outputs a loss and p. The loss is used for training. The loss is the categorical cross entropy loss between \n",
    "    p and every y sample. The mean of this is then taken to provide a per batch loss. \n",
    "    \n",
    "    Shapes are configured for a 10D y.\n",
    "    \n",
    "    \"\"\"\n",
    "    def __init__(self,**kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        \n",
    "    def build(self,batch_input_shape):\n",
    "        self.q = self.add_weight(name=\"q\",shape=[1,9],initializer=\"uniform\",trainable=True)\n",
    "        super().build(batch_input_shape)\n",
    "    \n",
    "    def call(self,X):\n",
    "        concatenated = tf.concat([self.q,tf.constant(np.array(0.0).reshape(1,-1),dtype=\"float32\")],axis=-1)\n",
    "        p = K.exp(concatenated)\n",
    "        p = tf.math.divide(p,K.sum(p))\n",
    "        loss = keras.losses.categorical_crossentropy(X,p)\n",
    "        loss = tf.reduce_mean(loss)\n",
    "        return loss,p \n",
    "    \n",
    "    def compute_output_shape(self,batch_input_shape):\n",
    "        return tf.TensorShape(10)\n",
    "    \n",
    "\n",
    "class FullModel_only_labelled(keras.models.Model):\n",
    "    def __init__(self,N_parameter,beta,variational_encoder,variational_decoder,classifier,y_distribution,**kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.encoder = variational_encoder\n",
    "        self.decoder = variational_decoder\n",
    "        self.classifier = classifier  \n",
    "        self.y_distribution = y_distribution\n",
    "        self.N = N_parameter\n",
    "        self.beta = beta\n",
    "    def call(self,inputs):\n",
    "        \"\"\"Inputs is a list, as such:\n",
    "            inputs[0] is labelled X \n",
    "            inputs[1] is labelled y \n",
    "            \"\"\"\n",
    "        \n",
    "        X_labelled = inputs[0]\n",
    "        y_labelled = inputs[1]\n",
    "        \n",
    "        ############### LABELLED CASE #################\n",
    "        \n",
    "        codings_mean,codings_log_var,codings = self.encoder([X_labelled,y_labelled])\n",
    "        y_pred_label = self.classifier(X_labelled)\n",
    "        reconstructions = self.decoder([codings,y_labelled])\n",
    "\n",
    "        #LOSSES#\n",
    "        \n",
    "        recon_loss = labelled_loss_reconstruction(codings_log_var,codings_mean,X_labelled,reconstructions,self.beta)\n",
    "        cls_loss = labelled_cls_loss(y_labelled,y_pred_label,self.N)\n",
    "        y_dist_loss = self.y_distribution(y_labelled)[0]\n",
    "        labelled_loss = recon_loss + cls_loss + y_dist_loss\n",
    "       \n",
    "        \n",
    "        ############### ALL LOSSES #######################\n",
    "        \n",
    "        loss = labelled_loss\n",
    "        return loss\n",
    "\n",
    "    \n",
    "def build_model_only_labelled(n_hidden=1, n_neurons=723,input_shape=input_shape,beta=1,n_hidden_classifier=1,\n",
    "              n_neurons_classifier=300,N=30,codings_size=50):\n",
    "    \n",
    "    \"\"\"\n",
    "    Builds deep generative model.\n",
    "    \n",
    "    Parameters specify the architecture. Architecture is such that encoder and decoder have same number of nodes and hidden\n",
    "    layers. Done for simplicity. Classifier has its own architecture.\n",
    "    \n",
    "    Returns encoder,decoder,y_distribution, classifier and overall model. These can be used downstream.\n",
    "    \n",
    "    \"\"\"\n",
    "       \n",
    "    ########## ENCODER ###############\n",
    "    \n",
    "    x_in = keras.layers.Input(shape=[input_shape])\n",
    "    y_in = keras.layers.Input(shape=[10])\n",
    "    z = keras.layers.concatenate([x_in,y_in])\n",
    "    for layer in range(n_hidden):\n",
    "        z = keras.layers.Dense(n_neurons,activation=\"elu\",kernel_initializer=\"he_normal\")(z)\n",
    "        z = keras.layers.Dropout(0.3)(z)\n",
    "\n",
    "    codings_mean = keras.layers.Dense(codings_size)(z)\n",
    "    codings_log_var = keras.layers.Dense(codings_size)(z)\n",
    "    codings = Sampling()([codings_mean, codings_log_var])\n",
    "    variational_encoder = keras.models.Model(\n",
    "        inputs=[x_in,y_in], outputs=[codings_mean, codings_log_var, codings])\n",
    "    \n",
    "    \n",
    "    ########## DECODER ###############\n",
    "\n",
    "    latent = keras.layers.Input(shape=[codings_size])\n",
    "    l_merged = keras.layers.concatenate([latent,y_in])\n",
    "    x = l_merged\n",
    "    for layer in range(n_hidden):\n",
    "        x = keras.layers.Dense(n_neurons, activation=\"elu\",kernel_initializer=\"he_normal\")(x)\n",
    "        x = keras.layers.Dropout(0.3)(x)\n",
    "    x_out = keras.layers.Dense(input_shape,activation=\"sigmoid\")(x) \n",
    "    variational_decoder = keras.models.Model(inputs=[latent,y_in], outputs=[x_out])\n",
    "    \n",
    "    \n",
    "    ########### CLASSIFIER ############\n",
    "    \n",
    "    y_classifier = x_in\n",
    "    for layer in range(n_hidden_classifier):\n",
    "        y_classifier = keras.layers.Dense(n_neurons_classifier, activation=\"elu\",kernel_initializer=\"he_normal\")(y_classifier)\n",
    "        y_classifier = keras.layers.Dropout(rate=0.3)(y_classifier)\n",
    "    y_pred = keras.layers.Dense(10,activation=\"softmax\")(y_classifier) \n",
    "    classifier = keras.models.Model(inputs=[x_in], outputs=[y_pred])\n",
    "    \n",
    "    \n",
    "    ############ Y DISTRIBUTION #############\n",
    "    \n",
    "    loss,p = y_dist()(y_in)\n",
    "    y_distribution = keras.models.Model(inputs=[y_in],outputs=[loss,p])\n",
    "    \n",
    "    \n",
    "    ########## FULL MODEL #############\n",
    "    \n",
    "    model = FullModel_only_labelled(N_parameter=N,beta=beta,variational_encoder=variational_encoder,\n",
    "                  variational_decoder=variational_decoder,classifier=classifier,y_distribution=y_distribution)\n",
    "    \n",
    "    return variational_encoder,variational_decoder,classifier,y_distribution,model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loss functions ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def custom_mse(x,x_decoded_mean):\n",
    "    \"\"\"returns column of squared errors. Length of column is number of samples.\"\"\"\n",
    "    diff = (x-x_decoded_mean)**2\n",
    "    return K.sum(diff,axis=-1) /2 \n",
    "\n",
    "\n",
    "def labelled_loss_reconstruction(codings_log_var,codings_mean,x, x_decoded_mean,beta=1):\n",
    "    \"\"\"Labelled data. This is the reconstruction loss for the x.\"\"\"\n",
    "    recon_loss = custom_mse(x, x_decoded_mean)\n",
    "    kl_loss = - 0.5 * K.sum(1 + codings_log_var - K.square(codings_mean) - K.exp(codings_log_var), axis=-1)\n",
    "    #kl loss gives vector of one value for each sample in batch \n",
    "    return K.mean(recon_loss + beta*kl_loss)\n",
    "\n",
    "\n",
    "def dummy_loss(y,ypred):\n",
    "    \"\"\"This is a dummy loss that returns a value of zero. It is here as keras requires a loss term for each output.\n",
    "        The regression_loss_for_labelled_y gives the loss which depends on the log var and mean, so we don't need another\n",
    "        loss. But keras wants us to give separate losses for each. To keep keras happy, we use the dummy loss as a placeholder.\"\"\"\n",
    "    return 0.0\n",
    "\n",
    "def labelled_cls_loss(y, y_pred,N=383):\n",
    "    alpha = 0.1*N\n",
    "    cat_xent_loss = keras.losses.categorical_crossentropy(y, y_pred)\n",
    "    return alpha*K.mean(cat_xent_loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training functions ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "@tf.function\n",
    "def train_step(inputs):\n",
    "    with tf.GradientTape() as tape:\n",
    "        loss = model(inputs,training=True)\n",
    "        loss = tf.add_n([loss] + model.losses)\n",
    "    gradients = tape.gradient(loss, model.trainable_variables)\n",
    "    optimizer.apply_gradients(zip(gradients, model.trainable_variables))\n",
    "    return loss\n",
    "\n",
    "def fit_model(X_train_la, y_train_la,epochs,X_valid_la, y_valid_la,\n",
    "              patience,variational_encoder,variational_decoder,\n",
    "             classifier,y_distribution,model,Sampling=Sampling,y_dist=y_dist,batch_size=32,learning_rate=0.001,codings_size=50,\n",
    "             valid_set=True):\n",
    "\n",
    "    \"\"\"\n",
    "    Fits a single model. Gets the validation loss too if valid set exists. \n",
    "    And includes a version of early stopping, given by the patience.\n",
    "    Progress bars are shown too.\n",
    "    Number of epochs are specified by the parameter epochs.\n",
    "    \n",
    "    Need to pass in all the custom components. Maybe could put them in a dictionary for cleanliness.\n",
    "    \n",
    "    Valid set is True or False depending if you have one. If you don't, the model at the end of training is saved.\n",
    "    You must still pass in dummy valid sets even if valid_set=False.\n",
    "    \n",
    "    Returns list of training loss, and the minimum validation loss. It also saves the best encoder, decoder and\n",
    "    regressor so they can be used. \n",
    "    \n",
    "    e.g. usage fit_model(X_train_omics_labelled, train_set_labelled_y,50,X_valid_omics, valid_set_labelled_y,\n",
    "              10,variational_encoder=variational_encoder,variational_decoder=variational_decoder,\n",
    "             classifier=classifier,y_distribution=y_distribution,model=model,\n",
    "          Sampling=Sampling,y_dist=y_dist,batch_size=32,learning_rate=0.001,codings_size=50,valid_set=True)\n",
    "    \"\"\"\n",
    "    if valid_set is True:\n",
    "    \n",
    "        start = time.time()\n",
    "        history = []\n",
    "        K.clear_session()\n",
    "\n",
    "        @tf.function\n",
    "        def train_step(inputs):\n",
    "            with tf.GradientTape() as tape:\n",
    "                loss = model(inputs,training=True)\n",
    "                loss = tf.add_n([loss] + model.losses)\n",
    "            gradients = tape.gradient(loss, model.trainable_variables)\n",
    "            optimizer.apply_gradients(zip(gradients, model.trainable_variables))\n",
    "            return loss\n",
    "\n",
    "        validation_loss = []\n",
    "        optimizer=keras.optimizers.Adam(learning_rate=learning_rate)\n",
    "        batch_loss = []\n",
    "        batches_per_epoch = int(np.floor((X_train_la.shape[0])/batch_size))\n",
    "\n",
    "        for epoch in range(epochs):\n",
    "\n",
    "                print(\"Epoch {}/{}\".format(epoch,epochs))\n",
    "\n",
    "                for i in range(batches_per_epoch):\n",
    "\n",
    "                    batch_x_la, batch_y_la= create_batch_label(\n",
    "                        X_train_la, y_train_la,batch_size)\n",
    "\n",
    "                    inputs = [batch_x_la.to_numpy(),batch_y_la]\n",
    "                    loss = train_step(inputs)\n",
    "                    batch_loss.append(loss)\n",
    "                    average_batch_loss = list_average(batch_loss)\n",
    "                    print_status_bar(i*batch_size,X_train_la.shape[0],average_batch_loss)\n",
    "\n",
    "                training_loss_for_epoch = list_average(batch_loss)\n",
    "                batch_loss = []\n",
    "                history.append(training_loss_for_epoch)\n",
    "                val_loss = -validation_log_lik_sampling(y_valid_la,X_valid_la.to_numpy(),variational_decoder=variational_decoder,codings_size=codings_size)\n",
    "\n",
    "                validation_loss.append(val_loss)\n",
    "                print_status_bar_epoch(X_train_la.shape[0] \n",
    "                                 ,(X_train_la.shape[0] ),training_loss_for_epoch,val_loss )\n",
    "\n",
    "                #callback for early stopping\n",
    "                if epoch <= patience - 1:\n",
    "\n",
    "                    if epoch == 0:\n",
    "\n",
    "                        variational_encoder.save(\"variational_encoder.h5\")\n",
    "                        variational_decoder.save(\"variational_decoder.h5\")\n",
    "                        classifier.save(\"classifier.h5\")\n",
    "                        y_distribution.save(\"y_distribution.h5\")\n",
    "\n",
    "                    else:\n",
    "                        if all(val_loss<i for i in validation_loss[:-1]) is True:\n",
    "                            variational_encoder.save(\"variational_encoder.h5\")\n",
    "                            variational_decoder.save(\"variational_decoder.h5\")\n",
    "                            classifier.save(\"classifier.h5\")\n",
    "                            y_distribution.save(\"y_distribution.h5\")\n",
    "                #this statement means at least a model is saved. Because if the best model was before epoch > patience-1,\n",
    "                #then the statement below won't save any model, which is undesirable as we need to load a model. \n",
    "\n",
    "                if epoch > patience - 1:\n",
    "\n",
    "                    latest_val_loss = validation_loss[-patience:]\n",
    "                    if all(val_loss<i for i in latest_val_loss[:-2]) is True:\n",
    "                        variational_encoder.save(\"variational_encoder.h5\")\n",
    "                        variational_decoder.save(\"variational_decoder.h5\")\n",
    "                        classifier.save(\"classifier.h5\")\n",
    "                        y_distribution.save(\"y_distribution.h5\")\n",
    "                    if all(i>latest_val_loss[0] for i in latest_val_loss[1:]) is True:\n",
    "                        break     \n",
    "\n",
    "        #load best model#\n",
    "        variational_encoder = keras.models.load_model(\"variational_encoder.h5\", custom_objects={\n",
    "           \"Sampling\": Sampling\n",
    "        })\n",
    "        variational_decoder = keras.models.load_model(\"variational_decoder.h5\")\n",
    "        classifier = keras.models.load_model(\"classifier.h5\")     \n",
    "        y_distribution = keras.models.load_model(\"y_distribution.h5\", custom_objects={\n",
    "           \"y_dist\": y_dist\n",
    "        })    \n",
    "\n",
    "        done = time.time()\n",
    "        elapsed = done-start\n",
    "        print(\"Elapsed/s: \",elapsed)\n",
    "        print(\"Final training loss: \",training_loss_for_epoch)\n",
    "        print(\"best val loss: \", min(validation_loss))\n",
    "        \n",
    "        return history, min(validation_loss)\n",
    "\n",
    "        \n",
    "    else:\n",
    "        \n",
    "        start = time.time()\n",
    "        history = []\n",
    "        K.clear_session()\n",
    "\n",
    "        @tf.function\n",
    "        def train_step(inputs):\n",
    "            with tf.GradientTape() as tape:\n",
    "                loss = model(inputs,training=True)\n",
    "                loss = tf.add_n([loss] + model.losses)\n",
    "            gradients = tape.gradient(loss, model.trainable_variables)\n",
    "            optimizer.apply_gradients(zip(gradients, model.trainable_variables))\n",
    "            return loss\n",
    "\n",
    "        optimizer=keras.optimizers.Adam(learning_rate=learning_rate)\n",
    "        batch_loss = []\n",
    "        batches_per_epoch = int(np.floor((X_train_la.shape[0])/batch_size))        \n",
    "        val_loss = 0\n",
    "\n",
    "        for epoch in range(epochs):\n",
    "\n",
    "                print(\"Epoch {}/{}\".format(epoch,epochs))\n",
    "                for i in range(batches_per_epoch):\n",
    "\n",
    "                    batch_x_la, batch_y_la= create_batch_label(\n",
    "                        X_train_la, y_train_la,batch_size)\n",
    "\n",
    "                    inputs = [batch_x_la.to_numpy(),batch_y_la]\n",
    "                    loss = train_step(inputs)\n",
    "                    batch_loss.append(loss)\n",
    "                    average_batch_loss = list_average(batch_loss)\n",
    "                    print_status_bar(i*batch_size,X_train_la.shape[0],average_batch_loss)\n",
    "\n",
    "                training_loss_for_epoch = list_average(batch_loss)\n",
    "                batch_loss = []\n",
    "                history.append(training_loss_for_epoch)\n",
    "                print_status_bar_epoch(X_train_la.shape[0]\n",
    "                                 ,(X_train_la.shape[0]),training_loss_for_epoch,val_loss )\n",
    "        \n",
    "\n",
    "        variational_encoder.save(\"variational_encoder.h5\")\n",
    "        variational_decoder.save(\"variational_decoder.h5\")\n",
    "        classifier.save(\"classifier.h5\")\n",
    "        y_distribution.save(\"y_distribution.h5\")\n",
    "        \n",
    "        #load best model#\n",
    "        variational_encoder = keras.models.load_model(\"variational_encoder.h5\", custom_objects={\n",
    "           \"Sampling\": Sampling\n",
    "        })\n",
    "        variational_decoder = keras.models.load_model(\"variational_decoder.h5\")\n",
    "        classifier = keras.models.load_model(\"classifier.h5\")     \n",
    "        y_distribution = keras.models.load_model(\"y_distribution.h5\", custom_objects={\n",
    "           \"y_dist\": y_dist\n",
    "        })    \n",
    "\n",
    "        done = time.time()\n",
    "        elapsed = done-start\n",
    "        print(\"Elapsed/s: \",elapsed)\n",
    "        print(\"Final training loss: \",training_loss_for_epoch)\n",
    "        \n",
    "    \n",
    "        return history\n",
    "    \n",
    "def fit_model_search_labelled(X_train_la, y_train_la,epochs,X_valid_la, y_valid_la,\n",
    "              patience,variational_encoder,variational_decoder,\n",
    "             classifier,y_distribution,model,Sampling=Sampling,y_dist=y_dist,batch_size=32,learning_rate=0.001,\n",
    "                    codings_size=50):\n",
    "\n",
    "    \"\"\"\n",
    "    Use for hyperparameter search. \n",
    "    \n",
    "    Fits the model. Gets the validation loss too. And includes a version of early stopping, given by the patience.\n",
    "    Progress bars are shown too.\n",
    "    Number of epochs are specified by the parameter epochs.\n",
    "    \n",
    "    Need to pass in all the custom components. Maybe could put them in a dictionary for cleanliness.\n",
    "    \n",
    "    Returns list of training loss, and the minimum validation loss. It also saves the best encoder, decoder and\n",
    "    regressor so they can be used. \n",
    "    \n",
    "    \"\"\"\n",
    "    \n",
    "    start = time.time()\n",
    "    history = []\n",
    "    \n",
    "        \n",
    "    @tf.function\n",
    "    def train_step(inputs):\n",
    "        with tf.GradientTape() as tape:\n",
    "            loss = model(inputs,training=True)\n",
    "            loss = tf.add_n([loss] + model.losses)\n",
    "        gradients = tape.gradient(loss, model.trainable_variables)\n",
    "        optimizer.apply_gradients(zip(gradients, model.trainable_variables))\n",
    "        return loss\n",
    "    \n",
    "    validation_loss = []\n",
    "    optimizer=keras.optimizers.Adam(learning_rate=learning_rate)\n",
    "\n",
    "    batch_loss = []\n",
    "    \n",
    "    batches_per_epoch = int(np.floor((X_train_la.shape[0])/batch_size))\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "            \n",
    "            print(\"Epoch {}/{}\".format(epoch,epochs))\n",
    "            \n",
    "            for i in range(batches_per_epoch):\n",
    "                \n",
    "                batch_x_la, batch_y_la= create_batch_label(\n",
    "                    X_train_la, y_train_la,batch_size)\n",
    "\n",
    "                inputs = [batch_x_la.to_numpy(),batch_y_la]\n",
    "                loss = train_step(inputs)\n",
    "\n",
    "                batch_loss.append(loss)\n",
    "                \n",
    "                average_batch_loss = list_average(batch_loss)\n",
    "                \n",
    "                print_status_bar(i*batch_size,X_train_la.shape[0],average_batch_loss)\n",
    "            \n",
    "            training_loss_for_epoch = list_average(batch_loss)\n",
    "\n",
    "            batch_loss = []\n",
    "                \n",
    "            history.append(training_loss_for_epoch)\n",
    "            \n",
    "            val_loss = -validation_log_lik_sampling(y_valid_la,X_valid_la.to_numpy(),variational_decoder=variational_decoder,codings_size=codings_size)\n",
    "            \n",
    "            validation_loss.append(val_loss)\n",
    "            \n",
    "            print_status_bar_epoch(X_train_la.shape[0] \n",
    "                             ,(X_train_la.shape[0] ),training_loss_for_epoch,val_loss )\n",
    "            \n",
    "            #callback for early stopping\n",
    "            \n",
    "            if epoch <= patience - 1:\n",
    "                \n",
    "                if epoch == 0:\n",
    "                \n",
    "                    variational_encoder.save(\"variational_encoder_intermediate.h5\")\n",
    "                    variational_decoder.save(\"variational_decoder_intermediate.h5\")\n",
    "                    classifier.save(\"classifier_intermediate.h5\")\n",
    "                    y_distribution.save(\"y_distribution_intermediate.h5\")\n",
    "                    \n",
    "                else:\n",
    "                    if all(val_loss<i for i in validation_loss[:-1]) is True:\n",
    "                        variational_encoder.save(\"variational_encoder_intermediate.h5\")\n",
    "                        variational_decoder.save(\"variational_decoder_intermediate.h5\")\n",
    "                        classifier.save(\"classifier_intermediate.h5\")\n",
    "                        y_distribution.save(\"y_distribution_intermediate.h5\")\n",
    "            #this statement means at least a model is saved. Because if the best model was before epoch > patience-1,\n",
    "            #then the statement below won't save any model, which is undesirable as we need to load a model. \n",
    "            \n",
    "            if epoch > patience - 1:\n",
    "                                \n",
    "                latest_val_loss = validation_loss[-patience:]\n",
    "                if all(val_loss<i for i in latest_val_loss[:-1]) is True:\n",
    "                    variational_encoder.save(\"variational_encoder_intermediate.h5\")\n",
    "                    variational_decoder.save(\"variational_decoder_intermediate.h5\")\n",
    "                    classifier.save(\"classifier_intermediate.h5\")\n",
    "                    y_distribution.save(\"y_distribution_intermediate.h5\")\n",
    "                if all(i>latest_val_loss[0] for i in latest_val_loss[1:]) is True:\n",
    "                    break     \n",
    "    \n",
    "    #load best model#\n",
    "    variational_encoder = keras.models.load_model(\"variational_encoder_intermediate.h5\", custom_objects={\n",
    "       \"Sampling\": Sampling\n",
    "    })\n",
    "    variational_decoder = keras.models.load_model(\"variational_decoder_intermediate.h5\")\n",
    "    classifier = keras.models.load_model(\"classifier_intermediate.h5\")     \n",
    "    y_distribution = keras.models.load_model(\"y_distribution_intermediate.h5\", custom_objects={\n",
    "       \"y_dist\": y_dist\n",
    "    })    \n",
    "                \n",
    "    done = time.time()\n",
    "    elapsed = done-start\n",
    "    print(\"Elapsed/s: \",elapsed)\n",
    "    print(\"Final training loss: \",training_loss_for_epoch)\n",
    "    print(\"best val loss: \", min(validation_loss))\n",
    "    \n",
    "    return history, min(validation_loss)\n",
    "\n",
    "\n",
    "def hyperparameter_search_labelled(param_distribs,epochs,patience,n_iter,X_train_la=X_train_omics_labelled, \n",
    "                          y_train_la=train_set_labelled_y,\n",
    "                          X_valid_la=X_valid_omics, y_valid_la=valid_set_labelled_y):\n",
    "    \n",
    "    \"\"\"\n",
    "    Performs hyperparameter, random search. Assesses performance by determining the score on the validation set. \n",
    "    \n",
    "    Saves best models (encoder, decoder and regressor) and returns these. These can then be used downstream.\n",
    "    \n",
    "    Maybe want to implement something where the best model is then retrained on train+valid set?\n",
    "    \n",
    "    Param_distribs of the form: \n",
    "            param_distribs = {\n",
    "            \"n_hidden\": [1],\n",
    "            \"n_hidden_classifier\": [1],\n",
    "            \"beta\": [1],\n",
    "            \"n_neurons\": randint.rvs(50,1000-49,size=20,random_state=random_state).tolist(),\n",
    "           \"n_neurons_classifier\": randint.rvs(49,1000-49,size=20,random_state=random_state).tolist(),\n",
    "            \"codings_size\": randint.rvs(50,290-50,size=30,random_state=random_state).tolist(),\n",
    "            \"N\" :randint.rvs().tolist(),\n",
    "            \"learning_rate\" : ....\n",
    "            #\"codings_size\": [50]}\n",
    "            \n",
    "    There must be a value for every parameter. If you know the value you want to use, set it in the param_distribs\n",
    "    dictionary.\n",
    "    \n",
    "    Patience must be less than the number of epochs.\n",
    "    \n",
    "    e.g. result,variational_encoder,variational_decoder,classifier,y_distribution =\n",
    "            hyperparameter_search(param_distribs,500,10,n_iter=10)\n",
    "\n",
    "            \n",
    "    \"\"\"\n",
    "\n",
    "    np.random.seed(42) #needs to be here so that everything that follows is consistent\n",
    "\n",
    "    min_val_loss = []\n",
    "    master = {}\n",
    "\n",
    "    for i in range(n_iter): \n",
    "        K.clear_session()\n",
    "        master[i] = {}\n",
    "        master[i][\"parameters\"] = {}\n",
    "        \n",
    "        N= np.random.choice(param_distribs[\"N\"])\n",
    "        learning_rate= np.random.choice(param_distribs[\"learning_rate\"])\n",
    "        beta= np.random.choice(param_distribs[\"beta\"])\n",
    "        n_neurons =np.random.choice(param_distribs[\"n_neurons\"]) \n",
    "        n_neurons_classifier =np.random.choice(param_distribs[\"n_neurons_classifier\"]) \n",
    "        n_hidden  =np.random.choice(param_distribs[\"n_hidden\"]) \n",
    "        n_hidden_classifier  =np.random.choice(param_distribs[\"n_hidden_classifier\"]) \n",
    "        codings_size =np.random.choice(param_distribs[\"codings_size\"]) \n",
    "       \n",
    "        master[i][\"parameters\"][\"N\"] = N\n",
    "        master[i][\"parameters\"][\"learning_rate\"] = learning_rate\n",
    "        master[i][\"parameters\"][\"beta\"] = beta\n",
    "        master[i][\"parameters\"][\"n_neurons\"] = n_neurons\n",
    "        master[i][\"parameters\"][\"n_neurons_classifier\"] = n_neurons_classifier\n",
    "        master[i][\"parameters\"][\"n_hidden\"] = n_hidden\n",
    "        master[i][\"parameters\"][\"n_hidden_classifier\"] = n_hidden_classifier\n",
    "        master[i][\"parameters\"][\"codings_size\"] = codings_size\n",
    "\n",
    "        \n",
    "        variational_encoder,variational_decoder,classifier,y_distribution,model = build_model_only_labelled(n_hidden=n_hidden,       \n",
    "                                       n_neurons=n_neurons,beta=beta,n_hidden_classifier=n_hidden_classifier,\n",
    "                                        n_neurons_classifier=n_neurons_classifier,N=N,codings_size=codings_size)\n",
    "        \n",
    "                \n",
    "        history,val_loss = fit_model_search_labelled(X_train_la=X_train_la, y_train_la=y_train_la, \n",
    "                                 epochs=epochs,X_valid_la=X_valid_la, \n",
    "                                 y_valid_la=y_valid_la,patience=patience,variational_encoder=variational_encoder,\n",
    "                                variational_decoder=variational_decoder, classifier=classifier,\n",
    "                                y_distribution=y_distribution,model=model,Sampling=Sampling,y_dist=y_dist,\n",
    "                                            batch_size=32,learning_rate=learning_rate,codings_size=codings_size)\n",
    "        \n",
    "        \n",
    "\n",
    "        master[i][\"val_loss\"] = val_loss\n",
    "        min_val_loss.append(val_loss)\n",
    "\n",
    "        #If val loss is lowest, save this model. \n",
    "        if val_loss <=  min(min_val_loss):\n",
    "            os.rename(\"variational_encoder_intermediate.h5\",\"variational_encoder.h5\")\n",
    "            os.rename(\"variational_decoder_intermediate.h5\",\"variational_decoder.h5\")\n",
    "            os.rename(\"classifier_intermediate.h5\",\"classifier.h5\")\n",
    "            os.rename(\"y_distribution_intermediate.h5\",\"y_distribution.h5\")\n",
    "\n",
    "        print(master)\n",
    "            \n",
    "    #load best model#\n",
    "    variational_encoder = keras.models.load_model(\"variational_encoder.h5\", custom_objects={\n",
    "       \"Sampling\": Sampling\n",
    "    })\n",
    "    variational_decoder = keras.models.load_model(\"variational_decoder.h5\")\n",
    "    classifier = keras.models.load_model(\"classifier.h5\")     \n",
    "    y_distribution = keras.models.load_model(\"y_distribution.h5\", custom_objects={\n",
    "       \"y_dist\": y_dist\n",
    "    })    \n",
    "\n",
    "    result = sorted(master.items(), key=lambda item: item[1][\"val_loss\"])\n",
    "    return result,variational_encoder,variational_decoder,classifier,y_distribution\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hyperparameter Search #"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "param_distribs = {\n",
    "            \"n_hidden\": [1,2],\n",
    "            \"n_hidden_classifier\": [1,2],\n",
    "            \"beta\": [1,10,15],\n",
    "            \"n_neurons\": randint.rvs(50,600-49,size=20,random_state=random_state).tolist(),\n",
    "           \"n_neurons_classifier\": randint.rvs(20,120-20,size=20,random_state=random_state).tolist(),\n",
    "            \"codings_size\": randint.rvs(20,290-20,size=30,random_state=random_state).tolist(),\n",
    "            \"N\" :[1,50,100,150],\n",
    "            \"learning_rate\" : [0.001,0.0005,0.0001],\n",
    "            #\"codings_size\": [120,60]\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0/500\n",
      "WARNING:tensorflow:Layer full_model_only_labelled is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because it's dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "320/383 [========================>.....] - Loss for batch: 523.9136WARNING:tensorflow:Layer concatenate_1 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because it's dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "383/383 [==============================] - trainLoss: 523.9136  Val_loss: 24.3722 \n",
      "Epoch 1/500\n",
      "383/383 [==============================] - trainLoss: 173.2733  Val_loss: 22.3410 \n",
      "Epoch 2/500\n",
      "383/383 [==============================] - trainLoss: 134.6332  Val_loss: 21.5348 \n",
      "Epoch 3/500\n",
      "383/383 [==============================] - trainLoss: 111.9441  Val_loss: 20.6326 \n",
      "Epoch 4/500\n",
      "383/383 [==============================] - trainLoss: 102.1723  Val_loss: 19.8662 \n",
      "Epoch 5/500\n",
      "383/383 [==============================] - trainLoss: 93.2135  Val_loss: 18.9499 \n",
      "Epoch 6/500\n",
      "383/383 [==============================] - trainLoss: 88.5481  Val_loss: 18.0934 \n",
      "Epoch 7/500\n",
      "383/383 [==============================] - trainLoss: 83.3755  Val_loss: 17.4528 \n",
      "Epoch 8/500\n",
      "383/383 [==============================] - trainLoss: 78.5841  Val_loss: 17.0087 \n",
      "Epoch 9/500\n",
      "383/383 [==============================] - trainLoss: 75.6725  Val_loss: 16.6027 \n",
      "Epoch 10/500\n",
      "383/383 [==============================] - trainLoss: 74.0434  Val_loss: 16.0563 \n",
      "Epoch 11/500\n",
      "383/383 [==============================] - trainLoss: 69.4211  Val_loss: 15.6433 \n",
      "Epoch 12/500\n",
      "383/383 [==============================] - trainLoss: 66.5149  Val_loss: 15.3190 \n",
      "Epoch 13/500\n",
      "383/383 [==============================] - trainLoss: 65.2654  Val_loss: 15.0847 \n",
      "Epoch 14/500\n",
      "383/383 [==============================] - trainLoss: 63.0204  Val_loss: 14.7999 \n",
      "Epoch 15/500\n",
      "383/383 [==============================] - trainLoss: 60.7200  Val_loss: 14.6992 \n",
      "Epoch 16/500\n",
      "383/383 [==============================] - trainLoss: 59.7498  Val_loss: 14.5593 \n",
      "Epoch 17/500\n",
      "383/383 [==============================] - trainLoss: 57.3817  Val_loss: 14.3477 \n",
      "Epoch 18/500\n",
      "383/383 [==============================] - trainLoss: 57.0314  Val_loss: 14.1751 \n",
      "Epoch 19/500\n",
      "383/383 [==============================] - trainLoss: 53.7841  Val_loss: 13.9518 \n",
      "Epoch 20/500\n",
      "383/383 [==============================] - trainLoss: 51.3655  Val_loss: 13.7997 \n",
      "Epoch 21/500\n",
      "383/383 [==============================] - trainLoss: 50.1330  Val_loss: 13.7651 \n",
      "Epoch 22/500\n",
      "383/383 [==============================] - trainLoss: 49.6343  Val_loss: 13.7371 \n",
      "Epoch 23/500\n",
      "383/383 [==============================] - trainLoss: 49.1684  Val_loss: 13.6708 \n",
      "Epoch 24/500\n",
      "383/383 [==============================] - trainLoss: 48.5406  Val_loss: 13.5808 \n",
      "Epoch 25/500\n",
      "383/383 [==============================] - trainLoss: 47.0706  Val_loss: 13.6419 \n",
      "Epoch 26/500\n",
      "383/383 [==============================] - trainLoss: 46.2421  Val_loss: 13.6689 \n",
      "Epoch 27/500\n",
      "383/383 [==============================] - trainLoss: 45.1889  Val_loss: 13.5601 \n",
      "Epoch 28/500\n",
      "383/383 [==============================] - trainLoss: 44.9445  Val_loss: 13.4900 \n",
      "Epoch 29/500\n",
      "383/383 [==============================] - trainLoss: 43.5595  Val_loss: 13.4382 \n",
      "Epoch 30/500\n",
      "383/383 [==============================] - trainLoss: 42.8334  Val_loss: 13.3099 \n",
      "Epoch 31/500\n",
      "383/383 [==============================] - trainLoss: 42.6087  Val_loss: 13.2856 \n",
      "Epoch 32/500\n",
      "383/383 [==============================] - trainLoss: 40.2266  Val_loss: 13.3719 \n",
      "Epoch 33/500\n",
      "383/383 [==============================] - trainLoss: 40.9962  Val_loss: 13.2160 \n",
      "Epoch 34/500\n",
      "383/383 [==============================] - trainLoss: 39.4460  Val_loss: 13.2328 \n",
      "Epoch 35/500\n",
      "383/383 [==============================] - trainLoss: 39.7573  Val_loss: 13.2212 \n",
      "Epoch 36/500\n",
      "383/383 [==============================] - trainLoss: 39.0350  Val_loss: 13.1719 \n",
      "Epoch 37/500\n",
      "383/383 [==============================] - trainLoss: 39.1013  Val_loss: 13.1165 \n",
      "Epoch 38/500\n",
      "383/383 [==============================] - trainLoss: 37.8309  Val_loss: 13.2332 \n",
      "Epoch 39/500\n",
      "383/383 [==============================] - trainLoss: 36.7364  Val_loss: 13.2422 \n",
      "Epoch 40/500\n",
      "383/383 [==============================] - trainLoss: 36.3756  Val_loss: 13.1734 \n",
      "Epoch 41/500\n",
      "383/383 [==============================] - trainLoss: 36.2503  Val_loss: 13.0583 \n",
      "Epoch 42/500\n",
      "383/383 [==============================] - trainLoss: 35.9011  Val_loss: 13.0738 \n",
      "Epoch 43/500\n",
      "383/383 [==============================] - trainLoss: 35.1884  Val_loss: 13.0626 \n",
      "Epoch 44/500\n",
      "383/383 [==============================] - trainLoss: 35.2100  Val_loss: 13.0848 \n",
      "Epoch 45/500\n",
      "383/383 [==============================] - trainLoss: 35.1424  Val_loss: 13.1179 \n",
      "Epoch 46/500\n",
      "383/383 [==============================] - trainLoss: 33.8283  Val_loss: 13.0531 \n",
      "Epoch 47/500\n",
      "383/383 [==============================] - trainLoss: 33.5991  Val_loss: 12.9742 \n",
      "Epoch 48/500\n",
      "383/383 [==============================] - trainLoss: 33.3189  Val_loss: 12.9678 \n",
      "Epoch 49/500\n",
      "383/383 [==============================] - trainLoss: 33.4143  Val_loss: 12.9721 \n",
      "Epoch 50/500\n",
      "383/383 [==============================] - trainLoss: 32.3249  Val_loss: 13.0272 \n",
      "Epoch 51/500\n",
      "383/383 [==============================] - trainLoss: 32.3734  Val_loss: 12.9401 \n",
      "Epoch 52/500\n",
      "383/383 [==============================] - trainLoss: 31.2277  Val_loss: 12.9597 \n",
      "Epoch 53/500\n",
      "383/383 [==============================] - trainLoss: 31.6640  Val_loss: 13.0543 \n",
      "Epoch 54/500\n",
      "383/383 [==============================] - trainLoss: 31.2719  Val_loss: 12.9771 \n",
      "Epoch 55/500\n",
      "383/383 [==============================] - trainLoss: 30.7641  Val_loss: 12.9463 \n",
      "Epoch 56/500\n",
      "383/383 [==============================] - trainLoss: 31.1934  Val_loss: 12.8791 \n",
      "Epoch 57/500\n",
      "383/383 [==============================] - trainLoss: 31.0607  Val_loss: 12.7864 \n",
      "Epoch 58/500\n",
      "383/383 [==============================] - trainLoss: 30.2542  Val_loss: 12.9936 \n",
      "Epoch 59/500\n",
      "383/383 [==============================] - trainLoss: 29.6729  Val_loss: 12.9574 \n",
      "Epoch 60/500\n",
      "383/383 [==============================] - trainLoss: 30.2030  Val_loss: 12.8743 \n",
      "Epoch 61/500\n",
      "383/383 [==============================] - trainLoss: 29.9366  Val_loss: 12.9588 \n",
      "Epoch 62/500\n",
      "383/383 [==============================] - trainLoss: 28.8187  Val_loss: 12.8695 \n",
      "Epoch 63/500\n",
      "383/383 [==============================] - trainLoss: 29.1877  Val_loss: 12.8649 \n",
      "Epoch 64/500\n",
      "383/383 [==============================] - trainLoss: 29.0301  Val_loss: 12.8592 \n",
      "Epoch 65/500\n",
      "383/383 [==============================] - trainLoss: 28.6345  Val_loss: 12.8271 \n",
      "Epoch 66/500\n",
      "383/383 [==============================] - trainLoss: 28.1371  Val_loss: 12.7314 \n",
      "Epoch 67/500\n",
      "383/383 [==============================] - trainLoss: 28.2358  Val_loss: 12.7613 \n",
      "Epoch 68/500\n",
      "383/383 [==============================] - trainLoss: 28.0484  Val_loss: 12.8340 \n",
      "Epoch 69/500\n",
      "383/383 [==============================] - trainLoss: 27.7416  Val_loss: 12.7522 \n",
      "Epoch 70/500\n",
      "383/383 [==============================] - trainLoss: 27.5075  Val_loss: 12.8031 \n",
      "Epoch 71/500\n",
      "383/383 [==============================] - trainLoss: 27.1474  Val_loss: 12.9085 \n",
      "Epoch 72/500\n",
      "383/383 [==============================] - trainLoss: 27.4308  Val_loss: 12.9213 \n",
      "Epoch 73/500\n",
      "383/383 [==============================] - trainLoss: 26.7775  Val_loss: 12.8361 \n",
      "Epoch 74/500\n",
      "383/383 [==============================] - trainLoss: 27.0147  Val_loss: 12.7686 \n",
      "Epoch 75/500\n",
      "383/383 [==============================] - trainLoss: 25.9183  Val_loss: 12.7618 \n",
      "WARNING:tensorflow:No training configuration found in save file: the model was *not* compiled. Compile it manually.\n",
      "WARNING:tensorflow:No training configuration found in save file: the model was *not* compiled. Compile it manually.\n",
      "WARNING:tensorflow:No training configuration found in save file: the model was *not* compiled. Compile it manually.\n",
      "WARNING:tensorflow:No training configuration found in save file: the model was *not* compiled. Compile it manually.\n",
      "Elapsed/s:  49.24205183982849\n",
      "Final training loss:  tf.Tensor(25.918346, shape=(), dtype=float32)\n",
      "best val loss:  tf.Tensor(12.73137, shape=(), dtype=float32)\n",
      "{0: {'parameters': {'codings_size': 208, 'n_hidden_classifier': 1, 'N': 100, 'beta': 15, 'n_neurons': 516, 'n_neurons_classifier': 43, 'learning_rate': 0.001, 'n_hidden': 1}, 'val_loss': <tf.Tensor: shape=(), dtype=float32, numpy=12.73137>}}\n",
      "Epoch 0/500\n",
      "WARNING:tensorflow:Layer full_model_only_labelled is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because it's dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "320/383 [========================>.....] - Loss for batch: 43.8957WARNING:tensorflow:Layer concatenate_1 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because it's dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "383/383 [==============================] - trainLoss: 43.8957  Val_loss: 23.0490 \n",
      "Epoch 1/500\n",
      "383/383 [==============================] - trainLoss: 28.6940  Val_loss: 20.4467 \n",
      "Epoch 2/500\n",
      "383/383 [==============================] - trainLoss: 25.4204  Val_loss: 18.8528 \n",
      "Epoch 3/500\n",
      "383/383 [==============================] - trainLoss: 23.4589  Val_loss: 17.8092 \n",
      "Epoch 4/500\n",
      "383/383 [==============================] - trainLoss: 22.0481  Val_loss: 17.0477 \n",
      "Epoch 5/500\n",
      "383/383 [==============================] - trainLoss: 21.1720  Val_loss: 16.5342 \n",
      "Epoch 6/500\n",
      "383/383 [==============================] - trainLoss: 20.1125  Val_loss: 16.0528 \n",
      "Epoch 7/500\n",
      "383/383 [==============================] - trainLoss: 19.6022  Val_loss: 15.6803 \n",
      "Epoch 8/500\n",
      "383/383 [==============================] - trainLoss: 19.0646  Val_loss: 15.3990 \n",
      "Epoch 9/500\n",
      "383/383 [==============================] - trainLoss: 18.4677  Val_loss: 15.1939 \n",
      "Epoch 10/500\n",
      "383/383 [==============================] - trainLoss: 18.1012  Val_loss: 14.9824 \n",
      "Epoch 11/500\n",
      "383/383 [==============================] - trainLoss: 17.6259  Val_loss: 14.7986 \n",
      "Epoch 12/500\n",
      "383/383 [==============================] - trainLoss: 17.1531  Val_loss: 14.6043 \n",
      "Epoch 13/500\n",
      "383/383 [==============================] - trainLoss: 16.7177  Val_loss: 14.4089 \n",
      "Epoch 14/500\n",
      "383/383 [==============================] - trainLoss: 17.1529  Val_loss: 14.2911 \n",
      "Epoch 15/500\n",
      "383/383 [==============================] - trainLoss: 16.6192  Val_loss: 14.1864 \n",
      "Epoch 16/500\n",
      "383/383 [==============================] - trainLoss: 15.9481  Val_loss: 14.0948 \n",
      "Epoch 17/500\n",
      "383/383 [==============================] - trainLoss: 16.0631  Val_loss: 14.0003 \n",
      "Epoch 18/500\n",
      "383/383 [==============================] - trainLoss: 16.0582  Val_loss: 13.8328 \n",
      "Epoch 19/500\n",
      "383/383 [==============================] - trainLoss: 15.8066  Val_loss: 13.7988 \n",
      "Epoch 20/500\n",
      "383/383 [==============================] - trainLoss: 15.4326  Val_loss: 13.8343 \n",
      "Epoch 21/500\n",
      "383/383 [==============================] - trainLoss: 15.5957  Val_loss: 13.8257 \n",
      "Epoch 22/500\n",
      "383/383 [==============================] - trainLoss: 15.3037  Val_loss: 13.7179 \n",
      "Epoch 23/500\n",
      "383/383 [==============================] - trainLoss: 14.7250  Val_loss: 13.5655 \n",
      "Epoch 24/500\n",
      "383/383 [==============================] - trainLoss: 15.0888  Val_loss: 13.4738 \n",
      "Epoch 25/500\n",
      "383/383 [==============================] - trainLoss: 14.9273  Val_loss: 13.3703 \n",
      "Epoch 26/500\n",
      "383/383 [==============================] - trainLoss: 15.2682  Val_loss: 13.3778 \n",
      "Epoch 27/500\n",
      "383/383 [==============================] - trainLoss: 14.8663  Val_loss: 13.4544 \n",
      "Epoch 28/500\n",
      "383/383 [==============================] - trainLoss: 14.5871  Val_loss: 13.4939 \n",
      "Epoch 29/500\n",
      "383/383 [==============================] - trainLoss: 14.8282  Val_loss: 13.3889 \n",
      "Epoch 30/500\n",
      "383/383 [==============================] - trainLoss: 14.4697  Val_loss: 13.2959 \n",
      "Epoch 31/500\n",
      "383/383 [==============================] - trainLoss: 14.5786  Val_loss: 13.2169 \n",
      "Epoch 32/500\n",
      "383/383 [==============================] - trainLoss: 14.4439  Val_loss: 13.1287 \n",
      "Epoch 33/500\n",
      "383/383 [==============================] - trainLoss: 14.5278  Val_loss: 13.1126 \n",
      "Epoch 34/500\n",
      "383/383 [==============================] - trainLoss: 13.9760  Val_loss: 13.1360 \n",
      "Epoch 35/500\n",
      "383/383 [==============================] - trainLoss: 14.3327  Val_loss: 13.0939 \n",
      "Epoch 36/500\n",
      "383/383 [==============================] - trainLoss: 14.2885  Val_loss: 13.1375 \n",
      "Epoch 37/500\n",
      "383/383 [==============================] - trainLoss: 14.2073  Val_loss: 13.1571 \n",
      "Epoch 38/500\n",
      "383/383 [==============================] - trainLoss: 13.9561  Val_loss: 13.1597 \n",
      "Epoch 39/500\n",
      "383/383 [==============================] - trainLoss: 13.5965  Val_loss: 13.1500 \n",
      "Epoch 40/500\n",
      "383/383 [==============================] - trainLoss: 13.9523  Val_loss: 13.1232 \n",
      "Epoch 41/500\n",
      "383/383 [==============================] - trainLoss: 13.5527  Val_loss: 13.0550 \n",
      "Epoch 42/500\n",
      "383/383 [==============================] - trainLoss: 13.3289  Val_loss: 13.0055 \n",
      "Epoch 43/500\n",
      "383/383 [==============================] - trainLoss: 13.2125  Val_loss: 12.9706 \n",
      "Epoch 44/500\n",
      "383/383 [==============================] - trainLoss: 13.7719  Val_loss: 12.9422 \n",
      "Epoch 45/500\n",
      "383/383 [==============================] - trainLoss: 13.4062  Val_loss: 12.9559 \n",
      "Epoch 46/500\n",
      "383/383 [==============================] - trainLoss: 13.4345  Val_loss: 12.9069 \n",
      "Epoch 47/500\n",
      "383/383 [==============================] - trainLoss: 13.5454  Val_loss: 12.9247 \n",
      "Epoch 48/500\n",
      "383/383 [==============================] - trainLoss: 13.6054  Val_loss: 12.8697 \n",
      "Epoch 49/500\n",
      "383/383 [==============================] - trainLoss: 13.6068  Val_loss: 12.8620 \n",
      "Epoch 50/500\n",
      "383/383 [==============================] - trainLoss: 13.2463  Val_loss: 12.8932 \n",
      "Epoch 51/500\n",
      "383/383 [==============================] - trainLoss: 12.9073  Val_loss: 12.9068 \n",
      "Epoch 52/500\n",
      "383/383 [==============================] - trainLoss: 13.0476  Val_loss: 12.8017 \n",
      "Epoch 53/500\n",
      "383/383 [==============================] - trainLoss: 13.2483  Val_loss: 12.7585 \n",
      "Epoch 54/500\n",
      "383/383 [==============================] - trainLoss: 13.1546  Val_loss: 12.7606 \n",
      "Epoch 55/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "383/383 [==============================] - trainLoss: 13.0800  Val_loss: 12.7978 \n",
      "Epoch 56/500\n",
      "383/383 [==============================] - trainLoss: 12.9159  Val_loss: 12.8688 \n",
      "Epoch 57/500\n",
      "383/383 [==============================] - trainLoss: 12.7190  Val_loss: 12.8232 \n",
      "Epoch 58/500\n",
      "383/383 [==============================] - trainLoss: 13.0620  Val_loss: 12.8096 \n",
      "Epoch 59/500\n",
      "383/383 [==============================] - trainLoss: 12.7859  Val_loss: 12.8139 \n",
      "Epoch 60/500\n",
      "383/383 [==============================] - trainLoss: 12.7502  Val_loss: 12.8181 \n",
      "Epoch 61/500\n",
      "383/383 [==============================] - trainLoss: 12.8353  Val_loss: 12.8021 \n",
      "Epoch 62/500\n",
      "383/383 [==============================] - trainLoss: 13.1002  Val_loss: 12.8019 \n",
      "WARNING:tensorflow:No training configuration found in save file: the model was *not* compiled. Compile it manually.\n",
      "WARNING:tensorflow:No training configuration found in save file: the model was *not* compiled. Compile it manually.\n",
      "WARNING:tensorflow:No training configuration found in save file: the model was *not* compiled. Compile it manually.\n",
      "WARNING:tensorflow:No training configuration found in save file: the model was *not* compiled. Compile it manually.\n",
      "Elapsed/s:  37.84289288520813\n",
      "Final training loss:  tf.Tensor(13.100234, shape=(), dtype=float32)\n",
      "best val loss:  tf.Tensor(12.758459, shape=(), dtype=float32)\n",
      "{0: {'parameters': {'codings_size': 208, 'n_hidden_classifier': 1, 'N': 100, 'beta': 15, 'n_neurons': 516, 'n_neurons_classifier': 43, 'learning_rate': 0.001, 'n_hidden': 1}, 'val_loss': <tf.Tensor: shape=(), dtype=float32, numpy=12.73137>}, 1: {'parameters': {'codings_size': 207, 'n_hidden_classifier': 1, 'N': 50, 'beta': 1, 'n_neurons': 152, 'n_neurons_classifier': 40, 'learning_rate': 0.001, 'n_hidden': 1}, 'val_loss': <tf.Tensor: shape=(), dtype=float32, numpy=12.758459>}}\n",
      "Epoch 0/500\n",
      "WARNING:tensorflow:Layer full_model_only_labelled is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because it's dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "320/383 [========================>.....] - Loss for batch: 1064.2135WARNING:tensorflow:Layer concatenate_1 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because it's dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "383/383 [==============================] - trainLoss: 1064.2135  Val_loss: 29.3528 \n",
      "Epoch 1/500\n",
      "383/383 [==============================] - trainLoss: 580.6856  Val_loss: 28.7481 \n",
      "Epoch 2/500\n",
      "383/383 [==============================] - trainLoss: 387.0367  Val_loss: 28.0247 \n",
      "Epoch 3/500\n",
      "383/383 [==============================] - trainLoss: 304.9713  Val_loss: 27.3328 \n",
      "Epoch 4/500\n",
      "383/383 [==============================] - trainLoss: 254.4966  Val_loss: 26.7279 \n",
      "Epoch 5/500\n",
      "383/383 [==============================] - trainLoss: 222.8054  Val_loss: 26.1580 \n",
      "Epoch 6/500\n",
      "383/383 [==============================] - trainLoss: 196.6568  Val_loss: 25.5560 \n",
      "Epoch 7/500\n",
      "383/383 [==============================] - trainLoss: 179.2685  Val_loss: 25.0318 \n",
      "Epoch 8/500\n",
      "383/383 [==============================] - trainLoss: 163.4557  Val_loss: 24.6117 \n",
      "Epoch 9/500\n",
      "383/383 [==============================] - trainLoss: 149.5020  Val_loss: 24.1021 \n",
      "Epoch 10/500\n",
      "383/383 [==============================] - trainLoss: 135.5120  Val_loss: 23.6609 \n",
      "Epoch 11/500\n",
      "383/383 [==============================] - trainLoss: 128.6676  Val_loss: 23.3317 \n",
      "Epoch 12/500\n",
      "383/383 [==============================] - trainLoss: 121.7100  Val_loss: 22.9865 \n",
      "Epoch 13/500\n",
      "383/383 [==============================] - trainLoss: 115.5909  Val_loss: 22.6490 \n",
      "Epoch 14/500\n",
      "383/383 [==============================] - trainLoss: 109.0861  Val_loss: 22.3736 \n",
      "Epoch 15/500\n",
      "383/383 [==============================] - trainLoss: 104.8793  Val_loss: 22.0917 \n",
      "Epoch 16/500\n",
      "383/383 [==============================] - trainLoss: 101.9683  Val_loss: 21.9007 \n",
      "Epoch 17/500\n",
      "383/383 [==============================] - trainLoss: 102.7038  Val_loss: 21.6636 \n",
      "Epoch 18/500\n",
      "383/383 [==============================] - trainLoss: 95.9767  Val_loss: 21.4504 \n",
      "Epoch 19/500\n",
      "383/383 [==============================] - trainLoss: 93.3738  Val_loss: 21.3421 \n",
      "Epoch 20/500\n",
      "383/383 [==============================] - trainLoss: 95.1423  Val_loss: 21.1122 \n",
      "Epoch 21/500\n",
      "383/383 [==============================] - trainLoss: 92.9974  Val_loss: 20.9309 \n",
      "Epoch 22/500\n",
      "383/383 [==============================] - trainLoss: 87.7008  Val_loss: 20.7623 \n",
      "Epoch 23/500\n",
      "383/383 [==============================] - trainLoss: 87.0685  Val_loss: 20.6677 \n",
      "Epoch 24/500\n",
      "383/383 [==============================] - trainLoss: 88.6725  Val_loss: 20.4789 \n",
      "Epoch 25/500\n",
      "383/383 [==============================] - trainLoss: 85.4700  Val_loss: 20.3840 \n",
      "Epoch 26/500\n",
      "383/383 [==============================] - trainLoss: 85.2988  Val_loss: 20.2855 \n",
      "Epoch 27/500\n",
      "383/383 [==============================] - trainLoss: 83.0328  Val_loss: 20.1941 \n",
      "Epoch 28/500\n",
      "383/383 [==============================] - trainLoss: 81.9519  Val_loss: 20.0731 \n",
      "Epoch 29/500\n",
      "383/383 [==============================] - trainLoss: 82.8176  Val_loss: 19.9635 \n",
      "Epoch 30/500\n",
      "383/383 [==============================] - trainLoss: 83.1586  Val_loss: 19.8708 \n",
      "Epoch 31/500\n",
      "383/383 [==============================] - trainLoss: 81.6575  Val_loss: 19.7724 \n",
      "Epoch 32/500\n",
      "383/383 [==============================] - trainLoss: 78.0097  Val_loss: 19.7281 \n",
      "Epoch 33/500\n",
      "383/383 [==============================] - trainLoss: 77.8683  Val_loss: 19.6104 \n",
      "Epoch 34/500\n",
      "383/383 [==============================] - trainLoss: 76.1593  Val_loss: 19.5316 \n",
      "Epoch 35/500\n",
      "383/383 [==============================] - trainLoss: 77.1640  Val_loss: 19.4376 \n",
      "Epoch 36/500\n",
      "383/383 [==============================] - trainLoss: 75.4948  Val_loss: 19.3787 \n",
      "Epoch 37/500\n",
      "383/383 [==============================] - trainLoss: 75.9744  Val_loss: 19.2901 \n",
      "Epoch 38/500\n",
      "383/383 [==============================] - trainLoss: 76.3972  Val_loss: 19.1827 \n",
      "Epoch 39/500\n",
      "383/383 [==============================] - trainLoss: 73.4121  Val_loss: 19.1380 \n",
      "Epoch 40/500\n",
      "383/383 [==============================] - trainLoss: 70.3298  Val_loss: 19.0783 \n",
      "Epoch 41/500\n",
      "383/383 [==============================] - trainLoss: 69.7804  Val_loss: 18.9719 \n",
      "Epoch 42/500\n",
      "383/383 [==============================] - trainLoss: 70.9075  Val_loss: 18.8806 \n",
      "Epoch 43/500\n",
      "383/383 [==============================] - trainLoss: 69.3320  Val_loss: 18.8193 \n",
      "Epoch 44/500\n",
      "383/383 [==============================] - trainLoss: 69.8948  Val_loss: 18.7761 \n",
      "Epoch 45/500\n",
      "383/383 [==============================] - trainLoss: 68.3845  Val_loss: 18.6816 \n",
      "Epoch 46/500\n",
      "383/383 [==============================] - trainLoss: 70.5608  Val_loss: 18.6073 \n",
      "Epoch 47/500\n",
      "383/383 [==============================] - trainLoss: 69.2520  Val_loss: 18.5608 \n",
      "Epoch 48/500\n",
      "383/383 [==============================] - trainLoss: 68.1704  Val_loss: 18.4565 \n",
      "Epoch 49/500\n",
      "383/383 [==============================] - trainLoss: 67.6238  Val_loss: 18.4099 \n",
      "Epoch 50/500\n",
      "383/383 [==============================] - trainLoss: 67.4167  Val_loss: 18.3179 \n",
      "Epoch 51/500\n",
      "383/383 [==============================] - trainLoss: 65.6320  Val_loss: 18.2648 \n",
      "Epoch 52/500\n",
      "383/383 [==============================] - trainLoss: 65.5941  Val_loss: 18.1860 \n",
      "Epoch 53/500\n",
      "383/383 [==============================] - trainLoss: 64.3756  Val_loss: 18.1065 \n",
      "Epoch 54/500\n",
      "383/383 [==============================] - trainLoss: 64.7681  Val_loss: 18.0357 \n",
      "Epoch 55/500\n",
      "383/383 [==============================] - trainLoss: 65.9595  Val_loss: 17.9408 \n",
      "Epoch 56/500\n",
      "383/383 [==============================] - trainLoss: 66.4770  Val_loss: 17.9024 \n",
      "Epoch 57/500\n",
      "383/383 [==============================] - trainLoss: 62.1495  Val_loss: 17.8616 \n",
      "Epoch 58/500\n",
      "383/383 [==============================] - trainLoss: 62.2953  Val_loss: 17.7900 \n",
      "Epoch 59/500\n",
      "383/383 [==============================] - trainLoss: 61.5313  Val_loss: 17.7520 \n",
      "Epoch 60/500\n",
      "383/383 [==============================] - trainLoss: 60.0495  Val_loss: 17.6609 \n",
      "Epoch 61/500\n",
      "383/383 [==============================] - trainLoss: 61.5784  Val_loss: 17.6188 \n",
      "Epoch 62/500\n",
      "383/383 [==============================] - trainLoss: 61.1198  Val_loss: 17.5863 \n",
      "Epoch 63/500\n",
      "383/383 [==============================] - trainLoss: 59.9047  Val_loss: 17.4661 \n",
      "Epoch 64/500\n",
      "383/383 [==============================] - trainLoss: 60.3244  Val_loss: 17.4242 \n",
      "Epoch 65/500\n",
      "383/383 [==============================] - trainLoss: 59.5546  Val_loss: 17.3838 \n",
      "Epoch 66/500\n",
      "383/383 [==============================] - trainLoss: 58.5120  Val_loss: 17.3175 \n",
      "Epoch 67/500\n",
      "383/383 [==============================] - trainLoss: 57.7619  Val_loss: 17.2416 \n",
      "Epoch 68/500\n",
      "383/383 [==============================] - trainLoss: 58.0113  Val_loss: 17.2033 \n",
      "Epoch 69/500\n",
      "383/383 [==============================] - trainLoss: 58.5317  Val_loss: 17.1674 \n",
      "Epoch 70/500\n",
      "383/383 [==============================] - trainLoss: 57.1656  Val_loss: 17.1124 \n",
      "Epoch 71/500\n",
      "383/383 [==============================] - trainLoss: 56.8745  Val_loss: 17.0450 \n",
      "Epoch 72/500\n",
      "383/383 [==============================] - trainLoss: 57.3750  Val_loss: 16.9692 \n",
      "Epoch 73/500\n",
      "383/383 [==============================] - trainLoss: 56.4996  Val_loss: 16.9232 \n",
      "Epoch 74/500\n",
      "383/383 [==============================] - trainLoss: 55.1835  Val_loss: 16.8322 \n",
      "Epoch 75/500\n",
      "383/383 [==============================] - trainLoss: 55.8690  Val_loss: 16.7991 \n",
      "Epoch 76/500\n",
      "383/383 [==============================] - trainLoss: 55.9454  Val_loss: 16.7335 \n",
      "Epoch 77/500\n",
      "383/383 [==============================] - trainLoss: 54.1710  Val_loss: 16.6998 \n",
      "Epoch 78/500\n",
      "383/383 [==============================] - trainLoss: 53.5393  Val_loss: 16.6543 \n",
      "Epoch 79/500\n",
      "383/383 [==============================] - trainLoss: 55.1344  Val_loss: 16.6031 \n",
      "Epoch 80/500\n",
      "383/383 [==============================] - trainLoss: 53.0221  Val_loss: 16.5676 \n",
      "Epoch 81/500\n",
      "383/383 [==============================] - trainLoss: 53.4922  Val_loss: 16.4925 \n",
      "Epoch 82/500\n",
      "383/383 [==============================] - trainLoss: 52.7227  Val_loss: 16.4618 \n",
      "Epoch 83/500\n",
      "383/383 [==============================] - trainLoss: 52.5266  Val_loss: 16.3853 \n",
      "Epoch 84/500\n",
      "383/383 [==============================] - trainLoss: 51.9837  Val_loss: 16.3690 \n",
      "Epoch 85/500\n",
      "383/383 [==============================] - trainLoss: 51.5875  Val_loss: 16.3089 \n",
      "Epoch 86/500\n",
      "383/383 [==============================] - trainLoss: 51.3276  Val_loss: 16.2734 \n",
      "Epoch 87/500\n",
      "383/383 [==============================] - trainLoss: 52.2638  Val_loss: 16.1846 \n",
      "Epoch 88/500\n",
      "383/383 [==============================] - trainLoss: 51.1917  Val_loss: 16.1490 \n",
      "Epoch 89/500\n",
      "383/383 [==============================] - trainLoss: 51.2307  Val_loss: 16.0893 \n",
      "Epoch 90/500\n",
      "383/383 [==============================] - trainLoss: 50.4667  Val_loss: 16.0378 \n",
      "Epoch 91/500\n",
      "383/383 [==============================] - trainLoss: 50.1001  Val_loss: 16.0161 \n",
      "Epoch 92/500\n",
      "383/383 [==============================] - trainLoss: 50.3382  Val_loss: 15.9564 \n",
      "Epoch 93/500\n",
      "383/383 [==============================] - trainLoss: 47.7470  Val_loss: 15.9370 \n",
      "Epoch 94/500\n",
      "383/383 [==============================] - trainLoss: 47.8269  Val_loss: 15.8602 \n",
      "Epoch 95/500\n",
      "383/383 [==============================] - trainLoss: 49.9322  Val_loss: 15.8366 \n",
      "Epoch 96/500\n",
      "383/383 [==============================] - trainLoss: 47.6533  Val_loss: 15.8031 \n",
      "Epoch 97/500\n",
      "383/383 [==============================] - trainLoss: 46.4448  Val_loss: 15.7592 \n",
      "Epoch 98/500\n",
      "383/383 [==============================] - trainLoss: 47.7180  Val_loss: 15.7089 \n",
      "Epoch 99/500\n",
      "383/383 [==============================] - trainLoss: 47.0001  Val_loss: 15.6826 \n",
      "Epoch 100/500\n",
      "383/383 [==============================] - trainLoss: 48.1046  Val_loss: 15.6382 \n",
      "Epoch 101/500\n",
      "383/383 [==============================] - trainLoss: 47.7416  Val_loss: 15.6025 \n",
      "Epoch 102/500\n",
      "383/383 [==============================] - trainLoss: 47.0566  Val_loss: 15.5512 \n",
      "Epoch 103/500\n",
      "383/383 [==============================] - trainLoss: 45.9897  Val_loss: 15.5339 \n",
      "Epoch 104/500\n",
      "383/383 [==============================] - trainLoss: 45.9267  Val_loss: 15.4912 \n",
      "Epoch 105/500\n",
      "383/383 [==============================] - trainLoss: 45.9697  Val_loss: 15.4841 \n",
      "Epoch 106/500\n",
      "383/383 [==============================] - trainLoss: 45.7753  Val_loss: 15.4354 \n",
      "Epoch 107/500\n",
      "383/383 [==============================] - trainLoss: 46.6527  Val_loss: 15.4128 \n",
      "Epoch 108/500\n",
      "383/383 [==============================] - trainLoss: 45.4949  Val_loss: 15.3850 \n",
      "Epoch 109/500\n",
      "383/383 [==============================] - trainLoss: 45.3297  Val_loss: 15.3371 \n",
      "Epoch 110/500\n",
      "383/383 [==============================] - trainLoss: 44.5473  Val_loss: 15.3045 \n",
      "Epoch 111/500\n",
      "383/383 [==============================] - trainLoss: 44.4040  Val_loss: 15.2667 \n",
      "Epoch 112/500\n",
      "383/383 [==============================] - trainLoss: 44.7425  Val_loss: 15.2458 \n",
      "Epoch 113/500\n",
      "383/383 [==============================] - trainLoss: 43.8888  Val_loss: 15.2190 \n",
      "Epoch 114/500\n",
      "383/383 [==============================] - trainLoss: 42.7504  Val_loss: 15.1886 \n",
      "Epoch 115/500\n",
      "383/383 [==============================] - trainLoss: 42.2467  Val_loss: 15.1432 \n",
      "Epoch 116/500\n",
      "383/383 [==============================] - trainLoss: 43.4521  Val_loss: 15.0983 \n",
      "Epoch 117/500\n",
      "383/383 [==============================] - trainLoss: 42.9055  Val_loss: 15.0538 \n",
      "Epoch 118/500\n",
      "383/383 [==============================] - trainLoss: 43.1921  Val_loss: 15.0369 \n",
      "Epoch 119/500\n",
      "383/383 [==============================] - trainLoss: 42.1339  Val_loss: 15.0088 \n",
      "Epoch 120/500\n",
      "383/383 [==============================] - trainLoss: 43.1301  Val_loss: 14.9872 \n",
      "Epoch 121/500\n",
      "383/383 [==============================] - trainLoss: 41.9702  Val_loss: 14.9548 \n",
      "Epoch 122/500\n",
      "383/383 [==============================] - trainLoss: 41.9270  Val_loss: 14.9093 \n",
      "Epoch 123/500\n",
      "383/383 [==============================] - trainLoss: 41.4125  Val_loss: 14.8954 \n",
      "Epoch 124/500\n",
      "383/383 [==============================] - trainLoss: 40.8391  Val_loss: 14.8821 \n",
      "Epoch 125/500\n",
      "383/383 [==============================] - trainLoss: 41.4281  Val_loss: 14.8476 \n",
      "Epoch 126/500\n",
      "383/383 [==============================] - trainLoss: 40.8986  Val_loss: 14.8296 \n",
      "Epoch 127/500\n",
      "383/383 [==============================] - trainLoss: 40.8127  Val_loss: 14.7890 \n",
      "Epoch 128/500\n",
      "383/383 [==============================] - trainLoss: 40.5566  Val_loss: 14.7639 \n",
      "Epoch 129/500\n",
      "383/383 [==============================] - trainLoss: 40.8751  Val_loss: 14.7274 \n",
      "Epoch 130/500\n",
      "383/383 [==============================] - trainLoss: 40.2193  Val_loss: 14.7138 \n",
      "Epoch 131/500\n",
      "383/383 [==============================] - trainLoss: 40.5511  Val_loss: 14.6835 \n",
      "Epoch 132/500\n",
      "383/383 [==============================] - trainLoss: 39.6618  Val_loss: 14.6740 \n",
      "Epoch 133/500\n",
      "383/383 [==============================] - trainLoss: 39.8831  Val_loss: 14.6362 \n",
      "Epoch 134/500\n",
      "383/383 [==============================] - trainLoss: 39.6687  Val_loss: 14.6055 \n",
      "Epoch 135/500\n",
      "383/383 [==============================] - trainLoss: 39.3382  Val_loss: 14.5937 \n",
      "Epoch 136/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "383/383 [==============================] - trainLoss: 39.3131  Val_loss: 14.5845 \n",
      "Epoch 137/500\n",
      "383/383 [==============================] - trainLoss: 39.6620  Val_loss: 14.5759 \n",
      "Epoch 138/500\n",
      "383/383 [==============================] - trainLoss: 39.0237  Val_loss: 14.5636 \n",
      "Epoch 139/500\n",
      "383/383 [==============================] - trainLoss: 38.2134  Val_loss: 14.5302 \n",
      "Epoch 140/500\n",
      "383/383 [==============================] - trainLoss: 38.2463  Val_loss: 14.4932 \n",
      "Epoch 141/500\n",
      "383/383 [==============================] - trainLoss: 38.9267  Val_loss: 14.4773 \n",
      "Epoch 142/500\n",
      "383/383 [==============================] - trainLoss: 38.4717  Val_loss: 14.4541 \n",
      "Epoch 143/500\n",
      "383/383 [==============================] - trainLoss: 37.1069  Val_loss: 14.4329 \n",
      "Epoch 144/500\n",
      "383/383 [==============================] - trainLoss: 36.9219  Val_loss: 14.4111 \n",
      "Epoch 145/500\n",
      "383/383 [==============================] - trainLoss: 37.6413  Val_loss: 14.4243 \n",
      "Epoch 146/500\n",
      "383/383 [==============================] - trainLoss: 37.2542  Val_loss: 14.4001 \n",
      "Epoch 147/500\n",
      "383/383 [==============================] - trainLoss: 37.5473  Val_loss: 14.3631 \n",
      "Epoch 148/500\n",
      "383/383 [==============================] - trainLoss: 36.9927  Val_loss: 14.3437 \n",
      "Epoch 149/500\n",
      "383/383 [==============================] - trainLoss: 36.9917  Val_loss: 14.2911 \n",
      "Epoch 150/500\n",
      "383/383 [==============================] - trainLoss: 36.7561  Val_loss: 14.2848 \n",
      "Epoch 151/500\n",
      "383/383 [==============================] - trainLoss: 36.7768  Val_loss: 14.2676 \n",
      "Epoch 152/500\n",
      "383/383 [==============================] - trainLoss: 36.5963  Val_loss: 14.2485 \n",
      "Epoch 153/500\n",
      "383/383 [==============================] - trainLoss: 35.9163  Val_loss: 14.2508 \n",
      "Epoch 154/500\n",
      "383/383 [==============================] - trainLoss: 35.5639  Val_loss: 14.2519 \n",
      "Epoch 155/500\n",
      "383/383 [==============================] - trainLoss: 36.4309  Val_loss: 14.2356 \n",
      "Epoch 156/500\n",
      "383/383 [==============================] - trainLoss: 36.3069  Val_loss: 14.2279 \n",
      "Epoch 157/500\n",
      "383/383 [==============================] - trainLoss: 35.2000  Val_loss: 14.1931 \n",
      "Epoch 158/500\n",
      "383/383 [==============================] - trainLoss: 35.3981  Val_loss: 14.1719 \n",
      "Epoch 159/500\n",
      "383/383 [==============================] - trainLoss: 35.5602  Val_loss: 14.1629 \n",
      "Epoch 160/500\n",
      "383/383 [==============================] - trainLoss: 34.9771  Val_loss: 14.1662 \n",
      "Epoch 161/500\n",
      "383/383 [==============================] - trainLoss: 34.9540  Val_loss: 14.1494 \n",
      "Epoch 162/500\n",
      "383/383 [==============================] - trainLoss: 34.7545  Val_loss: 14.1550 \n",
      "Epoch 163/500\n",
      "383/383 [==============================] - trainLoss: 34.3554  Val_loss: 14.1219 \n",
      "Epoch 164/500\n",
      "383/383 [==============================] - trainLoss: 34.5387  Val_loss: 14.1134 \n",
      "Epoch 165/500\n",
      "383/383 [==============================] - trainLoss: 34.3326  Val_loss: 14.0971 \n",
      "Epoch 166/500\n",
      "383/383 [==============================] - trainLoss: 33.9052  Val_loss: 14.0814 \n",
      "Epoch 167/500\n",
      "383/383 [==============================] - trainLoss: 34.7454  Val_loss: 14.0715 \n",
      "Epoch 168/500\n",
      "383/383 [==============================] - trainLoss: 34.0123  Val_loss: 14.0662 \n",
      "Epoch 169/500\n",
      "383/383 [==============================] - trainLoss: 33.2652  Val_loss: 14.0211 \n",
      "Epoch 170/500\n",
      "383/383 [==============================] - trainLoss: 33.2435  Val_loss: 14.0233 \n",
      "Epoch 171/500\n",
      "383/383 [==============================] - trainLoss: 34.1849  Val_loss: 14.0117 \n",
      "Epoch 172/500\n",
      "383/383 [==============================] - trainLoss: 33.5937  Val_loss: 13.9890 \n",
      "Epoch 173/500\n",
      "383/383 [==============================] - trainLoss: 33.2962  Val_loss: 13.9666 \n",
      "Epoch 174/500\n",
      "383/383 [==============================] - trainLoss: 32.4602  Val_loss: 13.9530 \n",
      "Epoch 175/500\n",
      "383/383 [==============================] - trainLoss: 32.7346  Val_loss: 13.9491 \n",
      "Epoch 176/500\n",
      "383/383 [==============================] - trainLoss: 32.4962  Val_loss: 13.9392 \n",
      "Epoch 177/500\n",
      "383/383 [==============================] - trainLoss: 33.0668  Val_loss: 13.9177 \n",
      "Epoch 178/500\n",
      "383/383 [==============================] - trainLoss: 32.2849  Val_loss: 13.8966 \n",
      "Epoch 179/500\n",
      "383/383 [==============================] - trainLoss: 32.3876  Val_loss: 13.8765 \n",
      "Epoch 180/500\n",
      "383/383 [==============================] - trainLoss: 32.0422  Val_loss: 13.8789 \n",
      "Epoch 181/500\n",
      "383/383 [==============================] - trainLoss: 31.4470  Val_loss: 13.8666 \n",
      "Epoch 182/500\n",
      "383/383 [==============================] - trainLoss: 31.8586  Val_loss: 13.8608 \n",
      "Epoch 183/500\n",
      "383/383 [==============================] - trainLoss: 31.8814  Val_loss: 13.8467 \n",
      "Epoch 184/500\n",
      "383/383 [==============================] - trainLoss: 32.1096  Val_loss: 13.8332 \n",
      "Epoch 185/500\n",
      "383/383 [==============================] - trainLoss: 31.7371  Val_loss: 13.8217 \n",
      "Epoch 186/500\n",
      "383/383 [==============================] - trainLoss: 31.7643  Val_loss: 13.8002 \n",
      "Epoch 187/500\n",
      "383/383 [==============================] - trainLoss: 31.5637  Val_loss: 13.7990 \n",
      "Epoch 188/500\n",
      "383/383 [==============================] - trainLoss: 30.4832  Val_loss: 13.7991 \n",
      "Epoch 189/500\n",
      "383/383 [==============================] - trainLoss: 32.0188  Val_loss: 13.7825 \n",
      "Epoch 190/500\n",
      "383/383 [==============================] - trainLoss: 31.2407  Val_loss: 13.7786 \n",
      "Epoch 191/500\n",
      "383/383 [==============================] - trainLoss: 31.3804  Val_loss: 13.7711 \n",
      "Epoch 192/500\n",
      "383/383 [==============================] - trainLoss: 29.9837  Val_loss: 13.7605 \n",
      "Epoch 193/500\n",
      "383/383 [==============================] - trainLoss: 30.2746  Val_loss: 13.7577 \n",
      "Epoch 194/500\n",
      "383/383 [==============================] - trainLoss: 30.6929  Val_loss: 13.7866 \n",
      "Epoch 195/500\n",
      "383/383 [==============================] - trainLoss: 30.5689  Val_loss: 13.7714 \n",
      "Epoch 196/500\n",
      "383/383 [==============================] - trainLoss: 29.6811  Val_loss: 13.7449 \n",
      "Epoch 197/500\n",
      "383/383 [==============================] - trainLoss: 29.9554  Val_loss: 13.7281 \n",
      "Epoch 198/500\n",
      "383/383 [==============================] - trainLoss: 30.4456  Val_loss: 13.7179 \n",
      "Epoch 199/500\n",
      "383/383 [==============================] - trainLoss: 29.5380  Val_loss: 13.6994 \n",
      "Epoch 200/500\n",
      "383/383 [==============================] - trainLoss: 30.5590  Val_loss: 13.7137 \n",
      "Epoch 201/500\n",
      "383/383 [==============================] - trainLoss: 29.5909  Val_loss: 13.6811 \n",
      "Epoch 202/500\n",
      "383/383 [==============================] - trainLoss: 29.7251  Val_loss: 13.6476 \n",
      "Epoch 203/500\n",
      "383/383 [==============================] - trainLoss: 28.7267  Val_loss: 13.6521 \n",
      "Epoch 204/500\n",
      "383/383 [==============================] - trainLoss: 29.2200  Val_loss: 13.6252 \n",
      "Epoch 205/500\n",
      "383/383 [==============================] - trainLoss: 29.8731  Val_loss: 13.6062 \n",
      "Epoch 206/500\n",
      "383/383 [==============================] - trainLoss: 28.9666  Val_loss: 13.6154 \n",
      "Epoch 207/500\n",
      "383/383 [==============================] - trainLoss: 29.2969  Val_loss: 13.6046 \n",
      "Epoch 208/500\n",
      "383/383 [==============================] - trainLoss: 28.7074  Val_loss: 13.5929 \n",
      "Epoch 209/500\n",
      "383/383 [==============================] - trainLoss: 28.8330  Val_loss: 13.5831 \n",
      "Epoch 210/500\n",
      "383/383 [==============================] - trainLoss: 28.7947  Val_loss: 13.5787 \n",
      "Epoch 211/500\n",
      "383/383 [==============================] - trainLoss: 28.9498  Val_loss: 13.5821 \n",
      "Epoch 212/500\n",
      "383/383 [==============================] - trainLoss: 28.2218  Val_loss: 13.5656 \n",
      "Epoch 213/500\n",
      "383/383 [==============================] - trainLoss: 28.1846  Val_loss: 13.5501 \n",
      "Epoch 214/500\n",
      "383/383 [==============================] - trainLoss: 27.8709  Val_loss: 13.5494 \n",
      "Epoch 215/500\n",
      "383/383 [==============================] - trainLoss: 27.7856  Val_loss: 13.5266 \n",
      "Epoch 216/500\n",
      "383/383 [==============================] - trainLoss: 27.8563  Val_loss: 13.5121 \n",
      "Epoch 217/500\n",
      "383/383 [==============================] - trainLoss: 28.2664  Val_loss: 13.4956 \n",
      "Epoch 218/500\n",
      "383/383 [==============================] - trainLoss: 28.1397  Val_loss: 13.4705 \n",
      "Epoch 219/500\n",
      "383/383 [==============================] - trainLoss: 28.2625  Val_loss: 13.4833 \n",
      "Epoch 220/500\n",
      "383/383 [==============================] - trainLoss: 26.8407  Val_loss: 13.4696 \n",
      "Epoch 221/500\n",
      "383/383 [==============================] - trainLoss: 27.3141  Val_loss: 13.4710 \n",
      "Epoch 222/500\n",
      "383/383 [==============================] - trainLoss: 27.9471  Val_loss: 13.4741 \n",
      "Epoch 223/500\n",
      "383/383 [==============================] - trainLoss: 27.1030  Val_loss: 13.4840 \n",
      "Epoch 224/500\n",
      "383/383 [==============================] - trainLoss: 26.7834  Val_loss: 13.4654 \n",
      "Epoch 225/500\n",
      "383/383 [==============================] - trainLoss: 26.3992  Val_loss: 13.4432 \n",
      "Epoch 226/500\n",
      "383/383 [==============================] - trainLoss: 27.1699  Val_loss: 13.4578 \n",
      "Epoch 227/500\n",
      "383/383 [==============================] - trainLoss: 26.9182  Val_loss: 13.4521 \n",
      "Epoch 228/500\n",
      "383/383 [==============================] - trainLoss: 26.8528  Val_loss: 13.4383 \n",
      "Epoch 229/500\n",
      "383/383 [==============================] - trainLoss: 26.1211  Val_loss: 13.4334 \n",
      "Epoch 230/500\n",
      "383/383 [==============================] - trainLoss: 27.0772  Val_loss: 13.4303 \n",
      "Epoch 231/500\n",
      "383/383 [==============================] - trainLoss: 26.7216  Val_loss: 13.4236 \n",
      "Epoch 232/500\n",
      "383/383 [==============================] - trainLoss: 26.0364  Val_loss: 13.3992 \n",
      "Epoch 233/500\n",
      "383/383 [==============================] - trainLoss: 26.2827  Val_loss: 13.4068 \n",
      "Epoch 234/500\n",
      "383/383 [==============================] - trainLoss: 25.6763  Val_loss: 13.4036 \n",
      "Epoch 235/500\n",
      "383/383 [==============================] - trainLoss: 26.5492  Val_loss: 13.4048 \n",
      "Epoch 236/500\n",
      "383/383 [==============================] - trainLoss: 26.1853  Val_loss: 13.4092 \n",
      "Epoch 237/500\n",
      "383/383 [==============================] - trainLoss: 25.4132  Val_loss: 13.4070 \n",
      "Epoch 238/500\n",
      "383/383 [==============================] - trainLoss: 25.7847  Val_loss: 13.4208 \n",
      "Epoch 239/500\n",
      "383/383 [==============================] - trainLoss: 25.8768  Val_loss: 13.3985 \n",
      "Epoch 240/500\n",
      "383/383 [==============================] - trainLoss: 25.3708  Val_loss: 13.3819 \n",
      "Epoch 241/500\n",
      "383/383 [==============================] - trainLoss: 25.3205  Val_loss: 13.3757 \n",
      "Epoch 242/500\n",
      "383/383 [==============================] - trainLoss: 25.3134  Val_loss: 13.3695 \n",
      "Epoch 243/500\n",
      "383/383 [==============================] - trainLoss: 25.3674  Val_loss: 13.3577 \n",
      "Epoch 244/500\n",
      "383/383 [==============================] - trainLoss: 26.1417  Val_loss: 13.3451 \n",
      "Epoch 245/500\n",
      "383/383 [==============================] - trainLoss: 25.2091  Val_loss: 13.3545 \n",
      "Epoch 246/500\n",
      "383/383 [==============================] - trainLoss: 25.2584  Val_loss: 13.3628 \n",
      "Epoch 247/500\n",
      "383/383 [==============================] - trainLoss: 24.7710  Val_loss: 13.3408 \n",
      "Epoch 248/500\n",
      "383/383 [==============================] - trainLoss: 24.8432  Val_loss: 13.3324 \n",
      "Epoch 249/500\n",
      "383/383 [==============================] - trainLoss: 24.3659  Val_loss: 13.3283 \n",
      "Epoch 250/500\n",
      "383/383 [==============================] - trainLoss: 24.9648  Val_loss: 13.3139 \n",
      "Epoch 251/500\n",
      "383/383 [==============================] - trainLoss: 24.0692  Val_loss: 13.3156 \n",
      "Epoch 252/500\n",
      "383/383 [==============================] - trainLoss: 24.6695  Val_loss: 13.3075 \n",
      "Epoch 253/500\n",
      "383/383 [==============================] - trainLoss: 24.4461  Val_loss: 13.3056 \n",
      "Epoch 254/500\n",
      "383/383 [==============================] - trainLoss: 24.1150  Val_loss: 13.2912 \n",
      "Epoch 255/500\n",
      "383/383 [==============================] - trainLoss: 24.1344  Val_loss: 13.3034 \n",
      "Epoch 256/500\n",
      "383/383 [==============================] - trainLoss: 24.3211  Val_loss: 13.2931 \n",
      "Epoch 257/500\n",
      "383/383 [==============================] - trainLoss: 24.1659  Val_loss: 13.2900 \n",
      "Epoch 258/500\n",
      "383/383 [==============================] - trainLoss: 24.1613  Val_loss: 13.2644 \n",
      "Epoch 259/500\n",
      "383/383 [==============================] - trainLoss: 23.5578  Val_loss: 13.2612 \n",
      "Epoch 260/500\n",
      "383/383 [==============================] - trainLoss: 23.9134  Val_loss: 13.2639 \n",
      "Epoch 261/500\n",
      "383/383 [==============================] - trainLoss: 23.7477  Val_loss: 13.2671 \n",
      "Epoch 262/500\n",
      "383/383 [==============================] - trainLoss: 23.6768  Val_loss: 13.2604 \n",
      "Epoch 263/500\n",
      "383/383 [==============================] - trainLoss: 23.3094  Val_loss: 13.2521 \n",
      "Epoch 264/500\n",
      "383/383 [==============================] - trainLoss: 23.6637  Val_loss: 13.2517 \n",
      "Epoch 265/500\n",
      "383/383 [==============================] - trainLoss: 23.1720  Val_loss: 13.2572 \n",
      "Epoch 266/500\n",
      "383/383 [==============================] - trainLoss: 23.0636  Val_loss: 13.2534 \n",
      "Epoch 267/500\n",
      "383/383 [==============================] - trainLoss: 23.8306  Val_loss: 13.2518 \n",
      "Epoch 268/500\n",
      "383/383 [==============================] - trainLoss: 23.1748  Val_loss: 13.2643 \n",
      "Epoch 269/500\n",
      "383/383 [==============================] - trainLoss: 22.7546  Val_loss: 13.2598 \n",
      "Epoch 270/500\n",
      "383/383 [==============================] - trainLoss: 22.8657  Val_loss: 13.2563 \n",
      "Epoch 271/500\n",
      "383/383 [==============================] - trainLoss: 23.2303  Val_loss: 13.2440 \n",
      "Epoch 272/500\n",
      "383/383 [==============================] - trainLoss: 23.0446  Val_loss: 13.2453 \n",
      "Epoch 273/500\n",
      "383/383 [==============================] - trainLoss: 23.1197  Val_loss: 13.2249 \n",
      "Epoch 274/500\n",
      "383/383 [==============================] - trainLoss: 22.9831  Val_loss: 13.2394 \n",
      "Epoch 275/500\n",
      "383/383 [==============================] - trainLoss: 22.9735  Val_loss: 13.2303 \n",
      "Epoch 276/500\n",
      "383/383 [==============================] - trainLoss: 22.4244  Val_loss: 13.2264 \n",
      "Epoch 277/500\n",
      "383/383 [==============================] - trainLoss: 22.7128  Val_loss: 13.2057 \n",
      "Epoch 278/500\n",
      "383/383 [==============================] - trainLoss: 22.7589  Val_loss: 13.2096 \n",
      "Epoch 279/500\n",
      "383/383 [==============================] - trainLoss: 22.7176  Val_loss: 13.2237 \n",
      "Epoch 280/500\n",
      "383/383 [==============================] - trainLoss: 22.2161  Val_loss: 13.2093 \n",
      "Epoch 281/500\n",
      "383/383 [==============================] - trainLoss: 21.9842  Val_loss: 13.2059 \n",
      "Epoch 282/500\n",
      "383/383 [==============================] - trainLoss: 22.4176  Val_loss: 13.2026 \n",
      "Epoch 283/500\n",
      "383/383 [==============================] - trainLoss: 22.6690  Val_loss: 13.1996 \n",
      "Epoch 284/500\n",
      "383/383 [==============================] - trainLoss: 22.4172  Val_loss: 13.2065 \n",
      "Epoch 285/500\n",
      "383/383 [==============================] - trainLoss: 21.6728  Val_loss: 13.1881 \n",
      "Epoch 286/500\n",
      "383/383 [==============================] - trainLoss: 22.1898  Val_loss: 13.1906 \n",
      "Epoch 287/500\n",
      "383/383 [==============================] - trainLoss: 22.4219  Val_loss: 13.2029 \n",
      "Epoch 288/500\n",
      "383/383 [==============================] - trainLoss: 22.0215  Val_loss: 13.1876 \n",
      "Epoch 289/500\n",
      "383/383 [==============================] - trainLoss: 21.8303  Val_loss: 13.1674 \n",
      "Epoch 290/500\n",
      "383/383 [==============================] - trainLoss: 21.5745  Val_loss: 13.1596 \n",
      "Epoch 291/500\n",
      "383/383 [==============================] - trainLoss: 21.8394  Val_loss: 13.1720 \n",
      "Epoch 292/500\n",
      "383/383 [==============================] - trainLoss: 21.5828  Val_loss: 13.1719 \n",
      "Epoch 293/500\n",
      "383/383 [==============================] - trainLoss: 21.4454  Val_loss: 13.1548 \n",
      "Epoch 294/500\n",
      "383/383 [==============================] - trainLoss: 21.1476  Val_loss: 13.1584 \n",
      "Epoch 295/500\n",
      "383/383 [==============================] - trainLoss: 21.0451  Val_loss: 13.1475 \n",
      "Epoch 296/500\n",
      "383/383 [==============================] - trainLoss: 21.9170  Val_loss: 13.1584 \n",
      "Epoch 297/500\n",
      "383/383 [==============================] - trainLoss: 21.0418  Val_loss: 13.1480 \n",
      "Epoch 298/500\n",
      "383/383 [==============================] - trainLoss: 21.2285  Val_loss: 13.1261 \n",
      "Epoch 299/500\n",
      "383/383 [==============================] - trainLoss: 21.1021  Val_loss: 13.1331 \n",
      "Epoch 300/500\n",
      "383/383 [==============================] - trainLoss: 21.3339  Val_loss: 13.1251 \n",
      "Epoch 301/500\n",
      "383/383 [==============================] - trainLoss: 21.1418  Val_loss: 13.1369 \n",
      "Epoch 302/500\n",
      "383/383 [==============================] - trainLoss: 20.9874  Val_loss: 13.1170 \n",
      "Epoch 303/500\n",
      "383/383 [==============================] - trainLoss: 21.1140  Val_loss: 13.1022 \n",
      "Epoch 304/500\n",
      "383/383 [==============================] - trainLoss: 21.1773  Val_loss: 13.1064 \n",
      "Epoch 305/500\n",
      "383/383 [==============================] - trainLoss: 20.6512  Val_loss: 13.1044 \n",
      "Epoch 306/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "383/383 [==============================] - trainLoss: 20.9254  Val_loss: 13.0892 \n",
      "Epoch 307/500\n",
      "383/383 [==============================] - trainLoss: 20.2571  Val_loss: 13.0988 \n",
      "Epoch 308/500\n",
      "383/383 [==============================] - trainLoss: 20.5562  Val_loss: 13.0918 \n",
      "Epoch 309/500\n",
      "383/383 [==============================] - trainLoss: 20.2764  Val_loss: 13.0839 \n",
      "Epoch 310/500\n",
      "383/383 [==============================] - trainLoss: 19.9730  Val_loss: 13.0725 \n",
      "Epoch 311/500\n",
      "383/383 [==============================] - trainLoss: 20.1566  Val_loss: 13.0520 \n",
      "Epoch 312/500\n",
      "383/383 [==============================] - trainLoss: 20.3394  Val_loss: 13.0443 \n",
      "Epoch 313/500\n",
      "383/383 [==============================] - trainLoss: 20.5328  Val_loss: 13.0357 \n",
      "Epoch 314/500\n",
      "383/383 [==============================] - trainLoss: 20.1898  Val_loss: 13.0406 \n",
      "Epoch 315/500\n",
      "383/383 [==============================] - trainLoss: 20.3756  Val_loss: 13.0266 \n",
      "Epoch 316/500\n",
      "383/383 [==============================] - trainLoss: 20.2545  Val_loss: 13.0266 \n",
      "Epoch 317/500\n",
      "383/383 [==============================] - trainLoss: 20.1676  Val_loss: 13.0310 \n",
      "Epoch 318/500\n",
      "383/383 [==============================] - trainLoss: 19.6452  Val_loss: 13.0312 \n",
      "Epoch 319/500\n",
      "383/383 [==============================] - trainLoss: 19.5671  Val_loss: 13.0340 \n",
      "Epoch 320/500\n",
      "383/383 [==============================] - trainLoss: 19.8344  Val_loss: 13.0404 \n",
      "Epoch 321/500\n",
      "383/383 [==============================] - trainLoss: 19.7373  Val_loss: 13.0394 \n",
      "Epoch 322/500\n",
      "383/383 [==============================] - trainLoss: 19.3740  Val_loss: 13.0261 \n",
      "Epoch 323/500\n",
      "383/383 [==============================] - trainLoss: 19.8721  Val_loss: 13.0144 \n",
      "Epoch 324/500\n",
      "383/383 [==============================] - trainLoss: 19.5319  Val_loss: 13.0158 \n",
      "Epoch 325/500\n",
      "383/383 [==============================] - trainLoss: 19.3652  Val_loss: 13.0287 \n",
      "Epoch 326/500\n",
      "383/383 [==============================] - trainLoss: 19.1656  Val_loss: 13.0275 \n",
      "Epoch 327/500\n",
      "383/383 [==============================] - trainLoss: 19.7712  Val_loss: 13.0292 \n",
      "Epoch 328/500\n",
      "383/383 [==============================] - trainLoss: 19.3766  Val_loss: 13.0410 \n",
      "Epoch 329/500\n",
      "383/383 [==============================] - trainLoss: 19.2490  Val_loss: 13.0338 \n",
      "Epoch 330/500\n",
      "383/383 [==============================] - trainLoss: 19.1483  Val_loss: 13.0377 \n",
      "Epoch 331/500\n",
      "383/383 [==============================] - trainLoss: 19.2831  Val_loss: 13.0399 \n",
      "Epoch 332/500\n",
      "383/383 [==============================] - trainLoss: 19.2455  Val_loss: 13.0427 \n",
      "WARNING:tensorflow:No training configuration found in save file: the model was *not* compiled. Compile it manually.\n",
      "WARNING:tensorflow:No training configuration found in save file: the model was *not* compiled. Compile it manually.\n",
      "WARNING:tensorflow:No training configuration found in save file: the model was *not* compiled. Compile it manually.\n",
      "WARNING:tensorflow:No training configuration found in save file: the model was *not* compiled. Compile it manually.\n",
      "Elapsed/s:  198.10134387016296\n",
      "Final training loss:  tf.Tensor(19.245537, shape=(), dtype=float32)\n",
      "best val loss:  tf.Tensor(13.014389, shape=(), dtype=float32)\n",
      "{0: {'parameters': {'codings_size': 208, 'n_hidden_classifier': 1, 'N': 100, 'beta': 15, 'n_neurons': 516, 'n_neurons_classifier': 43, 'learning_rate': 0.001, 'n_hidden': 1}, 'val_loss': <tf.Tensor: shape=(), dtype=float32, numpy=12.73137>}, 1: {'parameters': {'codings_size': 207, 'n_hidden_classifier': 1, 'N': 50, 'beta': 1, 'n_neurons': 152, 'n_neurons_classifier': 40, 'learning_rate': 0.001, 'n_hidden': 1}, 'val_loss': <tf.Tensor: shape=(), dtype=float32, numpy=12.758459>}, 2: {'parameters': {'codings_size': 177, 'n_hidden_classifier': 2, 'N': 50, 'beta': 15, 'n_neurons': 398, 'n_neurons_classifier': 80, 'learning_rate': 0.0001, 'n_hidden': 1}, 'val_loss': <tf.Tensor: shape=(), dtype=float32, numpy=13.014389>}}\n",
      "Epoch 0/500\n",
      "WARNING:tensorflow:Layer full_model_only_labelled is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because it's dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "320/383 [========================>.....] - Loss for batch: 204.5078WARNING:tensorflow:Layer concatenate_1 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because it's dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "383/383 [==============================] - trainLoss: 204.5078  Val_loss: 22.1927 \n",
      "Epoch 1/500\n",
      "383/383 [==============================] - trainLoss: 70.4063  Val_loss: 19.2217 \n",
      "Epoch 2/500\n",
      "383/383 [==============================] - trainLoss: 47.8456  Val_loss: 17.9301 \n",
      "Epoch 3/500\n",
      "383/383 [==============================] - trainLoss: 38.7846  Val_loss: 16.9407 \n",
      "Epoch 4/500\n",
      "383/383 [==============================] - trainLoss: 34.1801  Val_loss: 16.1020 \n",
      "Epoch 5/500\n",
      "383/383 [==============================] - trainLoss: 31.4878  Val_loss: 15.4337 \n",
      "Epoch 6/500\n",
      "383/383 [==============================] - trainLoss: 29.1441  Val_loss: 15.0001 \n",
      "Epoch 7/500\n",
      "383/383 [==============================] - trainLoss: 27.1442  Val_loss: 14.7799 \n",
      "Epoch 8/500\n",
      "383/383 [==============================] - trainLoss: 24.8015  Val_loss: 14.6468 \n",
      "Epoch 9/500\n",
      "383/383 [==============================] - trainLoss: 24.3005  Val_loss: 14.5181 \n",
      "Epoch 10/500\n",
      "383/383 [==============================] - trainLoss: 23.9382  Val_loss: 14.3587 \n",
      "Epoch 11/500\n",
      "383/383 [==============================] - trainLoss: 22.4352  Val_loss: 14.2025 \n",
      "Epoch 12/500\n",
      "383/383 [==============================] - trainLoss: 21.6083  Val_loss: 14.0303 \n",
      "Epoch 13/500\n",
      "383/383 [==============================] - trainLoss: 21.4531  Val_loss: 13.9268 \n",
      "Epoch 14/500\n",
      "383/383 [==============================] - trainLoss: 20.5140  Val_loss: 13.8467 \n",
      "Epoch 15/500\n",
      "383/383 [==============================] - trainLoss: 19.5116  Val_loss: 13.7227 \n",
      "Epoch 16/500\n",
      "383/383 [==============================] - trainLoss: 18.9800  Val_loss: 13.6892 \n",
      "Epoch 17/500\n",
      "383/383 [==============================] - trainLoss: 18.2568  Val_loss: 13.6222 \n",
      "Epoch 18/500\n",
      "383/383 [==============================] - trainLoss: 17.8145  Val_loss: 13.5614 \n",
      "Epoch 19/500\n",
      "383/383 [==============================] - trainLoss: 17.0373  Val_loss: 13.4794 \n",
      "Epoch 20/500\n",
      "383/383 [==============================] - trainLoss: 16.4644  Val_loss: 13.5559 \n",
      "Epoch 21/500\n",
      "383/383 [==============================] - trainLoss: 16.2928  Val_loss: 13.5317 \n",
      "Epoch 22/500\n",
      "383/383 [==============================] - trainLoss: 15.7226  Val_loss: 13.4245 \n",
      "Epoch 23/500\n",
      "383/383 [==============================] - trainLoss: 15.4314  Val_loss: 13.3147 \n",
      "Epoch 24/500\n",
      "383/383 [==============================] - trainLoss: 14.9750  Val_loss: 13.2612 \n",
      "Epoch 25/500\n",
      "383/383 [==============================] - trainLoss: 14.5075  Val_loss: 13.2992 \n",
      "Epoch 26/500\n",
      "383/383 [==============================] - trainLoss: 14.3057  Val_loss: 13.4066 \n",
      "Epoch 27/500\n",
      "383/383 [==============================] - trainLoss: 13.9619  Val_loss: 13.4032 \n",
      "Epoch 28/500\n",
      "383/383 [==============================] - trainLoss: 13.8180  Val_loss: 13.3550 \n",
      "Epoch 29/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "383/383 [==============================] - trainLoss: 13.2795  Val_loss: 13.1720 \n",
      "Epoch 30/500\n",
      "383/383 [==============================] - trainLoss: 12.5820  Val_loss: 13.2352 \n",
      "Epoch 31/500\n",
      "383/383 [==============================] - trainLoss: 12.6542  Val_loss: 13.2733 \n",
      "Epoch 32/500\n",
      "383/383 [==============================] - trainLoss: 12.3948  Val_loss: 13.3122 \n",
      "Epoch 33/500\n",
      "383/383 [==============================] - trainLoss: 12.1986  Val_loss: 13.1443 \n",
      "Epoch 34/500\n",
      "383/383 [==============================] - trainLoss: 11.8291  Val_loss: 13.0422 \n",
      "Epoch 35/500\n",
      "383/383 [==============================] - trainLoss: 11.4594  Val_loss: 13.0038 \n",
      "Epoch 36/500\n",
      "383/383 [==============================] - trainLoss: 11.4230  Val_loss: 12.9791 \n",
      "Epoch 37/500\n",
      "383/383 [==============================] - trainLoss: 11.1419  Val_loss: 13.1620 \n",
      "Epoch 38/500\n",
      "383/383 [==============================] - trainLoss: 11.1207  Val_loss: 13.1670 \n",
      "Epoch 39/500\n",
      "383/383 [==============================] - trainLoss: 10.8792  Val_loss: 13.1483 \n",
      "Epoch 40/500\n",
      "383/383 [==============================] - trainLoss: 10.8511  Val_loss: 13.0968 \n",
      "Epoch 41/500\n",
      "383/383 [==============================] - trainLoss: 10.5126  Val_loss: 13.0153 \n",
      "Epoch 42/500\n",
      "383/383 [==============================] - trainLoss: 10.4750  Val_loss: 13.0245 \n",
      "Epoch 43/500\n",
      "383/383 [==============================] - trainLoss: 9.9454  Val_loss: 13.0867 \n",
      "Epoch 44/500\n",
      "383/383 [==============================] - trainLoss: 10.0709  Val_loss: 13.0497 \n",
      "Epoch 45/500\n",
      "383/383 [==============================] - trainLoss: 9.9340  Val_loss: 12.9967 \n",
      "WARNING:tensorflow:No training configuration found in save file: the model was *not* compiled. Compile it manually.\n",
      "WARNING:tensorflow:No training configuration found in save file: the model was *not* compiled. Compile it manually.\n",
      "WARNING:tensorflow:No training configuration found in save file: the model was *not* compiled. Compile it manually.\n",
      "WARNING:tensorflow:No training configuration found in save file: the model was *not* compiled. Compile it manually.\n",
      "Elapsed/s:  24.111103773117065\n",
      "Final training loss:  tf.Tensor(9.93402, shape=(), dtype=float32)\n",
      "best val loss:  tf.Tensor(12.97913, shape=(), dtype=float32)\n",
      "{0: {'parameters': {'codings_size': 208, 'n_hidden_classifier': 1, 'N': 100, 'beta': 15, 'n_neurons': 516, 'n_neurons_classifier': 43, 'learning_rate': 0.001, 'n_hidden': 1}, 'val_loss': <tf.Tensor: shape=(), dtype=float32, numpy=12.73137>}, 1: {'parameters': {'codings_size': 207, 'n_hidden_classifier': 1, 'N': 50, 'beta': 1, 'n_neurons': 152, 'n_neurons_classifier': 40, 'learning_rate': 0.001, 'n_hidden': 1}, 'val_loss': <tf.Tensor: shape=(), dtype=float32, numpy=12.758459>}, 2: {'parameters': {'codings_size': 177, 'n_hidden_classifier': 2, 'N': 50, 'beta': 15, 'n_neurons': 398, 'n_neurons_classifier': 80, 'learning_rate': 0.0001, 'n_hidden': 1}, 'val_loss': <tf.Tensor: shape=(), dtype=float32, numpy=13.014389>}, 3: {'parameters': {'codings_size': 57, 'n_hidden_classifier': 2, 'N': 1, 'beta': 15, 'n_neurons': 516, 'n_neurons_classifier': 43, 'learning_rate': 0.001, 'n_hidden': 1}, 'val_loss': <tf.Tensor: shape=(), dtype=float32, numpy=12.97913>}}\n",
      "Epoch 0/500\n",
      "WARNING:tensorflow:Layer full_model_only_labelled is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because it's dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "320/383 [========================>.....] - Loss for batch: 143.5652WARNING:tensorflow:Layer concatenate_1 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because it's dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "383/383 [==============================] - trainLoss: 143.5652  Val_loss: 29.6471 \n",
      "Epoch 1/500\n",
      "383/383 [==============================] - trainLoss: 96.8239  Val_loss: 28.1509 \n",
      "Epoch 2/500\n",
      "383/383 [==============================] - trainLoss: 82.1855  Val_loss: 26.9433 \n",
      "Epoch 3/500\n",
      "383/383 [==============================] - trainLoss: 72.9095  Val_loss: 25.8812 \n",
      "Epoch 4/500\n",
      "383/383 [==============================] - trainLoss: 67.3613  Val_loss: 24.9799 \n",
      "Epoch 5/500\n",
      "383/383 [==============================] - trainLoss: 62.6646  Val_loss: 24.3067 \n",
      "Epoch 6/500\n",
      "383/383 [==============================] - trainLoss: 58.0784  Val_loss: 23.7023 \n",
      "Epoch 7/500\n",
      "383/383 [==============================] - trainLoss: 55.7625  Val_loss: 23.2111 \n",
      "Epoch 8/500\n",
      "383/383 [==============================] - trainLoss: 53.5796  Val_loss: 22.8539 \n",
      "Epoch 9/500\n",
      "383/383 [==============================] - trainLoss: 51.8328  Val_loss: 22.5536 \n",
      "Epoch 10/500\n",
      "383/383 [==============================] - trainLoss: 49.1974  Val_loss: 22.2612 \n",
      "Epoch 11/500\n",
      "383/383 [==============================] - trainLoss: 48.5951  Val_loss: 22.0118 \n",
      "Epoch 12/500\n",
      "383/383 [==============================] - trainLoss: 46.9057  Val_loss: 21.7982 \n",
      "Epoch 13/500\n",
      "383/383 [==============================] - trainLoss: 45.9198  Val_loss: 21.5679 \n",
      "Epoch 14/500\n",
      "383/383 [==============================] - trainLoss: 46.3488  Val_loss: 21.4683 \n",
      "Epoch 15/500\n",
      "383/383 [==============================] - trainLoss: 44.1494  Val_loss: 21.3847 \n",
      "Epoch 16/500\n",
      "383/383 [==============================] - trainLoss: 43.6399  Val_loss: 21.1698 \n",
      "Epoch 17/500\n",
      "383/383 [==============================] - trainLoss: 43.6819  Val_loss: 21.0938 \n",
      "Epoch 18/500\n",
      "383/383 [==============================] - trainLoss: 43.0896  Val_loss: 20.8981 \n",
      "Epoch 19/500\n",
      "383/383 [==============================] - trainLoss: 42.0856  Val_loss: 20.8800 \n",
      "Epoch 20/500\n",
      "383/383 [==============================] - trainLoss: 42.2658  Val_loss: 20.7564 \n",
      "Epoch 21/500\n",
      "383/383 [==============================] - trainLoss: 40.8390  Val_loss: 20.5453 \n",
      "Epoch 22/500\n",
      "383/383 [==============================] - trainLoss: 42.0770  Val_loss: 20.4613 \n",
      "Epoch 23/500\n",
      "383/383 [==============================] - trainLoss: 40.5619  Val_loss: 20.2476 \n",
      "Epoch 24/500\n",
      "383/383 [==============================] - trainLoss: 40.7872  Val_loss: 20.1008 \n",
      "Epoch 25/500\n",
      "383/383 [==============================] - trainLoss: 40.5900  Val_loss: 20.0245 \n",
      "Epoch 26/500\n",
      "383/383 [==============================] - trainLoss: 40.2475  Val_loss: 19.8958 \n",
      "Epoch 27/500\n",
      "383/383 [==============================] - trainLoss: 40.2036  Val_loss: 19.7356 \n",
      "Epoch 28/500\n",
      "383/383 [==============================] - trainLoss: 39.4476  Val_loss: 19.6027 \n",
      "Epoch 29/500\n",
      "383/383 [==============================] - trainLoss: 39.0317  Val_loss: 19.4635 \n",
      "Epoch 30/500\n",
      "383/383 [==============================] - trainLoss: 38.8142  Val_loss: 19.2913 \n",
      "Epoch 31/500\n",
      "383/383 [==============================] - trainLoss: 39.0443  Val_loss: 19.1718 \n",
      "Epoch 32/500\n",
      "383/383 [==============================] - trainLoss: 38.3687  Val_loss: 19.0579 \n",
      "Epoch 33/500\n",
      "383/383 [==============================] - trainLoss: 38.2642  Val_loss: 18.9034 \n",
      "Epoch 34/500\n",
      "383/383 [==============================] - trainLoss: 37.4461  Val_loss: 18.7501 \n",
      "Epoch 35/500\n",
      "383/383 [==============================] - trainLoss: 37.7585  Val_loss: 18.6417 \n",
      "Epoch 36/500\n",
      "383/383 [==============================] - trainLoss: 37.5758  Val_loss: 18.4912 \n",
      "Epoch 37/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "383/383 [==============================] - trainLoss: 38.1818  Val_loss: 18.3754 \n",
      "Epoch 38/500\n",
      "383/383 [==============================] - trainLoss: 37.3304  Val_loss: 18.2191 \n",
      "Epoch 39/500\n",
      "383/383 [==============================] - trainLoss: 37.0249  Val_loss: 18.1205 \n",
      "Epoch 40/500\n",
      "383/383 [==============================] - trainLoss: 36.8070  Val_loss: 17.9998 \n",
      "Epoch 41/500\n",
      "383/383 [==============================] - trainLoss: 36.3873  Val_loss: 17.8877 \n",
      "Epoch 42/500\n",
      "383/383 [==============================] - trainLoss: 36.8013  Val_loss: 17.8326 \n",
      "Epoch 43/500\n",
      "383/383 [==============================] - trainLoss: 36.3533  Val_loss: 17.6781 \n",
      "Epoch 44/500\n",
      "383/383 [==============================] - trainLoss: 35.7903  Val_loss: 17.6263 \n",
      "Epoch 45/500\n",
      "383/383 [==============================] - trainLoss: 36.0802  Val_loss: 17.5682 \n",
      "Epoch 46/500\n",
      "383/383 [==============================] - trainLoss: 35.4781  Val_loss: 17.4417 \n",
      "Epoch 47/500\n",
      "383/383 [==============================] - trainLoss: 35.6080  Val_loss: 17.3363 \n",
      "Epoch 48/500\n",
      "383/383 [==============================] - trainLoss: 35.3845  Val_loss: 17.2730 \n",
      "Epoch 49/500\n",
      "383/383 [==============================] - trainLoss: 35.2762  Val_loss: 17.1570 \n",
      "Epoch 50/500\n",
      "383/383 [==============================] - trainLoss: 34.9735  Val_loss: 17.1309 \n",
      "Epoch 51/500\n",
      "383/383 [==============================] - trainLoss: 35.1718  Val_loss: 17.0590 \n",
      "Epoch 52/500\n",
      "383/383 [==============================] - trainLoss: 35.3574  Val_loss: 16.9788 \n",
      "Epoch 53/500\n",
      "383/383 [==============================] - trainLoss: 34.1250  Val_loss: 16.8882 \n",
      "Epoch 54/500\n",
      "383/383 [==============================] - trainLoss: 34.2184  Val_loss: 16.7574 \n",
      "Epoch 55/500\n",
      "383/383 [==============================] - trainLoss: 34.7467  Val_loss: 16.6612 \n",
      "Epoch 56/500\n",
      "383/383 [==============================] - trainLoss: 34.2362  Val_loss: 16.5547 \n",
      "Epoch 57/500\n",
      "383/383 [==============================] - trainLoss: 33.7809  Val_loss: 16.4759 \n",
      "Epoch 58/500\n",
      "383/383 [==============================] - trainLoss: 33.7556  Val_loss: 16.3768 \n",
      "Epoch 59/500\n",
      "383/383 [==============================] - trainLoss: 33.5384  Val_loss: 16.2334 \n",
      "Epoch 60/500\n",
      "383/383 [==============================] - trainLoss: 33.7581  Val_loss: 16.1728 \n",
      "Epoch 61/500\n",
      "383/383 [==============================] - trainLoss: 33.7025  Val_loss: 16.1490 \n",
      "Epoch 62/500\n",
      "383/383 [==============================] - trainLoss: 33.2418  Val_loss: 16.0685 \n",
      "Epoch 63/500\n",
      "383/383 [==============================] - trainLoss: 32.8900  Val_loss: 16.0048 \n",
      "Epoch 64/500\n",
      "383/383 [==============================] - trainLoss: 32.3481  Val_loss: 15.8969 \n",
      "Epoch 65/500\n",
      "383/383 [==============================] - trainLoss: 33.0667  Val_loss: 15.8372 \n",
      "Epoch 66/500\n",
      "383/383 [==============================] - trainLoss: 33.2238  Val_loss: 15.7562 \n",
      "Epoch 67/500\n",
      "383/383 [==============================] - trainLoss: 32.6414  Val_loss: 15.7272 \n",
      "Epoch 68/500\n",
      "383/383 [==============================] - trainLoss: 32.7137  Val_loss: 15.6764 \n",
      "Epoch 69/500\n",
      "383/383 [==============================] - trainLoss: 32.3914  Val_loss: 15.6842 \n",
      "Epoch 70/500\n",
      "383/383 [==============================] - trainLoss: 32.1493  Val_loss: 15.6381 \n",
      "Epoch 71/500\n",
      "383/383 [==============================] - trainLoss: 31.5835  Val_loss: 15.5472 \n",
      "Epoch 72/500\n",
      "383/383 [==============================] - trainLoss: 32.2305  Val_loss: 15.5227 \n",
      "Epoch 73/500\n",
      "383/383 [==============================] - trainLoss: 31.3632  Val_loss: 15.4975 \n",
      "Epoch 74/500\n",
      "383/383 [==============================] - trainLoss: 31.4602  Val_loss: 15.3995 \n",
      "Epoch 75/500\n",
      "383/383 [==============================] - trainLoss: 31.2467  Val_loss: 15.3875 \n",
      "Epoch 76/500\n",
      "383/383 [==============================] - trainLoss: 31.6756  Val_loss: 15.3269 \n",
      "Epoch 77/500\n",
      "383/383 [==============================] - trainLoss: 31.5308  Val_loss: 15.2735 \n",
      "Epoch 78/500\n",
      "383/383 [==============================] - trainLoss: 31.3463  Val_loss: 15.2578 \n",
      "Epoch 79/500\n",
      "383/383 [==============================] - trainLoss: 31.1786  Val_loss: 15.1975 \n",
      "Epoch 80/500\n",
      "383/383 [==============================] - trainLoss: 31.6574  Val_loss: 15.1427 \n",
      "Epoch 81/500\n",
      "383/383 [==============================] - trainLoss: 32.1105  Val_loss: 15.0756 \n",
      "Epoch 82/500\n",
      "383/383 [==============================] - trainLoss: 30.6002  Val_loss: 15.0592 \n",
      "Epoch 83/500\n",
      "383/383 [==============================] - trainLoss: 31.3914  Val_loss: 15.0842 \n",
      "Epoch 84/500\n",
      "383/383 [==============================] - trainLoss: 30.7507  Val_loss: 15.0823 \n",
      "Epoch 85/500\n",
      "383/383 [==============================] - trainLoss: 30.6042  Val_loss: 15.0247 \n",
      "Epoch 86/500\n",
      "383/383 [==============================] - trainLoss: 30.9770  Val_loss: 14.9474 \n",
      "Epoch 87/500\n",
      "383/383 [==============================] - trainLoss: 30.6878  Val_loss: 14.8834 \n",
      "Epoch 88/500\n",
      "383/383 [==============================] - trainLoss: 30.6459  Val_loss: 14.8385 \n",
      "Epoch 89/500\n",
      "383/383 [==============================] - trainLoss: 30.3716  Val_loss: 14.7955 \n",
      "Epoch 90/500\n",
      "383/383 [==============================] - trainLoss: 29.5234  Val_loss: 14.7707 \n",
      "Epoch 91/500\n",
      "383/383 [==============================] - trainLoss: 29.8044  Val_loss: 14.7501 \n",
      "Epoch 92/500\n",
      "383/383 [==============================] - trainLoss: 29.3373  Val_loss: 14.7101 \n",
      "Epoch 93/500\n",
      "383/383 [==============================] - trainLoss: 29.6974  Val_loss: 14.6659 \n",
      "Epoch 94/500\n",
      "383/383 [==============================] - trainLoss: 29.7847  Val_loss: 14.6309 \n",
      "Epoch 95/500\n",
      "383/383 [==============================] - trainLoss: 29.3512  Val_loss: 14.5647 \n",
      "Epoch 96/500\n",
      "383/383 [==============================] - trainLoss: 29.9047  Val_loss: 14.5656 \n",
      "Epoch 97/500\n",
      "383/383 [==============================] - trainLoss: 29.4853  Val_loss: 14.6271 \n",
      "Epoch 98/500\n",
      "383/383 [==============================] - trainLoss: 29.7935  Val_loss: 14.6639 \n",
      "Epoch 99/500\n",
      "383/383 [==============================] - trainLoss: 29.5352  Val_loss: 14.6456 \n",
      "Epoch 100/500\n",
      "383/383 [==============================] - trainLoss: 29.6229  Val_loss: 14.6133 \n",
      "Epoch 101/500\n",
      "383/383 [==============================] - trainLoss: 29.6752  Val_loss: 14.5164 \n",
      "Epoch 102/500\n",
      "383/383 [==============================] - trainLoss: 29.1840  Val_loss: 14.4659 \n",
      "Epoch 103/500\n",
      "383/383 [==============================] - trainLoss: 29.1279  Val_loss: 14.4383 \n",
      "Epoch 104/500\n",
      "383/383 [==============================] - trainLoss: 29.3063  Val_loss: 14.4159 \n",
      "Epoch 105/500\n",
      "383/383 [==============================] - trainLoss: 28.8127  Val_loss: 14.4098 \n",
      "Epoch 106/500\n",
      "383/383 [==============================] - trainLoss: 28.9047  Val_loss: 14.4372 \n",
      "Epoch 107/500\n",
      "383/383 [==============================] - trainLoss: 28.7787  Val_loss: 14.4758 \n",
      "Epoch 108/500\n",
      "383/383 [==============================] - trainLoss: 29.0681  Val_loss: 14.3943 \n",
      "Epoch 109/500\n",
      "383/383 [==============================] - trainLoss: 28.4765  Val_loss: 14.3427 \n",
      "Epoch 110/500\n",
      "383/383 [==============================] - trainLoss: 29.1072  Val_loss: 14.3065 \n",
      "Epoch 111/500\n",
      "383/383 [==============================] - trainLoss: 29.0960  Val_loss: 14.3118 \n",
      "Epoch 112/500\n",
      "383/383 [==============================] - trainLoss: 29.0901  Val_loss: 14.2848 \n",
      "Epoch 113/500\n",
      "383/383 [==============================] - trainLoss: 28.5284  Val_loss: 14.2604 \n",
      "Epoch 114/500\n",
      "383/383 [==============================] - trainLoss: 28.3812  Val_loss: 14.2812 \n",
      "Epoch 115/500\n",
      "383/383 [==============================] - trainLoss: 28.2188  Val_loss: 14.2754 \n",
      "Epoch 116/500\n",
      "383/383 [==============================] - trainLoss: 28.5057  Val_loss: 14.2885 \n",
      "Epoch 117/500\n",
      "383/383 [==============================] - trainLoss: 28.5976  Val_loss: 14.2803 \n",
      "Epoch 118/500\n",
      "383/383 [==============================] - trainLoss: 28.6012  Val_loss: 14.2920 \n",
      "Epoch 119/500\n",
      "383/383 [==============================] - trainLoss: 28.4051  Val_loss: 14.1839 \n",
      "Epoch 120/500\n",
      "383/383 [==============================] - trainLoss: 27.9064  Val_loss: 14.1237 \n",
      "Epoch 121/500\n",
      "383/383 [==============================] - trainLoss: 28.0197  Val_loss: 14.1554 \n",
      "Epoch 122/500\n",
      "383/383 [==============================] - trainLoss: 28.1237  Val_loss: 14.1125 \n",
      "Epoch 123/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "383/383 [==============================] - trainLoss: 28.0647  Val_loss: 14.1122 \n",
      "Epoch 124/500\n",
      "383/383 [==============================] - trainLoss: 28.0649  Val_loss: 14.1385 \n",
      "Epoch 125/500\n",
      "383/383 [==============================] - trainLoss: 27.9943  Val_loss: 14.1360 \n",
      "Epoch 126/500\n",
      "383/383 [==============================] - trainLoss: 28.4776  Val_loss: 14.0957 \n",
      "Epoch 127/500\n",
      "383/383 [==============================] - trainLoss: 27.7149  Val_loss: 14.0707 \n",
      "Epoch 128/500\n",
      "383/383 [==============================] - trainLoss: 27.4790  Val_loss: 14.0273 \n",
      "Epoch 129/500\n",
      "383/383 [==============================] - trainLoss: 27.4526  Val_loss: 14.0500 \n",
      "Epoch 130/500\n",
      "383/383 [==============================] - trainLoss: 27.5783  Val_loss: 14.0301 \n",
      "Epoch 131/500\n",
      "383/383 [==============================] - trainLoss: 27.9481  Val_loss: 14.0471 \n",
      "Epoch 132/500\n",
      "383/383 [==============================] - trainLoss: 27.2352  Val_loss: 14.0823 \n",
      "Epoch 133/500\n",
      "383/383 [==============================] - trainLoss: 27.2979  Val_loss: 14.0101 \n",
      "Epoch 134/500\n",
      "383/383 [==============================] - trainLoss: 27.5413  Val_loss: 13.9408 \n",
      "Epoch 135/500\n",
      "383/383 [==============================] - trainLoss: 28.3716  Val_loss: 13.8785 \n",
      "Epoch 136/500\n",
      "383/383 [==============================] - trainLoss: 26.9257  Val_loss: 13.7967 \n",
      "Epoch 137/500\n",
      "383/383 [==============================] - trainLoss: 27.2040  Val_loss: 13.8146 \n",
      "Epoch 138/500\n",
      "383/383 [==============================] - trainLoss: 27.1067  Val_loss: 13.8918 \n",
      "Epoch 139/500\n",
      "383/383 [==============================] - trainLoss: 27.3001  Val_loss: 13.9300 \n",
      "Epoch 140/500\n",
      "383/383 [==============================] - trainLoss: 26.9552  Val_loss: 13.8942 \n",
      "Epoch 141/500\n",
      "383/383 [==============================] - trainLoss: 26.6898  Val_loss: 13.8270 \n",
      "Epoch 142/500\n",
      "383/383 [==============================] - trainLoss: 27.3133  Val_loss: 13.8636 \n",
      "Epoch 143/500\n",
      "383/383 [==============================] - trainLoss: 26.6548  Val_loss: 13.8587 \n",
      "Epoch 144/500\n",
      "383/383 [==============================] - trainLoss: 26.2948  Val_loss: 13.8196 \n",
      "Epoch 145/500\n",
      "383/383 [==============================] - trainLoss: 27.2824  Val_loss: 13.7947 \n",
      "Epoch 146/500\n",
      "383/383 [==============================] - trainLoss: 26.4941  Val_loss: 13.7545 \n",
      "Epoch 147/500\n",
      "383/383 [==============================] - trainLoss: 27.0818  Val_loss: 13.7616 \n",
      "Epoch 148/500\n",
      "383/383 [==============================] - trainLoss: 26.9957  Val_loss: 13.7376 \n",
      "Epoch 149/500\n",
      "383/383 [==============================] - trainLoss: 26.8885  Val_loss: 13.7603 \n",
      "Epoch 150/500\n",
      "383/383 [==============================] - trainLoss: 26.6340  Val_loss: 13.7970 \n",
      "Epoch 151/500\n",
      "383/383 [==============================] - trainLoss: 26.2282  Val_loss: 13.7788 \n",
      "Epoch 152/500\n",
      "383/383 [==============================] - trainLoss: 25.6062  Val_loss: 13.8517 \n",
      "Epoch 153/500\n",
      "383/383 [==============================] - trainLoss: 26.2648  Val_loss: 13.8687 \n",
      "Epoch 154/500\n",
      "383/383 [==============================] - trainLoss: 25.7980  Val_loss: 13.8519 \n",
      "Epoch 155/500\n",
      "383/383 [==============================] - trainLoss: 26.5582  Val_loss: 13.8379 \n",
      "Epoch 156/500\n",
      "383/383 [==============================] - trainLoss: 26.3543  Val_loss: 13.7271 \n",
      "Epoch 157/500\n",
      "383/383 [==============================] - trainLoss: 26.9127  Val_loss: 13.6592 \n",
      "Epoch 158/500\n",
      "383/383 [==============================] - trainLoss: 25.9607  Val_loss: 13.6075 \n",
      "Epoch 159/500\n",
      "383/383 [==============================] - trainLoss: 26.3138  Val_loss: 13.5903 \n",
      "Epoch 160/500\n",
      "383/383 [==============================] - trainLoss: 25.8123  Val_loss: 13.6294 \n",
      "Epoch 161/500\n",
      "383/383 [==============================] - trainLoss: 25.5261  Val_loss: 13.7100 \n",
      "Epoch 162/500\n",
      "383/383 [==============================] - trainLoss: 25.6070  Val_loss: 13.6688 \n",
      "Epoch 163/500\n",
      "383/383 [==============================] - trainLoss: 25.5958  Val_loss: 13.6216 \n",
      "Epoch 164/500\n",
      "383/383 [==============================] - trainLoss: 26.6350  Val_loss: 13.6513 \n",
      "Epoch 165/500\n",
      "383/383 [==============================] - trainLoss: 26.4269  Val_loss: 13.6719 \n",
      "Epoch 166/500\n",
      "383/383 [==============================] - trainLoss: 25.7172  Val_loss: 13.6812 \n",
      "Epoch 167/500\n",
      "383/383 [==============================] - trainLoss: 26.3044  Val_loss: 13.6238 \n",
      "Epoch 168/500\n",
      "383/383 [==============================] - trainLoss: 25.6617  Val_loss: 13.5021 \n",
      "Epoch 169/500\n",
      "383/383 [==============================] - trainLoss: 25.7283  Val_loss: 13.5027 \n",
      "Epoch 170/500\n",
      "383/383 [==============================] - trainLoss: 25.3920  Val_loss: 13.5786 \n",
      "Epoch 171/500\n",
      "383/383 [==============================] - trainLoss: 25.8139  Val_loss: 13.6628 \n",
      "Epoch 172/500\n",
      "383/383 [==============================] - trainLoss: 25.6379  Val_loss: 13.6702 \n",
      "Epoch 173/500\n",
      "383/383 [==============================] - trainLoss: 25.3171  Val_loss: 13.6218 \n",
      "Epoch 174/500\n",
      "383/383 [==============================] - trainLoss: 25.5657  Val_loss: 13.6408 \n",
      "Epoch 175/500\n",
      "383/383 [==============================] - trainLoss: 25.7933  Val_loss: 13.6241 \n",
      "Epoch 176/500\n",
      "383/383 [==============================] - trainLoss: 25.8100  Val_loss: 13.6263 \n",
      "Epoch 177/500\n",
      "383/383 [==============================] - trainLoss: 25.6453  Val_loss: 13.5768 \n",
      "WARNING:tensorflow:No training configuration found in save file: the model was *not* compiled. Compile it manually.\n",
      "WARNING:tensorflow:No training configuration found in save file: the model was *not* compiled. Compile it manually.\n",
      "WARNING:tensorflow:No training configuration found in save file: the model was *not* compiled. Compile it manually.\n",
      "WARNING:tensorflow:No training configuration found in save file: the model was *not* compiled. Compile it manually.\n",
      "Elapsed/s:  119.10308742523193\n",
      "Final training loss:  tf.Tensor(25.64527, shape=(), dtype=float32)\n",
      "best val loss:  tf.Tensor(13.502145, shape=(), dtype=float32)\n",
      "{0: {'parameters': {'codings_size': 208, 'n_hidden_classifier': 1, 'N': 100, 'beta': 15, 'n_neurons': 516, 'n_neurons_classifier': 43, 'learning_rate': 0.001, 'n_hidden': 1}, 'val_loss': <tf.Tensor: shape=(), dtype=float32, numpy=12.73137>}, 1: {'parameters': {'codings_size': 207, 'n_hidden_classifier': 1, 'N': 50, 'beta': 1, 'n_neurons': 152, 'n_neurons_classifier': 40, 'learning_rate': 0.001, 'n_hidden': 1}, 'val_loss': <tf.Tensor: shape=(), dtype=float32, numpy=12.758459>}, 2: {'parameters': {'codings_size': 177, 'n_hidden_classifier': 2, 'N': 50, 'beta': 15, 'n_neurons': 398, 'n_neurons_classifier': 80, 'learning_rate': 0.0001, 'n_hidden': 1}, 'val_loss': <tf.Tensor: shape=(), dtype=float32, numpy=13.014389>}, 3: {'parameters': {'codings_size': 57, 'n_hidden_classifier': 2, 'N': 1, 'beta': 15, 'n_neurons': 516, 'n_neurons_classifier': 43, 'learning_rate': 0.001, 'n_hidden': 1}, 'val_loss': <tf.Tensor: shape=(), dtype=float32, numpy=12.97913>}, 4: {'parameters': {'codings_size': 126, 'n_hidden_classifier': 2, 'N': 100, 'beta': 1, 'n_neurons': 422, 'n_neurons_classifier': 52, 'learning_rate': 0.0001, 'n_hidden': 2}, 'val_loss': <tf.Tensor: shape=(), dtype=float32, numpy=13.502145>}}\n",
      "Epoch 0/500\n",
      "WARNING:tensorflow:Layer full_model_only_labelled is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because it's dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "320/383 [========================>.....] - Loss for batch: 133.0141WARNING:tensorflow:Layer concatenate_1 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because it's dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "383/383 [==============================] - trainLoss: 133.0141  Val_loss: 26.5285 \n",
      "Epoch 1/500\n",
      "383/383 [==============================] - trainLoss: 61.7813  Val_loss: 23.6714 \n",
      "Epoch 2/500\n",
      "383/383 [==============================] - trainLoss: 45.4841  Val_loss: 22.4144 \n",
      "Epoch 3/500\n",
      "383/383 [==============================] - trainLoss: 38.7664  Val_loss: 21.6201 \n",
      "Epoch 4/500\n",
      "383/383 [==============================] - trainLoss: 35.9970  Val_loss: 20.9365 \n",
      "Epoch 5/500\n",
      "383/383 [==============================] - trainLoss: 34.1588  Val_loss: 20.2119 \n",
      "Epoch 6/500\n",
      "383/383 [==============================] - trainLoss: 32.7169  Val_loss: 19.5774 \n",
      "Epoch 7/500\n",
      "383/383 [==============================] - trainLoss: 31.5434  Val_loss: 18.9580 \n",
      "Epoch 8/500\n",
      "383/383 [==============================] - trainLoss: 30.6486  Val_loss: 18.2471 \n",
      "Epoch 9/500\n",
      "383/383 [==============================] - trainLoss: 29.2695  Val_loss: 17.6874 \n",
      "Epoch 10/500\n",
      "383/383 [==============================] - trainLoss: 28.7588  Val_loss: 17.2906 \n",
      "Epoch 11/500\n",
      "383/383 [==============================] - trainLoss: 27.4573  Val_loss: 16.8926 \n",
      "Epoch 12/500\n",
      "383/383 [==============================] - trainLoss: 27.2661  Val_loss: 16.4255 \n",
      "Epoch 13/500\n",
      "383/383 [==============================] - trainLoss: 26.6219  Val_loss: 16.0886 \n",
      "Epoch 14/500\n",
      "383/383 [==============================] - trainLoss: 25.8092  Val_loss: 15.8888 \n",
      "Epoch 15/500\n",
      "383/383 [==============================] - trainLoss: 24.8271  Val_loss: 15.6823 \n",
      "Epoch 16/500\n",
      "383/383 [==============================] - trainLoss: 24.6572  Val_loss: 15.5086 \n",
      "Epoch 17/500\n",
      "383/383 [==============================] - trainLoss: 23.8685  Val_loss: 15.2206 \n",
      "Epoch 18/500\n",
      "383/383 [==============================] - trainLoss: 23.3124  Val_loss: 15.0493 \n",
      "Epoch 19/500\n",
      "383/383 [==============================] - trainLoss: 22.9194  Val_loss: 14.8984 \n",
      "Epoch 20/500\n",
      "383/383 [==============================] - trainLoss: 22.7301  Val_loss: 14.7153 \n",
      "Epoch 21/500\n",
      "383/383 [==============================] - trainLoss: 22.3009  Val_loss: 14.5206 \n",
      "Epoch 22/500\n",
      "383/383 [==============================] - trainLoss: 21.6289  Val_loss: 14.4816 \n",
      "Epoch 23/500\n",
      "383/383 [==============================] - trainLoss: 21.1735  Val_loss: 14.4092 \n",
      "Epoch 24/500\n",
      "383/383 [==============================] - trainLoss: 21.1948  Val_loss: 14.2779 \n",
      "Epoch 25/500\n",
      "383/383 [==============================] - trainLoss: 20.8927  Val_loss: 14.1974 \n",
      "Epoch 26/500\n",
      "383/383 [==============================] - trainLoss: 20.8634  Val_loss: 14.3029 \n",
      "Epoch 27/500\n",
      "383/383 [==============================] - trainLoss: 20.2656  Val_loss: 14.0001 \n",
      "Epoch 28/500\n",
      "383/383 [==============================] - trainLoss: 19.9964  Val_loss: 13.9287 \n",
      "Epoch 29/500\n",
      "383/383 [==============================] - trainLoss: 19.5685  Val_loss: 13.8394 \n",
      "Epoch 30/500\n",
      "383/383 [==============================] - trainLoss: 20.2224  Val_loss: 13.8818 \n",
      "Epoch 31/500\n",
      "383/383 [==============================] - trainLoss: 19.3207  Val_loss: 14.0263 \n",
      "Epoch 32/500\n",
      "383/383 [==============================] - trainLoss: 18.8015  Val_loss: 13.8006 \n",
      "Epoch 33/500\n",
      "383/383 [==============================] - trainLoss: 18.7788  Val_loss: 13.6723 \n",
      "Epoch 34/500\n",
      "383/383 [==============================] - trainLoss: 18.7281  Val_loss: 13.5631 \n",
      "Epoch 35/500\n",
      "383/383 [==============================] - trainLoss: 18.5218  Val_loss: 13.8261 \n",
      "Epoch 36/500\n",
      "383/383 [==============================] - trainLoss: 17.7538  Val_loss: 13.8303 \n",
      "Epoch 37/500\n",
      "383/383 [==============================] - trainLoss: 17.7845  Val_loss: 13.6711 \n",
      "Epoch 38/500\n",
      "383/383 [==============================] - trainLoss: 18.1098  Val_loss: 13.7962 \n",
      "Epoch 39/500\n",
      "383/383 [==============================] - trainLoss: 17.7843  Val_loss: 13.6302 \n",
      "Epoch 40/500\n",
      "383/383 [==============================] - trainLoss: 17.6850  Val_loss: 13.4883 \n",
      "Epoch 41/500\n",
      "383/383 [==============================] - trainLoss: 17.2442  Val_loss: 13.5924 \n",
      "Epoch 42/500\n",
      "383/383 [==============================] - trainLoss: 17.4756  Val_loss: 13.6001 \n",
      "Epoch 43/500\n",
      "383/383 [==============================] - trainLoss: 17.1850  Val_loss: 13.6266 \n",
      "Epoch 44/500\n",
      "383/383 [==============================] - trainLoss: 16.6908  Val_loss: 13.6394 \n",
      "Epoch 45/500\n",
      "383/383 [==============================] - trainLoss: 17.0992  Val_loss: 13.2968 \n",
      "Epoch 46/500\n",
      "383/383 [==============================] - trainLoss: 16.5451  Val_loss: 13.5521 \n",
      "Epoch 47/500\n",
      "383/383 [==============================] - trainLoss: 16.7049  Val_loss: 13.2937 \n",
      "Epoch 48/500\n",
      "383/383 [==============================] - trainLoss: 16.6437  Val_loss: 13.3957 \n",
      "Epoch 49/500\n",
      "383/383 [==============================] - trainLoss: 16.0791  Val_loss: 13.3532 \n",
      "Epoch 50/500\n",
      "383/383 [==============================] - trainLoss: 16.3142  Val_loss: 13.5146 \n",
      "Epoch 51/500\n",
      "383/383 [==============================] - trainLoss: 16.0923  Val_loss: 13.3367 \n",
      "Epoch 52/500\n",
      "383/383 [==============================] - trainLoss: 16.2982  Val_loss: 13.4998 \n",
      "Epoch 53/500\n",
      "383/383 [==============================] - trainLoss: 16.4978  Val_loss: 13.3829 \n",
      "Epoch 54/500\n",
      "383/383 [==============================] - trainLoss: 16.0956  Val_loss: 13.3610 \n",
      "Epoch 55/500\n",
      "383/383 [==============================] - trainLoss: 15.9718  Val_loss: 13.6493 \n",
      "Epoch 56/500\n",
      "383/383 [==============================] - trainLoss: 15.8612  Val_loss: 13.2400 \n",
      "Epoch 57/500\n",
      "383/383 [==============================] - trainLoss: 15.4706  Val_loss: 13.2837 \n",
      "Epoch 58/500\n",
      "383/383 [==============================] - trainLoss: 15.4592  Val_loss: 13.4392 \n",
      "Epoch 59/500\n",
      "383/383 [==============================] - trainLoss: 15.2305  Val_loss: 13.1693 \n",
      "Epoch 60/500\n",
      "383/383 [==============================] - trainLoss: 15.4717  Val_loss: 13.3267 \n",
      "Epoch 61/500\n",
      "383/383 [==============================] - trainLoss: 15.4330  Val_loss: 13.2431 \n",
      "Epoch 62/500\n",
      "383/383 [==============================] - trainLoss: 15.0355  Val_loss: 13.1976 \n",
      "Epoch 63/500\n",
      "383/383 [==============================] - trainLoss: 15.1939  Val_loss: 13.3379 \n",
      "Epoch 64/500\n",
      "383/383 [==============================] - trainLoss: 15.0153  Val_loss: 13.3564 \n",
      "Epoch 65/500\n",
      "383/383 [==============================] - trainLoss: 15.0084  Val_loss: 13.2082 \n",
      "Epoch 66/500\n",
      "383/383 [==============================] - trainLoss: 14.9506  Val_loss: 13.1463 \n",
      "Epoch 67/500\n",
      "383/383 [==============================] - trainLoss: 15.2807  Val_loss: 13.2518 \n",
      "Epoch 68/500\n",
      "383/383 [==============================] - trainLoss: 14.3190  Val_loss: 13.1095 \n",
      "Epoch 69/500\n",
      "383/383 [==============================] - trainLoss: 15.0440  Val_loss: 13.2082 \n",
      "Epoch 70/500\n",
      "383/383 [==============================] - trainLoss: 14.8395  Val_loss: 12.9091 \n",
      "Epoch 71/500\n",
      "383/383 [==============================] - trainLoss: 14.6291  Val_loss: 13.3025 \n",
      "Epoch 72/500\n",
      "383/383 [==============================] - trainLoss: 14.2324  Val_loss: 13.1360 \n",
      "Epoch 73/500\n",
      "383/383 [==============================] - trainLoss: 14.0658  Val_loss: 13.2089 \n",
      "Epoch 74/500\n",
      "383/383 [==============================] - trainLoss: 14.5860  Val_loss: 13.2099 \n",
      "Epoch 75/500\n",
      "383/383 [==============================] - trainLoss: 14.3082  Val_loss: 13.1640 \n",
      "Epoch 76/500\n",
      "383/383 [==============================] - trainLoss: 14.3471  Val_loss: 13.0792 \n",
      "Epoch 77/500\n",
      "383/383 [==============================] - trainLoss: 14.0788  Val_loss: 13.3320 \n",
      "Epoch 78/500\n",
      "383/383 [==============================] - trainLoss: 13.9082  Val_loss: 13.1398 \n",
      "Epoch 79/500\n",
      "383/383 [==============================] - trainLoss: 13.6572  Val_loss: 13.0498 \n",
      "WARNING:tensorflow:No training configuration found in save file: the model was *not* compiled. Compile it manually.\n",
      "WARNING:tensorflow:No training configuration found in save file: the model was *not* compiled. Compile it manually.\n",
      "WARNING:tensorflow:No training configuration found in save file: the model was *not* compiled. Compile it manually.\n",
      "WARNING:tensorflow:No training configuration found in save file: the model was *not* compiled. Compile it manually.\n",
      "Elapsed/s:  58.67855381965637\n",
      "Final training loss:  tf.Tensor(13.65719, shape=(), dtype=float32)\n",
      "best val loss:  tf.Tensor(12.909143, shape=(), dtype=float32)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{0: {'parameters': {'codings_size': 208, 'n_hidden_classifier': 1, 'N': 100, 'beta': 15, 'n_neurons': 516, 'n_neurons_classifier': 43, 'learning_rate': 0.001, 'n_hidden': 1}, 'val_loss': <tf.Tensor: shape=(), dtype=float32, numpy=12.73137>}, 1: {'parameters': {'codings_size': 207, 'n_hidden_classifier': 1, 'N': 50, 'beta': 1, 'n_neurons': 152, 'n_neurons_classifier': 40, 'learning_rate': 0.001, 'n_hidden': 1}, 'val_loss': <tf.Tensor: shape=(), dtype=float32, numpy=12.758459>}, 2: {'parameters': {'codings_size': 177, 'n_hidden_classifier': 2, 'N': 50, 'beta': 15, 'n_neurons': 398, 'n_neurons_classifier': 80, 'learning_rate': 0.0001, 'n_hidden': 1}, 'val_loss': <tf.Tensor: shape=(), dtype=float32, numpy=13.014389>}, 3: {'parameters': {'codings_size': 57, 'n_hidden_classifier': 2, 'N': 1, 'beta': 15, 'n_neurons': 516, 'n_neurons_classifier': 43, 'learning_rate': 0.001, 'n_hidden': 1}, 'val_loss': <tf.Tensor: shape=(), dtype=float32, numpy=12.97913>}, 4: {'parameters': {'codings_size': 126, 'n_hidden_classifier': 2, 'N': 100, 'beta': 1, 'n_neurons': 422, 'n_neurons_classifier': 52, 'learning_rate': 0.0001, 'n_hidden': 2}, 'val_loss': <tf.Tensor: shape=(), dtype=float32, numpy=13.502145>}, 5: {'parameters': {'codings_size': 230, 'n_hidden_classifier': 2, 'N': 50, 'beta': 1, 'n_neurons': 380, 'n_neurons_classifier': 57, 'learning_rate': 0.0005, 'n_hidden': 2}, 'val_loss': <tf.Tensor: shape=(), dtype=float32, numpy=12.909143>}}\n",
      "Epoch 0/500\n",
      "WARNING:tensorflow:Layer full_model_only_labelled is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because it's dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "320/383 [========================>.....] - Loss for batch: 141.4903WARNING:tensorflow:Layer concatenate_1 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because it's dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "383/383 [==============================] - trainLoss: 141.4903  Val_loss: 29.4439 \n",
      "Epoch 1/500\n",
      "383/383 [==============================] - trainLoss: 99.3109  Val_loss: 27.9751 \n",
      "Epoch 2/500\n",
      "383/383 [==============================] - trainLoss: 85.6702  Val_loss: 26.7000 \n",
      "Epoch 3/500\n",
      "383/383 [==============================] - trainLoss: 78.0343  Val_loss: 25.6278 \n",
      "Epoch 4/500\n",
      "383/383 [==============================] - trainLoss: 72.8117  Val_loss: 24.7209 \n",
      "Epoch 5/500\n",
      "383/383 [==============================] - trainLoss: 69.1102  Val_loss: 24.0190 \n",
      "Epoch 6/500\n",
      "383/383 [==============================] - trainLoss: 65.9476  Val_loss: 23.3066 \n",
      "Epoch 7/500\n",
      "383/383 [==============================] - trainLoss: 63.1677  Val_loss: 22.7724 \n",
      "Epoch 8/500\n",
      "383/383 [==============================] - trainLoss: 60.7299  Val_loss: 22.2546 \n",
      "Epoch 9/500\n",
      "383/383 [==============================] - trainLoss: 58.6021  Val_loss: 21.9378 \n",
      "Epoch 10/500\n",
      "383/383 [==============================] - trainLoss: 57.9964  Val_loss: 21.5837 \n",
      "Epoch 11/500\n",
      "383/383 [==============================] - trainLoss: 56.8116  Val_loss: 21.3223 \n",
      "Epoch 12/500\n",
      "383/383 [==============================] - trainLoss: 55.5271  Val_loss: 21.1023 \n",
      "Epoch 13/500\n",
      "383/383 [==============================] - trainLoss: 54.8379  Val_loss: 20.9085 \n",
      "Epoch 14/500\n",
      "383/383 [==============================] - trainLoss: 53.1567  Val_loss: 20.6853 \n",
      "Epoch 15/500\n",
      "383/383 [==============================] - trainLoss: 52.5777  Val_loss: 20.5187 \n",
      "Epoch 16/500\n",
      "383/383 [==============================] - trainLoss: 51.8749  Val_loss: 20.3745 \n",
      "Epoch 17/500\n",
      "383/383 [==============================] - trainLoss: 51.1037  Val_loss: 20.2366 \n",
      "Epoch 18/500\n",
      "383/383 [==============================] - trainLoss: 51.5296  Val_loss: 19.9806 \n",
      "Epoch 19/500\n",
      "383/383 [==============================] - trainLoss: 51.2572  Val_loss: 19.9019 \n",
      "Epoch 20/500\n",
      "383/383 [==============================] - trainLoss: 50.4244  Val_loss: 19.7410 \n",
      "Epoch 21/500\n",
      "383/383 [==============================] - trainLoss: 48.9286  Val_loss: 19.6433 \n",
      "Epoch 22/500\n",
      "383/383 [==============================] - trainLoss: 49.3254  Val_loss: 19.5138 \n",
      "Epoch 23/500\n",
      "383/383 [==============================] - trainLoss: 50.2019  Val_loss: 19.3764 \n",
      "Epoch 24/500\n",
      "383/383 [==============================] - trainLoss: 48.5915  Val_loss: 19.2467 \n",
      "Epoch 25/500\n",
      "383/383 [==============================] - trainLoss: 49.3564  Val_loss: 19.1041 \n",
      "Epoch 26/500\n",
      "383/383 [==============================] - trainLoss: 48.8966  Val_loss: 18.9731 \n",
      "Epoch 27/500\n",
      "383/383 [==============================] - trainLoss: 48.4769  Val_loss: 18.8598 \n",
      "Epoch 28/500\n",
      "383/383 [==============================] - trainLoss: 47.7666  Val_loss: 18.6939 \n",
      "Epoch 29/500\n",
      "383/383 [==============================] - trainLoss: 47.6876  Val_loss: 18.5645 \n",
      "Epoch 30/500\n",
      "383/383 [==============================] - trainLoss: 47.4257  Val_loss: 18.4251 \n",
      "Epoch 31/500\n",
      "383/383 [==============================] - trainLoss: 47.4401  Val_loss: 18.2754 \n",
      "Epoch 32/500\n",
      "383/383 [==============================] - trainLoss: 46.8964  Val_loss: 18.1745 \n",
      "Epoch 33/500\n",
      "383/383 [==============================] - trainLoss: 46.2972  Val_loss: 18.0615 \n",
      "Epoch 34/500\n",
      "383/383 [==============================] - trainLoss: 47.3822  Val_loss: 17.9287 \n",
      "Epoch 35/500\n",
      "383/383 [==============================] - trainLoss: 46.6142  Val_loss: 17.8075 \n",
      "Epoch 36/500\n",
      "383/383 [==============================] - trainLoss: 47.3062  Val_loss: 17.6823 \n",
      "Epoch 37/500\n",
      "383/383 [==============================] - trainLoss: 46.9469  Val_loss: 17.5161 \n",
      "Epoch 38/500\n",
      "383/383 [==============================] - trainLoss: 45.6579  Val_loss: 17.3990 \n",
      "Epoch 39/500\n",
      "383/383 [==============================] - trainLoss: 46.4467  Val_loss: 17.2800 \n",
      "Epoch 40/500\n",
      "383/383 [==============================] - trainLoss: 45.7634  Val_loss: 17.1703 \n",
      "Epoch 41/500\n",
      "383/383 [==============================] - trainLoss: 45.5519  Val_loss: 17.0490 \n",
      "Epoch 42/500\n",
      "383/383 [==============================] - trainLoss: 45.4670  Val_loss: 16.9299 \n",
      "Epoch 43/500\n",
      "383/383 [==============================] - trainLoss: 45.0187  Val_loss: 16.8424 \n",
      "Epoch 44/500\n",
      "383/383 [==============================] - trainLoss: 44.7302  Val_loss: 16.7873 \n",
      "Epoch 45/500\n",
      "383/383 [==============================] - trainLoss: 45.1942  Val_loss: 16.6491 \n",
      "Epoch 46/500\n",
      "383/383 [==============================] - trainLoss: 43.9078  Val_loss: 16.5505 \n",
      "Epoch 47/500\n",
      "383/383 [==============================] - trainLoss: 44.9553  Val_loss: 16.4670 \n",
      "Epoch 48/500\n",
      "383/383 [==============================] - trainLoss: 44.0309  Val_loss: 16.4203 \n",
      "Epoch 49/500\n",
      "383/383 [==============================] - trainLoss: 44.0984  Val_loss: 16.3664 \n",
      "Epoch 50/500\n",
      "383/383 [==============================] - trainLoss: 44.1632  Val_loss: 16.2643 \n",
      "Epoch 51/500\n",
      "383/383 [==============================] - trainLoss: 43.3900  Val_loss: 16.1798 \n",
      "Epoch 52/500\n",
      "383/383 [==============================] - trainLoss: 44.0485  Val_loss: 16.1222 \n",
      "Epoch 53/500\n",
      "383/383 [==============================] - trainLoss: 43.9098  Val_loss: 16.0680 \n",
      "Epoch 54/500\n",
      "383/383 [==============================] - trainLoss: 43.6116  Val_loss: 16.0110 \n",
      "Epoch 55/500\n",
      "383/383 [==============================] - trainLoss: 42.7189  Val_loss: 15.9102 \n",
      "Epoch 56/500\n",
      "383/383 [==============================] - trainLoss: 43.2502  Val_loss: 15.8204 \n",
      "Epoch 57/500\n",
      "383/383 [==============================] - trainLoss: 43.3509  Val_loss: 15.7689 \n",
      "Epoch 58/500\n",
      "383/383 [==============================] - trainLoss: 43.1530  Val_loss: 15.6369 \n",
      "Epoch 59/500\n",
      "383/383 [==============================] - trainLoss: 41.5422  Val_loss: 15.6021 \n",
      "Epoch 60/500\n",
      "383/383 [==============================] - trainLoss: 42.2294  Val_loss: 15.5078 \n",
      "Epoch 61/500\n",
      "383/383 [==============================] - trainLoss: 43.0394  Val_loss: 15.4761 \n",
      "Epoch 62/500\n",
      "383/383 [==============================] - trainLoss: 42.8645  Val_loss: 15.4418 \n",
      "Epoch 63/500\n",
      "383/383 [==============================] - trainLoss: 41.8713  Val_loss: 15.3776 \n",
      "Epoch 64/500\n",
      "383/383 [==============================] - trainLoss: 42.5845  Val_loss: 15.3266 \n",
      "Epoch 65/500\n",
      "383/383 [==============================] - trainLoss: 42.1780  Val_loss: 15.2876 \n",
      "Epoch 66/500\n",
      "383/383 [==============================] - trainLoss: 42.7070  Val_loss: 15.2425 \n",
      "Epoch 67/500\n",
      "383/383 [==============================] - trainLoss: 41.9305  Val_loss: 15.2220 \n",
      "Epoch 68/500\n",
      "383/383 [==============================] - trainLoss: 41.1110  Val_loss: 15.1762 \n",
      "Epoch 69/500\n",
      "383/383 [==============================] - trainLoss: 41.7156  Val_loss: 15.1040 \n",
      "Epoch 70/500\n",
      "383/383 [==============================] - trainLoss: 41.3824  Val_loss: 15.0591 \n",
      "Epoch 71/500\n",
      "383/383 [==============================] - trainLoss: 42.1860  Val_loss: 15.0458 \n",
      "Epoch 72/500\n",
      "383/383 [==============================] - trainLoss: 41.7266  Val_loss: 14.9844 \n",
      "Epoch 73/500\n",
      "383/383 [==============================] - trainLoss: 41.4624  Val_loss: 14.9484 \n",
      "Epoch 74/500\n",
      "383/383 [==============================] - trainLoss: 41.3011  Val_loss: 14.9100 \n",
      "Epoch 75/500\n",
      "383/383 [==============================] - trainLoss: 40.5554  Val_loss: 14.8521 \n",
      "Epoch 76/500\n",
      "383/383 [==============================] - trainLoss: 40.1820  Val_loss: 14.8235 \n",
      "Epoch 77/500\n",
      "383/383 [==============================] - trainLoss: 40.9685  Val_loss: 14.7716 \n",
      "Epoch 78/500\n",
      "383/383 [==============================] - trainLoss: 40.8294  Val_loss: 14.7146 \n",
      "Epoch 79/500\n",
      "383/383 [==============================] - trainLoss: 39.8687  Val_loss: 14.6649 \n",
      "Epoch 80/500\n",
      "383/383 [==============================] - trainLoss: 41.3426  Val_loss: 14.6036 \n",
      "Epoch 81/500\n",
      "383/383 [==============================] - trainLoss: 40.4574  Val_loss: 14.5137 \n",
      "Epoch 82/500\n",
      "383/383 [==============================] - trainLoss: 40.4264  Val_loss: 14.5069 \n",
      "Epoch 83/500\n",
      "383/383 [==============================] - trainLoss: 39.9136  Val_loss: 14.5021 \n",
      "Epoch 84/500\n",
      "383/383 [==============================] - trainLoss: 40.2347  Val_loss: 14.4805 \n",
      "Epoch 85/500\n",
      "383/383 [==============================] - trainLoss: 39.6075  Val_loss: 14.4575 \n",
      "Epoch 86/500\n",
      "383/383 [==============================] - trainLoss: 39.9501  Val_loss: 14.4130 \n",
      "Epoch 87/500\n",
      "383/383 [==============================] - trainLoss: 40.3719  Val_loss: 14.4014 \n",
      "Epoch 88/500\n",
      "383/383 [==============================] - trainLoss: 39.7693  Val_loss: 14.4034 \n",
      "Epoch 89/500\n",
      "383/383 [==============================] - trainLoss: 41.0849  Val_loss: 14.3962 \n",
      "Epoch 90/500\n",
      "383/383 [==============================] - trainLoss: 39.9702  Val_loss: 14.4189 \n",
      "Epoch 91/500\n",
      "383/383 [==============================] - trainLoss: 39.6588  Val_loss: 14.4038 \n",
      "Epoch 92/500\n",
      "383/383 [==============================] - trainLoss: 39.4700  Val_loss: 14.3236 \n",
      "Epoch 93/500\n",
      "383/383 [==============================] - trainLoss: 39.8083  Val_loss: 14.2638 \n",
      "Epoch 94/500\n",
      "383/383 [==============================] - trainLoss: 40.3595  Val_loss: 14.2511 \n",
      "Epoch 95/500\n",
      "383/383 [==============================] - trainLoss: 39.5886  Val_loss: 14.2590 \n",
      "Epoch 96/500\n",
      "383/383 [==============================] - trainLoss: 38.9755  Val_loss: 14.2627 \n",
      "Epoch 97/500\n",
      "383/383 [==============================] - trainLoss: 38.6829  Val_loss: 14.2370 \n",
      "Epoch 98/500\n",
      "383/383 [==============================] - trainLoss: 39.1451  Val_loss: 14.1836 \n",
      "Epoch 99/500\n",
      "383/383 [==============================] - trainLoss: 38.9251  Val_loss: 14.1707 \n",
      "Epoch 100/500\n",
      "383/383 [==============================] - trainLoss: 39.4208  Val_loss: 14.1624 \n",
      "Epoch 101/500\n",
      "383/383 [==============================] - trainLoss: 39.0377  Val_loss: 14.1406 \n",
      "Epoch 102/500\n",
      "383/383 [==============================] - trainLoss: 38.5374  Val_loss: 14.1230 \n",
      "Epoch 103/500\n",
      "383/383 [==============================] - trainLoss: 38.1764  Val_loss: 14.0877 \n",
      "Epoch 104/500\n",
      "383/383 [==============================] - trainLoss: 38.7763  Val_loss: 14.0189 \n",
      "Epoch 105/500\n",
      "383/383 [==============================] - trainLoss: 38.4258  Val_loss: 14.0146 \n",
      "Epoch 106/500\n",
      "383/383 [==============================] - trainLoss: 38.1329  Val_loss: 13.9740 \n",
      "Epoch 107/500\n",
      "383/383 [==============================] - trainLoss: 39.0025  Val_loss: 13.9280 \n",
      "Epoch 108/500\n",
      "383/383 [==============================] - trainLoss: 38.4206  Val_loss: 13.9129 \n",
      "Epoch 109/500\n",
      "383/383 [==============================] - trainLoss: 38.8384  Val_loss: 13.9158 \n",
      "Epoch 110/500\n",
      "383/383 [==============================] - trainLoss: 39.2639  Val_loss: 13.9360 \n",
      "Epoch 111/500\n",
      "383/383 [==============================] - trainLoss: 39.4655  Val_loss: 13.9126 \n",
      "Epoch 112/500\n",
      "383/383 [==============================] - trainLoss: 37.6822  Val_loss: 13.8944 \n",
      "Epoch 113/500\n",
      "383/383 [==============================] - trainLoss: 37.9029  Val_loss: 13.9329 \n",
      "Epoch 114/500\n",
      "383/383 [==============================] - trainLoss: 38.2236  Val_loss: 13.9237 \n",
      "Epoch 115/500\n",
      "383/383 [==============================] - trainLoss: 38.2565  Val_loss: 13.9544 \n",
      "Epoch 116/500\n",
      "383/383 [==============================] - trainLoss: 38.2942  Val_loss: 13.9346 \n",
      "Epoch 117/500\n",
      "383/383 [==============================] - trainLoss: 38.0306  Val_loss: 13.8846 \n",
      "Epoch 118/500\n",
      "383/383 [==============================] - trainLoss: 38.6197  Val_loss: 13.8283 \n",
      "Epoch 119/500\n",
      "383/383 [==============================] - trainLoss: 38.4467  Val_loss: 13.8502 \n",
      "Epoch 120/500\n",
      "383/383 [==============================] - trainLoss: 38.3470  Val_loss: 13.8339 \n",
      "Epoch 121/500\n",
      "383/383 [==============================] - trainLoss: 37.2184  Val_loss: 13.8879 \n",
      "Epoch 122/500\n",
      "383/383 [==============================] - trainLoss: 37.6039  Val_loss: 13.9089 \n",
      "Epoch 123/500\n",
      "383/383 [==============================] - trainLoss: 37.6240  Val_loss: 13.8977 \n",
      "Epoch 124/500\n",
      "383/383 [==============================] - trainLoss: 36.8860  Val_loss: 13.8532 \n",
      "Epoch 125/500\n",
      "383/383 [==============================] - trainLoss: 37.7467  Val_loss: 13.7699 \n",
      "Epoch 126/500\n",
      "383/383 [==============================] - trainLoss: 36.5297  Val_loss: 13.7023 \n",
      "Epoch 127/500\n",
      "383/383 [==============================] - trainLoss: 37.2184  Val_loss: 13.7226 \n",
      "Epoch 128/500\n",
      "383/383 [==============================] - trainLoss: 36.9525  Val_loss: 13.7759 \n",
      "Epoch 129/500\n",
      "383/383 [==============================] - trainLoss: 36.8690  Val_loss: 13.8157 \n",
      "Epoch 130/500\n",
      "383/383 [==============================] - trainLoss: 37.2545  Val_loss: 13.7864 \n",
      "Epoch 131/500\n",
      "383/383 [==============================] - trainLoss: 36.9615  Val_loss: 13.8145 \n",
      "Epoch 132/500\n",
      "383/383 [==============================] - trainLoss: 37.6538  Val_loss: 13.7864 \n",
      "Epoch 133/500\n",
      "383/383 [==============================] - trainLoss: 36.9174  Val_loss: 13.7417 \n",
      "Epoch 134/500\n",
      "383/383 [==============================] - trainLoss: 36.7639  Val_loss: 13.7611 \n",
      "Epoch 135/500\n",
      "383/383 [==============================] - trainLoss: 36.7002  Val_loss: 13.7345 \n",
      "WARNING:tensorflow:No training configuration found in save file: the model was *not* compiled. Compile it manually.\n",
      "WARNING:tensorflow:No training configuration found in save file: the model was *not* compiled. Compile it manually.\n",
      "WARNING:tensorflow:No training configuration found in save file: the model was *not* compiled. Compile it manually.\n",
      "WARNING:tensorflow:No training configuration found in save file: the model was *not* compiled. Compile it manually.\n",
      "Elapsed/s:  89.214115858078\n",
      "Final training loss:  tf.Tensor(36.700203, shape=(), dtype=float32)\n",
      "best val loss:  tf.Tensor(13.702262, shape=(), dtype=float32)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{0: {'parameters': {'codings_size': 208, 'n_hidden_classifier': 1, 'N': 100, 'beta': 15, 'n_neurons': 516, 'n_neurons_classifier': 43, 'learning_rate': 0.001, 'n_hidden': 1}, 'val_loss': <tf.Tensor: shape=(), dtype=float32, numpy=12.73137>}, 1: {'parameters': {'codings_size': 207, 'n_hidden_classifier': 1, 'N': 50, 'beta': 1, 'n_neurons': 152, 'n_neurons_classifier': 40, 'learning_rate': 0.001, 'n_hidden': 1}, 'val_loss': <tf.Tensor: shape=(), dtype=float32, numpy=12.758459>}, 2: {'parameters': {'codings_size': 177, 'n_hidden_classifier': 2, 'N': 50, 'beta': 15, 'n_neurons': 398, 'n_neurons_classifier': 80, 'learning_rate': 0.0001, 'n_hidden': 1}, 'val_loss': <tf.Tensor: shape=(), dtype=float32, numpy=13.014389>}, 3: {'parameters': {'codings_size': 57, 'n_hidden_classifier': 2, 'N': 1, 'beta': 15, 'n_neurons': 516, 'n_neurons_classifier': 43, 'learning_rate': 0.001, 'n_hidden': 1}, 'val_loss': <tf.Tensor: shape=(), dtype=float32, numpy=12.97913>}, 4: {'parameters': {'codings_size': 126, 'n_hidden_classifier': 2, 'N': 100, 'beta': 1, 'n_neurons': 422, 'n_neurons_classifier': 52, 'learning_rate': 0.0001, 'n_hidden': 2}, 'val_loss': <tf.Tensor: shape=(), dtype=float32, numpy=13.502145>}, 5: {'parameters': {'codings_size': 230, 'n_hidden_classifier': 2, 'N': 50, 'beta': 1, 'n_neurons': 380, 'n_neurons_classifier': 57, 'learning_rate': 0.0005, 'n_hidden': 2}, 'val_loss': <tf.Tensor: shape=(), dtype=float32, numpy=12.909143>}, 6: {'parameters': {'codings_size': 94, 'n_hidden_classifier': 1, 'N': 150, 'beta': 1, 'n_neurons': 422, 'n_neurons_classifier': 41, 'learning_rate': 0.0001, 'n_hidden': 2}, 'val_loss': <tf.Tensor: shape=(), dtype=float32, numpy=13.702262>}}\n",
      "Epoch 0/500\n",
      "WARNING:tensorflow:Layer full_model_only_labelled is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because it's dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "320/383 [========================>.....] - Loss for batch: 637.9167WARNING:tensorflow:Layer concatenate_1 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because it's dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "383/383 [==============================] - trainLoss: 637.9167  Val_loss: 26.8794 \n",
      "Epoch 1/500\n",
      "383/383 [==============================] - trainLoss: 201.5033  Val_loss: 24.6115 \n",
      "Epoch 2/500\n",
      "383/383 [==============================] - trainLoss: 136.7622  Val_loss: 22.7455 \n",
      "Epoch 3/500\n",
      "383/383 [==============================] - trainLoss: 113.0720  Val_loss: 21.4121 \n",
      "Epoch 4/500\n",
      "383/383 [==============================] - trainLoss: 106.7797  Val_loss: 20.5351 \n",
      "Epoch 5/500\n",
      "383/383 [==============================] - trainLoss: 100.2165  Val_loss: 19.8860 \n",
      "Epoch 6/500\n",
      "383/383 [==============================] - trainLoss: 93.7456  Val_loss: 19.3358 \n",
      "Epoch 7/500\n",
      "383/383 [==============================] - trainLoss: 90.4024  Val_loss: 18.9611 \n",
      "Epoch 8/500\n",
      "383/383 [==============================] - trainLoss: 86.5776  Val_loss: 18.5833 \n",
      "Epoch 9/500\n",
      "383/383 [==============================] - trainLoss: 86.2656  Val_loss: 18.2634 \n",
      "Epoch 10/500\n",
      "383/383 [==============================] - trainLoss: 79.9218  Val_loss: 17.9574 \n",
      "Epoch 11/500\n",
      "383/383 [==============================] - trainLoss: 76.6259  Val_loss: 17.7067 \n",
      "Epoch 12/500\n",
      "383/383 [==============================] - trainLoss: 76.5126  Val_loss: 17.4492 \n",
      "Epoch 13/500\n",
      "383/383 [==============================] - trainLoss: 74.3698  Val_loss: 17.1825 \n",
      "Epoch 14/500\n",
      "383/383 [==============================] - trainLoss: 70.7687  Val_loss: 16.9470 \n",
      "Epoch 15/500\n",
      "383/383 [==============================] - trainLoss: 69.2874  Val_loss: 16.6504 \n",
      "Epoch 16/500\n",
      "383/383 [==============================] - trainLoss: 66.7926  Val_loss: 16.4611 \n",
      "Epoch 17/500\n",
      "383/383 [==============================] - trainLoss: 65.2917  Val_loss: 16.3199 \n",
      "Epoch 18/500\n",
      "383/383 [==============================] - trainLoss: 65.5616  Val_loss: 16.1352 \n",
      "Epoch 19/500\n",
      "383/383 [==============================] - trainLoss: 63.9614  Val_loss: 15.9344 \n",
      "Epoch 20/500\n",
      "383/383 [==============================] - trainLoss: 63.5535  Val_loss: 15.7513 \n",
      "Epoch 21/500\n",
      "383/383 [==============================] - trainLoss: 60.2562  Val_loss: 15.6152 \n",
      "Epoch 22/500\n",
      "383/383 [==============================] - trainLoss: 59.1968  Val_loss: 15.4509 \n",
      "Epoch 23/500\n",
      "383/383 [==============================] - trainLoss: 57.5925  Val_loss: 15.2451 \n",
      "Epoch 24/500\n",
      "383/383 [==============================] - trainLoss: 57.6722  Val_loss: 15.1002 \n",
      "Epoch 25/500\n",
      "383/383 [==============================] - trainLoss: 57.3666  Val_loss: 14.9641 \n",
      "Epoch 26/500\n",
      "383/383 [==============================] - trainLoss: 55.1252  Val_loss: 14.8672 \n",
      "Epoch 27/500\n",
      "383/383 [==============================] - trainLoss: 53.3826  Val_loss: 14.7766 \n",
      "Epoch 28/500\n",
      "383/383 [==============================] - trainLoss: 53.2507  Val_loss: 14.6651 \n",
      "Epoch 29/500\n",
      "383/383 [==============================] - trainLoss: 52.0885  Val_loss: 14.5692 \n",
      "Epoch 30/500\n",
      "383/383 [==============================] - trainLoss: 50.6667  Val_loss: 14.4575 \n",
      "Epoch 31/500\n",
      "383/383 [==============================] - trainLoss: 50.6756  Val_loss: 14.3714 \n",
      "Epoch 32/500\n",
      "383/383 [==============================] - trainLoss: 51.7754  Val_loss: 14.2871 \n",
      "Epoch 33/500\n",
      "383/383 [==============================] - trainLoss: 49.0671  Val_loss: 14.1997 \n",
      "Epoch 34/500\n",
      "383/383 [==============================] - trainLoss: 48.4006  Val_loss: 14.1358 \n",
      "Epoch 35/500\n",
      "383/383 [==============================] - trainLoss: 48.1976  Val_loss: 14.0336 \n",
      "Epoch 36/500\n",
      "383/383 [==============================] - trainLoss: 47.4115  Val_loss: 13.9634 \n",
      "Epoch 37/500\n",
      "383/383 [==============================] - trainLoss: 45.8314  Val_loss: 13.9004 \n",
      "Epoch 38/500\n",
      "383/383 [==============================] - trainLoss: 44.5831  Val_loss: 13.9178 \n",
      "Epoch 39/500\n",
      "383/383 [==============================] - trainLoss: 44.7811  Val_loss: 13.8847 \n",
      "Epoch 40/500\n",
      "383/383 [==============================] - trainLoss: 44.7410  Val_loss: 13.8204 \n",
      "Epoch 41/500\n",
      "383/383 [==============================] - trainLoss: 43.9287  Val_loss: 13.7587 \n",
      "Epoch 42/500\n",
      "383/383 [==============================] - trainLoss: 43.5246  Val_loss: 13.7107 \n",
      "Epoch 43/500\n",
      "383/383 [==============================] - trainLoss: 41.8543  Val_loss: 13.6998 \n",
      "Epoch 44/500\n",
      "383/383 [==============================] - trainLoss: 43.2341  Val_loss: 13.6917 \n",
      "Epoch 45/500\n",
      "383/383 [==============================] - trainLoss: 42.5172  Val_loss: 13.6723 \n",
      "Epoch 46/500\n",
      "383/383 [==============================] - trainLoss: 41.2515  Val_loss: 13.6545 \n",
      "Epoch 47/500\n",
      "383/383 [==============================] - trainLoss: 41.6341  Val_loss: 13.5725 \n",
      "Epoch 48/500\n",
      "383/383 [==============================] - trainLoss: 40.2755  Val_loss: 13.5026 \n",
      "Epoch 49/500\n",
      "383/383 [==============================] - trainLoss: 39.5559  Val_loss: 13.4597 \n",
      "Epoch 50/500\n",
      "383/383 [==============================] - trainLoss: 38.3815  Val_loss: 13.4430 \n",
      "Epoch 51/500\n",
      "383/383 [==============================] - trainLoss: 38.6752  Val_loss: 13.4185 \n",
      "Epoch 52/500\n",
      "383/383 [==============================] - trainLoss: 38.3291  Val_loss: 13.4066 \n",
      "Epoch 53/500\n",
      "383/383 [==============================] - trainLoss: 38.7293  Val_loss: 13.3966 \n",
      "Epoch 54/500\n",
      "383/383 [==============================] - trainLoss: 37.1132  Val_loss: 13.3892 \n",
      "Epoch 55/500\n",
      "383/383 [==============================] - trainLoss: 37.4984  Val_loss: 13.3753 \n",
      "Epoch 56/500\n",
      "383/383 [==============================] - trainLoss: 36.1787  Val_loss: 13.3272 \n",
      "Epoch 57/500\n",
      "383/383 [==============================] - trainLoss: 35.8402  Val_loss: 13.2926 \n",
      "Epoch 58/500\n",
      "383/383 [==============================] - trainLoss: 35.9014  Val_loss: 13.2957 \n",
      "Epoch 59/500\n",
      "383/383 [==============================] - trainLoss: 36.3532  Val_loss: 13.2369 \n",
      "Epoch 60/500\n",
      "383/383 [==============================] - trainLoss: 35.3267  Val_loss: 13.2251 \n",
      "Epoch 61/500\n",
      "383/383 [==============================] - trainLoss: 35.1684  Val_loss: 13.1772 \n",
      "Epoch 62/500\n",
      "383/383 [==============================] - trainLoss: 34.7607  Val_loss: 13.1795 \n",
      "Epoch 63/500\n",
      "383/383 [==============================] - trainLoss: 34.4191  Val_loss: 13.1839 \n",
      "Epoch 64/500\n",
      "383/383 [==============================] - trainLoss: 34.7908  Val_loss: 13.2027 \n",
      "Epoch 65/500\n",
      "383/383 [==============================] - trainLoss: 33.9726  Val_loss: 13.1819 \n",
      "Epoch 66/500\n",
      "383/383 [==============================] - trainLoss: 33.1635  Val_loss: 13.1640 \n",
      "Epoch 67/500\n",
      "383/383 [==============================] - trainLoss: 32.1350  Val_loss: 13.1427 \n",
      "Epoch 68/500\n",
      "383/383 [==============================] - trainLoss: 33.0792  Val_loss: 13.1422 \n",
      "Epoch 69/500\n",
      "383/383 [==============================] - trainLoss: 32.5703  Val_loss: 13.0811 \n",
      "Epoch 70/500\n",
      "383/383 [==============================] - trainLoss: 33.0054  Val_loss: 13.0360 \n",
      "Epoch 71/500\n",
      "383/383 [==============================] - trainLoss: 31.0825  Val_loss: 13.0199 \n",
      "Epoch 72/500\n",
      "383/383 [==============================] - trainLoss: 31.3528  Val_loss: 13.0437 \n",
      "Epoch 73/500\n",
      "383/383 [==============================] - trainLoss: 31.4653  Val_loss: 13.0216 \n",
      "Epoch 74/500\n",
      "383/383 [==============================] - trainLoss: 29.9211  Val_loss: 13.0421 \n",
      "Epoch 75/500\n",
      "383/383 [==============================] - trainLoss: 30.1306  Val_loss: 13.0614 \n",
      "Epoch 76/500\n",
      "383/383 [==============================] - trainLoss: 31.6472  Val_loss: 13.0546 \n",
      "Epoch 77/500\n",
      "383/383 [==============================] - trainLoss: 30.5896  Val_loss: 13.0473 \n",
      "Epoch 78/500\n",
      "383/383 [==============================] - trainLoss: 29.6933  Val_loss: 13.0231 \n",
      "Epoch 79/500\n",
      "383/383 [==============================] - trainLoss: 30.6924  Val_loss: 13.0308 \n",
      "Epoch 80/500\n",
      "383/383 [==============================] - trainLoss: 28.9751  Val_loss: 13.0590 \n",
      "WARNING:tensorflow:No training configuration found in save file: the model was *not* compiled. Compile it manually.\n",
      "WARNING:tensorflow:No training configuration found in save file: the model was *not* compiled. Compile it manually.\n",
      "WARNING:tensorflow:No training configuration found in save file: the model was *not* compiled. Compile it manually.\n",
      "WARNING:tensorflow:No training configuration found in save file: the model was *not* compiled. Compile it manually.\n",
      "Elapsed/s:  49.061809062957764\n",
      "Final training loss:  tf.Tensor(28.975134, shape=(), dtype=float32)\n",
      "best val loss:  tf.Tensor(13.019903, shape=(), dtype=float32)\n",
      "{0: {'parameters': {'codings_size': 208, 'n_hidden_classifier': 1, 'N': 100, 'beta': 15, 'n_neurons': 516, 'n_neurons_classifier': 43, 'learning_rate': 0.001, 'n_hidden': 1}, 'val_loss': <tf.Tensor: shape=(), dtype=float32, numpy=12.73137>}, 1: {'parameters': {'codings_size': 207, 'n_hidden_classifier': 1, 'N': 50, 'beta': 1, 'n_neurons': 152, 'n_neurons_classifier': 40, 'learning_rate': 0.001, 'n_hidden': 1}, 'val_loss': <tf.Tensor: shape=(), dtype=float32, numpy=12.758459>}, 2: {'parameters': {'codings_size': 177, 'n_hidden_classifier': 2, 'N': 50, 'beta': 15, 'n_neurons': 398, 'n_neurons_classifier': 80, 'learning_rate': 0.0001, 'n_hidden': 1}, 'val_loss': <tf.Tensor: shape=(), dtype=float32, numpy=13.014389>}, 3: {'parameters': {'codings_size': 57, 'n_hidden_classifier': 2, 'N': 1, 'beta': 15, 'n_neurons': 516, 'n_neurons_classifier': 43, 'learning_rate': 0.001, 'n_hidden': 1}, 'val_loss': <tf.Tensor: shape=(), dtype=float32, numpy=12.97913>}, 4: {'parameters': {'codings_size': 126, 'n_hidden_classifier': 2, 'N': 100, 'beta': 1, 'n_neurons': 422, 'n_neurons_classifier': 52, 'learning_rate': 0.0001, 'n_hidden': 2}, 'val_loss': <tf.Tensor: shape=(), dtype=float32, numpy=13.502145>}, 5: {'parameters': {'codings_size': 230, 'n_hidden_classifier': 2, 'N': 50, 'beta': 1, 'n_neurons': 380, 'n_neurons_classifier': 57, 'learning_rate': 0.0005, 'n_hidden': 2}, 'val_loss': <tf.Tensor: shape=(), dtype=float32, numpy=12.909143>}, 6: {'parameters': {'codings_size': 94, 'n_hidden_classifier': 1, 'N': 150, 'beta': 1, 'n_neurons': 422, 'n_neurons_classifier': 41, 'learning_rate': 0.0001, 'n_hidden': 2}, 'val_loss': <tf.Tensor: shape=(), dtype=float32, numpy=13.702262>}, 7: {'parameters': {'codings_size': 207, 'n_hidden_classifier': 2, 'N': 100, 'beta': 15, 'n_neurons': 264, 'n_neurons_classifier': 80, 'learning_rate': 0.0005, 'n_hidden': 1}, 'val_loss': <tf.Tensor: shape=(), dtype=float32, numpy=13.019903>}}\n",
      "Epoch 0/500\n",
      "WARNING:tensorflow:Layer full_model_only_labelled is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because it's dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "320/383 [========================>.....] - Loss for batch: 1128.2603WARNING:tensorflow:Layer concatenate_1 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because it's dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "383/383 [==============================] - trainLoss: 1128.2603  Val_loss: 26.2554 \n",
      "Epoch 1/500\n",
      "383/383 [==============================] - trainLoss: 689.1127  Val_loss: 25.6520 \n",
      "Epoch 2/500\n",
      "383/383 [==============================] - trainLoss: 501.2369  Val_loss: 25.1554 \n",
      "Epoch 3/500\n",
      "383/383 [==============================] - trainLoss: 388.4264  Val_loss: 24.6379 \n",
      "Epoch 4/500\n",
      "383/383 [==============================] - trainLoss: 313.2223  Val_loss: 24.1536 \n",
      "Epoch 5/500\n",
      "383/383 [==============================] - trainLoss: 271.7069  Val_loss: 23.7123 \n",
      "Epoch 6/500\n",
      "383/383 [==============================] - trainLoss: 240.1284  Val_loss: 23.2695 \n",
      "Epoch 7/500\n",
      "383/383 [==============================] - trainLoss: 212.4087  Val_loss: 22.8249 \n",
      "Epoch 8/500\n",
      "383/383 [==============================] - trainLoss: 190.5037  Val_loss: 22.4692 \n",
      "Epoch 9/500\n",
      "383/383 [==============================] - trainLoss: 174.1859  Val_loss: 22.1028 \n",
      "Epoch 10/500\n",
      "383/383 [==============================] - trainLoss: 155.9383  Val_loss: 21.7310 \n",
      "Epoch 11/500\n",
      "383/383 [==============================] - trainLoss: 141.6425  Val_loss: 21.3720 \n",
      "Epoch 12/500\n",
      "383/383 [==============================] - trainLoss: 134.1671  Val_loss: 21.1041 \n",
      "Epoch 13/500\n",
      "383/383 [==============================] - trainLoss: 119.9859  Val_loss: 20.7601 \n",
      "Epoch 14/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "383/383 [==============================] - trainLoss: 114.0568  Val_loss: 20.4388 \n",
      "Epoch 15/500\n",
      "383/383 [==============================] - trainLoss: 112.3851  Val_loss: 20.1855 \n",
      "Epoch 16/500\n",
      "383/383 [==============================] - trainLoss: 107.9404  Val_loss: 19.9331 \n",
      "Epoch 17/500\n",
      "383/383 [==============================] - trainLoss: 104.0274  Val_loss: 19.7083 \n",
      "Epoch 18/500\n",
      "383/383 [==============================] - trainLoss: 100.2401  Val_loss: 19.4536 \n",
      "Epoch 19/500\n",
      "383/383 [==============================] - trainLoss: 95.0149  Val_loss: 19.1851 \n",
      "Epoch 20/500\n",
      "383/383 [==============================] - trainLoss: 96.4373  Val_loss: 19.0020 \n",
      "Epoch 21/500\n",
      "383/383 [==============================] - trainLoss: 93.1035  Val_loss: 18.8187 \n",
      "Epoch 22/500\n",
      "383/383 [==============================] - trainLoss: 93.4867  Val_loss: 18.6814 \n",
      "Epoch 23/500\n",
      "383/383 [==============================] - trainLoss: 86.4179  Val_loss: 18.5058 \n",
      "Epoch 24/500\n",
      "383/383 [==============================] - trainLoss: 87.1142  Val_loss: 18.3381 \n",
      "Epoch 25/500\n",
      "383/383 [==============================] - trainLoss: 84.6986  Val_loss: 18.1855 \n",
      "Epoch 26/500\n",
      "383/383 [==============================] - trainLoss: 84.7687  Val_loss: 18.0497 \n",
      "Epoch 27/500\n",
      "383/383 [==============================] - trainLoss: 82.5386  Val_loss: 17.9096 \n",
      "Epoch 28/500\n",
      "383/383 [==============================] - trainLoss: 83.1954  Val_loss: 17.8288 \n",
      "Epoch 29/500\n",
      "383/383 [==============================] - trainLoss: 78.3250  Val_loss: 17.7189 \n",
      "Epoch 30/500\n",
      "383/383 [==============================] - trainLoss: 75.7755  Val_loss: 17.6440 \n",
      "Epoch 31/500\n",
      "383/383 [==============================] - trainLoss: 76.4197  Val_loss: 17.5472 \n",
      "Epoch 32/500\n",
      "383/383 [==============================] - trainLoss: 76.0770  Val_loss: 17.4775 \n",
      "Epoch 33/500\n",
      "383/383 [==============================] - trainLoss: 75.6112  Val_loss: 17.4036 \n",
      "Epoch 34/500\n",
      "383/383 [==============================] - trainLoss: 78.2744  Val_loss: 17.3220 \n",
      "Epoch 35/500\n",
      "383/383 [==============================] - trainLoss: 72.5977  Val_loss: 17.2295 \n",
      "Epoch 36/500\n",
      "383/383 [==============================] - trainLoss: 71.9002  Val_loss: 17.1162 \n",
      "Epoch 37/500\n",
      "383/383 [==============================] - trainLoss: 71.6276  Val_loss: 17.0486 \n",
      "Epoch 38/500\n",
      "383/383 [==============================] - trainLoss: 70.6160  Val_loss: 17.0150 \n",
      "Epoch 39/500\n",
      "383/383 [==============================] - trainLoss: 69.8133  Val_loss: 16.9173 \n",
      "Epoch 40/500\n",
      "383/383 [==============================] - trainLoss: 70.1871  Val_loss: 16.8539 \n",
      "Epoch 41/500\n",
      "383/383 [==============================] - trainLoss: 69.1101  Val_loss: 16.7461 \n",
      "Epoch 42/500\n",
      "383/383 [==============================] - trainLoss: 68.6828  Val_loss: 16.7094 \n",
      "Epoch 43/500\n",
      "383/383 [==============================] - trainLoss: 67.7691  Val_loss: 16.6268 \n",
      "Epoch 44/500\n",
      "383/383 [==============================] - trainLoss: 67.7937  Val_loss: 16.5795 \n",
      "Epoch 45/500\n",
      "383/383 [==============================] - trainLoss: 64.9447  Val_loss: 16.5247 \n",
      "Epoch 46/500\n",
      "383/383 [==============================] - trainLoss: 64.5451  Val_loss: 16.4413 \n",
      "Epoch 47/500\n",
      "383/383 [==============================] - trainLoss: 64.4458  Val_loss: 16.3922 \n",
      "Epoch 48/500\n",
      "383/383 [==============================] - trainLoss: 63.8938  Val_loss: 16.3142 \n",
      "Epoch 49/500\n",
      "383/383 [==============================] - trainLoss: 59.1623  Val_loss: 16.2729 \n",
      "Epoch 50/500\n",
      "383/383 [==============================] - trainLoss: 59.9455  Val_loss: 16.2126 \n",
      "Epoch 51/500\n",
      "383/383 [==============================] - trainLoss: 61.3263  Val_loss: 16.1501 \n",
      "Epoch 52/500\n",
      "383/383 [==============================] - trainLoss: 59.5479  Val_loss: 16.1168 \n",
      "Epoch 53/500\n",
      "383/383 [==============================] - trainLoss: 58.5385  Val_loss: 16.0673 \n",
      "Epoch 54/500\n",
      "383/383 [==============================] - trainLoss: 59.0904  Val_loss: 16.0317 \n",
      "Epoch 55/500\n",
      "383/383 [==============================] - trainLoss: 56.7087  Val_loss: 15.9974 \n",
      "Epoch 56/500\n",
      "383/383 [==============================] - trainLoss: 59.0467  Val_loss: 15.9204 \n",
      "Epoch 57/500\n",
      "383/383 [==============================] - trainLoss: 57.2735  Val_loss: 15.8484 \n",
      "Epoch 58/500\n",
      "383/383 [==============================] - trainLoss: 57.2557  Val_loss: 15.8102 \n",
      "Epoch 59/500\n",
      "383/383 [==============================] - trainLoss: 54.2934  Val_loss: 15.7758 \n",
      "Epoch 60/500\n",
      "383/383 [==============================] - trainLoss: 53.7056  Val_loss: 15.7042 \n",
      "Epoch 61/500\n",
      "383/383 [==============================] - trainLoss: 53.0982  Val_loss: 15.6965 \n",
      "Epoch 62/500\n",
      "383/383 [==============================] - trainLoss: 53.2787  Val_loss: 15.6396 \n",
      "Epoch 63/500\n",
      "383/383 [==============================] - trainLoss: 53.7741  Val_loss: 15.6221 \n",
      "Epoch 64/500\n",
      "383/383 [==============================] - trainLoss: 51.1952  Val_loss: 15.5594 \n",
      "Epoch 65/500\n",
      "383/383 [==============================] - trainLoss: 52.2554  Val_loss: 15.5469 \n",
      "Epoch 66/500\n",
      "383/383 [==============================] - trainLoss: 52.3752  Val_loss: 15.4763 \n",
      "Epoch 67/500\n",
      "383/383 [==============================] - trainLoss: 51.2321  Val_loss: 15.4372 \n",
      "Epoch 68/500\n",
      "383/383 [==============================] - trainLoss: 50.4876  Val_loss: 15.4001 \n",
      "Epoch 69/500\n",
      "383/383 [==============================] - trainLoss: 50.8267  Val_loss: 15.3476 \n",
      "Epoch 70/500\n",
      "383/383 [==============================] - trainLoss: 49.5073  Val_loss: 15.3011 \n",
      "Epoch 71/500\n",
      "383/383 [==============================] - trainLoss: 49.1896  Val_loss: 15.2817 \n",
      "Epoch 72/500\n",
      "383/383 [==============================] - trainLoss: 49.3438  Val_loss: 15.2273 \n",
      "Epoch 73/500\n",
      "383/383 [==============================] - trainLoss: 47.7477  Val_loss: 15.2018 \n",
      "Epoch 74/500\n",
      "383/383 [==============================] - trainLoss: 47.8565  Val_loss: 15.1374 \n",
      "Epoch 75/500\n",
      "383/383 [==============================] - trainLoss: 47.2949  Val_loss: 15.1094 \n",
      "Epoch 76/500\n",
      "383/383 [==============================] - trainLoss: 48.0265  Val_loss: 15.0753 \n",
      "Epoch 77/500\n",
      "383/383 [==============================] - trainLoss: 44.8083  Val_loss: 15.0543 \n",
      "Epoch 78/500\n",
      "383/383 [==============================] - trainLoss: 45.9495  Val_loss: 15.0146 \n",
      "Epoch 79/500\n",
      "383/383 [==============================] - trainLoss: 44.3541  Val_loss: 14.9585 \n",
      "Epoch 80/500\n",
      "383/383 [==============================] - trainLoss: 45.8021  Val_loss: 14.9354 \n",
      "Epoch 81/500\n",
      "383/383 [==============================] - trainLoss: 43.5832  Val_loss: 14.9029 \n",
      "Epoch 82/500\n",
      "383/383 [==============================] - trainLoss: 44.7489  Val_loss: 14.8787 \n",
      "Epoch 83/500\n",
      "383/383 [==============================] - trainLoss: 45.5950  Val_loss: 14.8426 \n",
      "Epoch 84/500\n",
      "383/383 [==============================] - trainLoss: 42.9288  Val_loss: 14.8071 \n",
      "Epoch 85/500\n",
      "383/383 [==============================] - trainLoss: 43.2090  Val_loss: 14.7912 \n",
      "Epoch 86/500\n",
      "383/383 [==============================] - trainLoss: 42.5040  Val_loss: 14.7670 \n",
      "Epoch 87/500\n",
      "383/383 [==============================] - trainLoss: 43.8120  Val_loss: 14.7271 \n",
      "Epoch 88/500\n",
      "383/383 [==============================] - trainLoss: 41.4267  Val_loss: 14.6982 \n",
      "Epoch 89/500\n",
      "383/383 [==============================] - trainLoss: 40.6418  Val_loss: 14.6902 \n",
      "Epoch 90/500\n",
      "383/383 [==============================] - trainLoss: 41.8748  Val_loss: 14.6663 \n",
      "Epoch 91/500\n",
      "383/383 [==============================] - trainLoss: 41.1689  Val_loss: 14.6543 \n",
      "Epoch 92/500\n",
      "383/383 [==============================] - trainLoss: 40.8156  Val_loss: 14.6360 \n",
      "Epoch 93/500\n",
      "383/383 [==============================] - trainLoss: 40.8492  Val_loss: 14.6219 \n",
      "Epoch 94/500\n",
      "383/383 [==============================] - trainLoss: 39.0874  Val_loss: 14.5899 \n",
      "Epoch 95/500\n",
      "383/383 [==============================] - trainLoss: 38.7138  Val_loss: 14.5516 \n",
      "Epoch 96/500\n",
      "383/383 [==============================] - trainLoss: 38.3133  Val_loss: 14.5102 \n",
      "Epoch 97/500\n",
      "383/383 [==============================] - trainLoss: 38.8246  Val_loss: 14.4927 \n",
      "Epoch 98/500\n",
      "383/383 [==============================] - trainLoss: 38.1643  Val_loss: 14.4649 \n",
      "Epoch 99/500\n",
      "383/383 [==============================] - trainLoss: 36.6685  Val_loss: 14.4581 \n",
      "Epoch 100/500\n",
      "383/383 [==============================] - trainLoss: 37.0197  Val_loss: 14.4230 \n",
      "Epoch 101/500\n",
      "383/383 [==============================] - trainLoss: 35.8507  Val_loss: 14.4006 \n",
      "Epoch 102/500\n",
      "383/383 [==============================] - trainLoss: 36.6813  Val_loss: 14.4026 \n",
      "Epoch 103/500\n",
      "383/383 [==============================] - trainLoss: 36.3455  Val_loss: 14.3775 \n",
      "Epoch 104/500\n",
      "383/383 [==============================] - trainLoss: 36.0705  Val_loss: 14.3625 \n",
      "Epoch 105/500\n",
      "383/383 [==============================] - trainLoss: 36.2365  Val_loss: 14.3304 \n",
      "Epoch 106/500\n",
      "383/383 [==============================] - trainLoss: 35.5013  Val_loss: 14.3139 \n",
      "Epoch 107/500\n",
      "383/383 [==============================] - trainLoss: 35.3559  Val_loss: 14.2759 \n",
      "Epoch 108/500\n",
      "383/383 [==============================] - trainLoss: 35.3406  Val_loss: 14.2426 \n",
      "Epoch 109/500\n",
      "383/383 [==============================] - trainLoss: 34.6127  Val_loss: 14.2181 \n",
      "Epoch 110/500\n",
      "383/383 [==============================] - trainLoss: 34.2611  Val_loss: 14.1798 \n",
      "Epoch 111/500\n",
      "383/383 [==============================] - trainLoss: 34.0305  Val_loss: 14.1581 \n",
      "Epoch 112/500\n",
      "383/383 [==============================] - trainLoss: 33.8250  Val_loss: 14.1454 \n",
      "Epoch 113/500\n",
      "383/383 [==============================] - trainLoss: 34.3613  Val_loss: 14.1131 \n",
      "Epoch 114/500\n",
      "383/383 [==============================] - trainLoss: 33.2478  Val_loss: 14.1125 \n",
      "Epoch 115/500\n",
      "383/383 [==============================] - trainLoss: 32.8527  Val_loss: 14.1062 \n",
      "Epoch 116/500\n",
      "383/383 [==============================] - trainLoss: 33.8879  Val_loss: 14.1112 \n",
      "Epoch 117/500\n",
      "383/383 [==============================] - trainLoss: 33.0592  Val_loss: 14.1034 \n",
      "Epoch 118/500\n",
      "383/383 [==============================] - trainLoss: 32.5087  Val_loss: 14.0855 \n",
      "Epoch 119/500\n",
      "383/383 [==============================] - trainLoss: 33.1315  Val_loss: 14.0755 \n",
      "Epoch 120/500\n",
      "383/383 [==============================] - trainLoss: 31.7414  Val_loss: 14.0406 \n",
      "Epoch 121/500\n",
      "383/383 [==============================] - trainLoss: 31.9733  Val_loss: 14.0172 \n",
      "Epoch 122/500\n",
      "383/383 [==============================] - trainLoss: 30.4199  Val_loss: 14.0024 \n",
      "Epoch 123/500\n",
      "383/383 [==============================] - trainLoss: 31.4231  Val_loss: 13.9870 \n",
      "Epoch 124/500\n",
      "383/383 [==============================] - trainLoss: 31.2282  Val_loss: 13.9861 \n",
      "Epoch 125/500\n",
      "383/383 [==============================] - trainLoss: 30.9133  Val_loss: 13.9685 \n",
      "Epoch 126/500\n",
      "383/383 [==============================] - trainLoss: 30.5310  Val_loss: 13.9655 \n",
      "Epoch 127/500\n",
      "383/383 [==============================] - trainLoss: 30.5846  Val_loss: 13.9727 \n",
      "Epoch 128/500\n",
      "383/383 [==============================] - trainLoss: 30.4053  Val_loss: 13.9488 \n",
      "Epoch 129/500\n",
      "383/383 [==============================] - trainLoss: 29.4651  Val_loss: 13.9226 \n",
      "Epoch 130/500\n",
      "383/383 [==============================] - trainLoss: 29.5748  Val_loss: 13.9219 \n",
      "Epoch 131/500\n",
      "383/383 [==============================] - trainLoss: 29.3079  Val_loss: 13.9096 \n",
      "Epoch 132/500\n",
      "383/383 [==============================] - trainLoss: 29.2450  Val_loss: 13.9044 \n",
      "Epoch 133/500\n",
      "383/383 [==============================] - trainLoss: 28.2716  Val_loss: 13.8811 \n",
      "Epoch 134/500\n",
      "383/383 [==============================] - trainLoss: 29.0600  Val_loss: 13.8789 \n",
      "Epoch 135/500\n",
      "383/383 [==============================] - trainLoss: 28.4519  Val_loss: 13.8684 \n",
      "Epoch 136/500\n",
      "383/383 [==============================] - trainLoss: 27.9147  Val_loss: 13.8577 \n",
      "Epoch 137/500\n",
      "383/383 [==============================] - trainLoss: 27.6371  Val_loss: 13.8361 \n",
      "Epoch 138/500\n",
      "383/383 [==============================] - trainLoss: 28.2822  Val_loss: 13.8121 \n",
      "Epoch 139/500\n",
      "383/383 [==============================] - trainLoss: 27.4942  Val_loss: 13.8057 \n",
      "Epoch 140/500\n",
      "383/383 [==============================] - trainLoss: 28.1336  Val_loss: 13.7935 \n",
      "Epoch 141/500\n",
      "383/383 [==============================] - trainLoss: 27.6989  Val_loss: 13.7927 \n",
      "Epoch 142/500\n",
      "383/383 [==============================] - trainLoss: 27.0546  Val_loss: 13.7873 \n",
      "Epoch 143/500\n",
      "383/383 [==============================] - trainLoss: 26.7330  Val_loss: 13.7779 \n",
      "Epoch 144/500\n",
      "383/383 [==============================] - trainLoss: 26.2639  Val_loss: 13.7842 \n",
      "Epoch 145/500\n",
      "383/383 [==============================] - trainLoss: 26.5365  Val_loss: 13.7838 \n",
      "Epoch 146/500\n",
      "383/383 [==============================] - trainLoss: 26.2396  Val_loss: 13.7829 \n",
      "Epoch 147/500\n",
      "383/383 [==============================] - trainLoss: 26.1700  Val_loss: 13.7538 \n",
      "Epoch 148/500\n",
      "383/383 [==============================] - trainLoss: 25.8386  Val_loss: 13.7387 \n",
      "Epoch 149/500\n",
      "383/383 [==============================] - trainLoss: 25.6138  Val_loss: 13.7385 \n",
      "Epoch 150/500\n",
      "383/383 [==============================] - trainLoss: 26.9208  Val_loss: 13.7114 \n",
      "Epoch 151/500\n",
      "383/383 [==============================] - trainLoss: 25.4948  Val_loss: 13.7117 \n",
      "Epoch 152/500\n",
      "383/383 [==============================] - trainLoss: 24.9631  Val_loss: 13.6765 \n",
      "Epoch 153/500\n",
      "383/383 [==============================] - trainLoss: 25.2444  Val_loss: 13.6620 \n",
      "Epoch 154/500\n",
      "383/383 [==============================] - trainLoss: 25.2692  Val_loss: 13.6631 \n",
      "Epoch 155/500\n",
      "383/383 [==============================] - trainLoss: 25.1443  Val_loss: 13.6553 \n",
      "Epoch 156/500\n",
      "383/383 [==============================] - trainLoss: 24.6874  Val_loss: 13.6526 \n",
      "Epoch 157/500\n",
      "383/383 [==============================] - trainLoss: 24.1432  Val_loss: 13.6304 \n",
      "Epoch 158/500\n",
      "383/383 [==============================] - trainLoss: 23.4290  Val_loss: 13.6369 \n",
      "Epoch 159/500\n",
      "383/383 [==============================] - trainLoss: 24.3789  Val_loss: 13.6674 \n",
      "Epoch 160/500\n",
      "383/383 [==============================] - trainLoss: 23.5948  Val_loss: 13.6246 \n",
      "Epoch 161/500\n",
      "383/383 [==============================] - trainLoss: 23.4046  Val_loss: 13.6075 \n",
      "Epoch 162/500\n",
      "383/383 [==============================] - trainLoss: 23.7027  Val_loss: 13.5974 \n",
      "Epoch 163/500\n",
      "383/383 [==============================] - trainLoss: 24.2954  Val_loss: 13.5929 \n",
      "Epoch 164/500\n",
      "383/383 [==============================] - trainLoss: 23.0863  Val_loss: 13.5853 \n",
      "Epoch 165/500\n",
      "383/383 [==============================] - trainLoss: 22.9080  Val_loss: 13.5845 \n",
      "Epoch 166/500\n",
      "383/383 [==============================] - trainLoss: 23.0014  Val_loss: 13.5804 \n",
      "Epoch 167/500\n",
      "383/383 [==============================] - trainLoss: 22.6542  Val_loss: 13.5698 \n",
      "Epoch 168/500\n",
      "383/383 [==============================] - trainLoss: 22.3772  Val_loss: 13.5695 \n",
      "Epoch 169/500\n",
      "383/383 [==============================] - trainLoss: 23.1694  Val_loss: 13.5660 \n",
      "Epoch 170/500\n",
      "383/383 [==============================] - trainLoss: 21.9510  Val_loss: 13.5665 \n",
      "Epoch 171/500\n",
      "383/383 [==============================] - trainLoss: 22.8065  Val_loss: 13.5514 \n",
      "Epoch 172/500\n",
      "383/383 [==============================] - trainLoss: 21.5039  Val_loss: 13.5253 \n",
      "Epoch 173/500\n",
      "383/383 [==============================] - trainLoss: 21.6080  Val_loss: 13.5399 \n",
      "Epoch 174/500\n",
      "383/383 [==============================] - trainLoss: 21.6549  Val_loss: 13.5352 \n",
      "Epoch 175/500\n",
      "383/383 [==============================] - trainLoss: 21.6517  Val_loss: 13.5128 \n",
      "Epoch 176/500\n",
      "383/383 [==============================] - trainLoss: 20.9764  Val_loss: 13.5033 \n",
      "Epoch 177/500\n",
      "383/383 [==============================] - trainLoss: 21.2082  Val_loss: 13.4835 \n",
      "Epoch 178/500\n",
      "383/383 [==============================] - trainLoss: 21.3840  Val_loss: 13.4759 \n",
      "Epoch 179/500\n",
      "383/383 [==============================] - trainLoss: 21.0923  Val_loss: 13.4695 \n",
      "Epoch 180/500\n",
      "383/383 [==============================] - trainLoss: 21.3322  Val_loss: 13.4647 \n",
      "Epoch 181/500\n",
      "383/383 [==============================] - trainLoss: 20.9222  Val_loss: 13.4665 \n",
      "Epoch 182/500\n",
      "383/383 [==============================] - trainLoss: 20.5357  Val_loss: 13.4727 \n",
      "Epoch 183/500\n",
      "383/383 [==============================] - trainLoss: 20.5731  Val_loss: 13.4864 \n",
      "Epoch 184/500\n",
      "383/383 [==============================] - trainLoss: 20.6325  Val_loss: 13.4814 \n",
      "Epoch 185/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "383/383 [==============================] - trainLoss: 20.0047  Val_loss: 13.4636 \n",
      "Epoch 186/500\n",
      "383/383 [==============================] - trainLoss: 20.5269  Val_loss: 13.4392 \n",
      "Epoch 187/500\n",
      "383/383 [==============================] - trainLoss: 20.1262  Val_loss: 13.4380 \n",
      "Epoch 188/500\n",
      "383/383 [==============================] - trainLoss: 19.5478  Val_loss: 13.4127 \n",
      "Epoch 189/500\n",
      "383/383 [==============================] - trainLoss: 19.8065  Val_loss: 13.3892 \n",
      "Epoch 190/500\n",
      "383/383 [==============================] - trainLoss: 19.6267  Val_loss: 13.3672 \n",
      "Epoch 191/500\n",
      "383/383 [==============================] - trainLoss: 19.7830  Val_loss: 13.3682 \n",
      "Epoch 192/500\n",
      "383/383 [==============================] - trainLoss: 19.3273  Val_loss: 13.3699 \n",
      "Epoch 193/500\n",
      "383/383 [==============================] - trainLoss: 19.2940  Val_loss: 13.3768 \n",
      "Epoch 194/500\n",
      "383/383 [==============================] - trainLoss: 19.6538  Val_loss: 13.3815 \n",
      "Epoch 195/500\n",
      "383/383 [==============================] - trainLoss: 19.0472  Val_loss: 13.3997 \n",
      "Epoch 196/500\n",
      "383/383 [==============================] - trainLoss: 19.0212  Val_loss: 13.4084 \n",
      "Epoch 197/500\n",
      "383/383 [==============================] - trainLoss: 18.4030  Val_loss: 13.4099 \n",
      "Epoch 198/500\n",
      "383/383 [==============================] - trainLoss: 18.7263  Val_loss: 13.4076 \n",
      "Epoch 199/500\n",
      "383/383 [==============================] - trainLoss: 18.6002  Val_loss: 13.4008 \n",
      "WARNING:tensorflow:No training configuration found in save file: the model was *not* compiled. Compile it manually.\n",
      "WARNING:tensorflow:No training configuration found in save file: the model was *not* compiled. Compile it manually.\n",
      "WARNING:tensorflow:No training configuration found in save file: the model was *not* compiled. Compile it manually.\n",
      "WARNING:tensorflow:No training configuration found in save file: the model was *not* compiled. Compile it manually.\n",
      "Elapsed/s:  141.4534249305725\n",
      "Final training loss:  tf.Tensor(18.600174, shape=(), dtype=float32)\n",
      "best val loss:  tf.Tensor(13.367176, shape=(), dtype=float32)\n",
      "{0: {'parameters': {'codings_size': 208, 'n_hidden_classifier': 1, 'N': 100, 'beta': 15, 'n_neurons': 516, 'n_neurons_classifier': 43, 'learning_rate': 0.001, 'n_hidden': 1}, 'val_loss': <tf.Tensor: shape=(), dtype=float32, numpy=12.73137>}, 1: {'parameters': {'codings_size': 207, 'n_hidden_classifier': 1, 'N': 50, 'beta': 1, 'n_neurons': 152, 'n_neurons_classifier': 40, 'learning_rate': 0.001, 'n_hidden': 1}, 'val_loss': <tf.Tensor: shape=(), dtype=float32, numpy=12.758459>}, 2: {'parameters': {'codings_size': 177, 'n_hidden_classifier': 2, 'N': 50, 'beta': 15, 'n_neurons': 398, 'n_neurons_classifier': 80, 'learning_rate': 0.0001, 'n_hidden': 1}, 'val_loss': <tf.Tensor: shape=(), dtype=float32, numpy=13.014389>}, 3: {'parameters': {'codings_size': 57, 'n_hidden_classifier': 2, 'N': 1, 'beta': 15, 'n_neurons': 516, 'n_neurons_classifier': 43, 'learning_rate': 0.001, 'n_hidden': 1}, 'val_loss': <tf.Tensor: shape=(), dtype=float32, numpy=12.97913>}, 4: {'parameters': {'codings_size': 126, 'n_hidden_classifier': 2, 'N': 100, 'beta': 1, 'n_neurons': 422, 'n_neurons_classifier': 52, 'learning_rate': 0.0001, 'n_hidden': 2}, 'val_loss': <tf.Tensor: shape=(), dtype=float32, numpy=13.502145>}, 5: {'parameters': {'codings_size': 230, 'n_hidden_classifier': 2, 'N': 50, 'beta': 1, 'n_neurons': 380, 'n_neurons_classifier': 57, 'learning_rate': 0.0005, 'n_hidden': 2}, 'val_loss': <tf.Tensor: shape=(), dtype=float32, numpy=12.909143>}, 6: {'parameters': {'codings_size': 94, 'n_hidden_classifier': 1, 'N': 150, 'beta': 1, 'n_neurons': 422, 'n_neurons_classifier': 41, 'learning_rate': 0.0001, 'n_hidden': 2}, 'val_loss': <tf.Tensor: shape=(), dtype=float32, numpy=13.702262>}, 7: {'parameters': {'codings_size': 207, 'n_hidden_classifier': 2, 'N': 100, 'beta': 15, 'n_neurons': 264, 'n_neurons_classifier': 80, 'learning_rate': 0.0005, 'n_hidden': 1}, 'val_loss': <tf.Tensor: shape=(), dtype=float32, numpy=13.019903>}, 8: {'parameters': {'codings_size': 211, 'n_hidden_classifier': 2, 'N': 1, 'beta': 10, 'n_neurons': 152, 'n_neurons_classifier': 40, 'learning_rate': 0.0001, 'n_hidden': 2}, 'val_loss': <tf.Tensor: shape=(), dtype=float32, numpy=13.367176>}}\n",
      "Epoch 0/500\n",
      "WARNING:tensorflow:Layer full_model_only_labelled is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because it's dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "320/383 [========================>.....] - Loss for batch: 301.1646WARNING:tensorflow:Layer concatenate_1 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because it's dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "383/383 [==============================] - trainLoss: 301.1646  Val_loss: 24.9096 \n",
      "Epoch 1/500\n",
      "383/383 [==============================] - trainLoss: 97.8253  Val_loss: 23.3092 \n",
      "Epoch 2/500\n",
      "383/383 [==============================] - trainLoss: 74.6976  Val_loss: 21.7824 \n",
      "Epoch 3/500\n",
      "383/383 [==============================] - trainLoss: 66.7862  Val_loss: 20.5953 \n",
      "Epoch 4/500\n",
      "383/383 [==============================] - trainLoss: 62.2223  Val_loss: 19.6567 \n",
      "Epoch 5/500\n",
      "383/383 [==============================] - trainLoss: 57.7309  Val_loss: 18.8970 \n",
      "Epoch 6/500\n",
      "383/383 [==============================] - trainLoss: 53.6977  Val_loss: 18.3418 \n",
      "Epoch 7/500\n",
      "383/383 [==============================] - trainLoss: 51.4000  Val_loss: 17.8516 \n",
      "Epoch 8/500\n",
      "383/383 [==============================] - trainLoss: 50.0319  Val_loss: 17.4485 \n",
      "Epoch 9/500\n",
      "383/383 [==============================] - trainLoss: 47.9202  Val_loss: 17.1179 \n",
      "Epoch 10/500\n",
      "383/383 [==============================] - trainLoss: 45.7145  Val_loss: 16.8498 \n",
      "Epoch 11/500\n",
      "383/383 [==============================] - trainLoss: 45.1660  Val_loss: 16.4901 \n",
      "Epoch 12/500\n",
      "383/383 [==============================] - trainLoss: 44.1348  Val_loss: 16.3106 \n",
      "Epoch 13/500\n",
      "383/383 [==============================] - trainLoss: 41.1251  Val_loss: 16.1347 \n",
      "Epoch 14/500\n",
      "383/383 [==============================] - trainLoss: 40.7944  Val_loss: 15.9658 \n",
      "Epoch 15/500\n",
      "383/383 [==============================] - trainLoss: 39.3781  Val_loss: 15.8282 \n",
      "Epoch 16/500\n",
      "383/383 [==============================] - trainLoss: 37.6356  Val_loss: 15.6923 \n",
      "Epoch 17/500\n",
      "383/383 [==============================] - trainLoss: 37.9014  Val_loss: 15.5667 \n",
      "Epoch 18/500\n",
      "383/383 [==============================] - trainLoss: 36.1617  Val_loss: 15.4602 \n",
      "Epoch 19/500\n",
      "383/383 [==============================] - trainLoss: 34.5072  Val_loss: 15.3296 \n",
      "Epoch 20/500\n",
      "383/383 [==============================] - trainLoss: 34.6553  Val_loss: 15.1879 \n",
      "Epoch 21/500\n",
      "383/383 [==============================] - trainLoss: 33.5174  Val_loss: 15.0898 \n",
      "Epoch 22/500\n",
      "383/383 [==============================] - trainLoss: 32.9032  Val_loss: 15.0106 \n",
      "Epoch 23/500\n",
      "383/383 [==============================] - trainLoss: 32.2691  Val_loss: 14.8897 \n",
      "Epoch 24/500\n",
      "383/383 [==============================] - trainLoss: 31.4288  Val_loss: 14.8215 \n",
      "Epoch 25/500\n",
      "383/383 [==============================] - trainLoss: 31.0242  Val_loss: 14.7222 \n",
      "Epoch 26/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "383/383 [==============================] - trainLoss: 30.2670  Val_loss: 14.6102 \n",
      "Epoch 27/500\n",
      "383/383 [==============================] - trainLoss: 29.5120  Val_loss: 14.5233 \n",
      "Epoch 28/500\n",
      "383/383 [==============================] - trainLoss: 29.3031  Val_loss: 14.4160 \n",
      "Epoch 29/500\n",
      "383/383 [==============================] - trainLoss: 28.2803  Val_loss: 14.3146 \n",
      "Epoch 30/500\n",
      "383/383 [==============================] - trainLoss: 27.9622  Val_loss: 14.2724 \n",
      "Epoch 31/500\n",
      "383/383 [==============================] - trainLoss: 27.2859  Val_loss: 14.2190 \n",
      "Epoch 32/500\n",
      "383/383 [==============================] - trainLoss: 27.6318  Val_loss: 14.1706 \n",
      "Epoch 33/500\n",
      "383/383 [==============================] - trainLoss: 26.6218  Val_loss: 14.1399 \n",
      "Epoch 34/500\n",
      "383/383 [==============================] - trainLoss: 26.2902  Val_loss: 14.0635 \n",
      "Epoch 35/500\n",
      "383/383 [==============================] - trainLoss: 26.3932  Val_loss: 14.0279 \n",
      "Epoch 36/500\n",
      "383/383 [==============================] - trainLoss: 25.8923  Val_loss: 13.9803 \n",
      "Epoch 37/500\n",
      "383/383 [==============================] - trainLoss: 25.7469  Val_loss: 13.8866 \n",
      "Epoch 38/500\n",
      "383/383 [==============================] - trainLoss: 24.8655  Val_loss: 13.8238 \n",
      "Epoch 39/500\n",
      "383/383 [==============================] - trainLoss: 24.8792  Val_loss: 13.7866 \n",
      "Epoch 40/500\n",
      "383/383 [==============================] - trainLoss: 24.8782  Val_loss: 13.7562 \n",
      "Epoch 41/500\n",
      "383/383 [==============================] - trainLoss: 23.6189  Val_loss: 13.7019 \n",
      "Epoch 42/500\n",
      "383/383 [==============================] - trainLoss: 23.7271  Val_loss: 13.7071 \n",
      "Epoch 43/500\n",
      "383/383 [==============================] - trainLoss: 23.6154  Val_loss: 13.6779 \n",
      "Epoch 44/500\n",
      "383/383 [==============================] - trainLoss: 23.3583  Val_loss: 13.7013 \n",
      "Epoch 45/500\n",
      "383/383 [==============================] - trainLoss: 22.6035  Val_loss: 13.6917 \n",
      "Epoch 46/500\n",
      "383/383 [==============================] - trainLoss: 22.9293  Val_loss: 13.6781 \n",
      "Epoch 47/500\n",
      "383/383 [==============================] - trainLoss: 22.4821  Val_loss: 13.6049 \n",
      "Epoch 48/500\n",
      "383/383 [==============================] - trainLoss: 21.8770  Val_loss: 13.5366 \n",
      "Epoch 49/500\n",
      "383/383 [==============================] - trainLoss: 22.4963  Val_loss: 13.4844 \n",
      "Epoch 50/500\n",
      "383/383 [==============================] - trainLoss: 21.9639  Val_loss: 13.4721 \n",
      "Epoch 51/500\n",
      "383/383 [==============================] - trainLoss: 21.4240  Val_loss: 13.4602 \n",
      "Epoch 52/500\n",
      "383/383 [==============================] - trainLoss: 21.3148  Val_loss: 13.4494 \n",
      "Epoch 53/500\n",
      "383/383 [==============================] - trainLoss: 21.0813  Val_loss: 13.4658 \n",
      "Epoch 54/500\n",
      "383/383 [==============================] - trainLoss: 21.0067  Val_loss: 13.4446 \n",
      "Epoch 55/500\n",
      "383/383 [==============================] - trainLoss: 20.1592  Val_loss: 13.4332 \n",
      "Epoch 56/500\n",
      "383/383 [==============================] - trainLoss: 20.5457  Val_loss: 13.4324 \n",
      "Epoch 57/500\n",
      "383/383 [==============================] - trainLoss: 20.5847  Val_loss: 13.4038 \n",
      "Epoch 58/500\n",
      "383/383 [==============================] - trainLoss: 20.4645  Val_loss: 13.3562 \n",
      "Epoch 59/500\n",
      "383/383 [==============================] - trainLoss: 20.2118  Val_loss: 13.3203 \n",
      "Epoch 60/500\n",
      "383/383 [==============================] - trainLoss: 19.4869  Val_loss: 13.2763 \n",
      "Epoch 61/500\n",
      "383/383 [==============================] - trainLoss: 19.7086  Val_loss: 13.2582 \n",
      "Epoch 62/500\n",
      "383/383 [==============================] - trainLoss: 19.4757  Val_loss: 13.2757 \n",
      "Epoch 63/500\n",
      "383/383 [==============================] - trainLoss: 19.2015  Val_loss: 13.2860 \n",
      "Epoch 64/500\n",
      "383/383 [==============================] - trainLoss: 19.3878  Val_loss: 13.2813 \n",
      "Epoch 65/500\n",
      "383/383 [==============================] - trainLoss: 18.9990  Val_loss: 13.2391 \n",
      "Epoch 66/500\n",
      "383/383 [==============================] - trainLoss: 19.1833  Val_loss: 13.2044 \n",
      "Epoch 67/500\n",
      "383/383 [==============================] - trainLoss: 18.8869  Val_loss: 13.1937 \n",
      "Epoch 68/500\n",
      "383/383 [==============================] - trainLoss: 18.7808  Val_loss: 13.2069 \n",
      "Epoch 69/500\n",
      "383/383 [==============================] - trainLoss: 18.4152  Val_loss: 13.1916 \n",
      "Epoch 70/500\n",
      "383/383 [==============================] - trainLoss: 18.6140  Val_loss: 13.1931 \n",
      "Epoch 71/500\n",
      "383/383 [==============================] - trainLoss: 18.5398  Val_loss: 13.1654 \n",
      "Epoch 72/500\n",
      "383/383 [==============================] - trainLoss: 17.9630  Val_loss: 13.1260 \n",
      "Epoch 73/500\n",
      "383/383 [==============================] - trainLoss: 17.8574  Val_loss: 13.1387 \n",
      "Epoch 74/500\n",
      "383/383 [==============================] - trainLoss: 18.1306  Val_loss: 13.1181 \n",
      "Epoch 75/500\n",
      "383/383 [==============================] - trainLoss: 17.8309  Val_loss: 13.1043 \n",
      "Epoch 76/500\n",
      "383/383 [==============================] - trainLoss: 17.3194  Val_loss: 13.0725 \n",
      "Epoch 77/500\n",
      "383/383 [==============================] - trainLoss: 17.2799  Val_loss: 13.0607 \n",
      "Epoch 78/500\n",
      "383/383 [==============================] - trainLoss: 17.5407  Val_loss: 13.0087 \n",
      "Epoch 79/500\n",
      "383/383 [==============================] - trainLoss: 17.4998  Val_loss: 12.9647 \n",
      "Epoch 80/500\n",
      "383/383 [==============================] - trainLoss: 16.9358  Val_loss: 12.9656 \n",
      "Epoch 81/500\n",
      "383/383 [==============================] - trainLoss: 17.0959  Val_loss: 12.9543 \n",
      "Epoch 82/500\n",
      "383/383 [==============================] - trainLoss: 16.9650  Val_loss: 12.9343 \n",
      "Epoch 83/500\n",
      "383/383 [==============================] - trainLoss: 17.0037  Val_loss: 12.9745 \n",
      "Epoch 84/500\n",
      "383/383 [==============================] - trainLoss: 16.6394  Val_loss: 13.0004 \n",
      "Epoch 85/500\n",
      "383/383 [==============================] - trainLoss: 16.5056  Val_loss: 12.9989 \n",
      "Epoch 86/500\n",
      "383/383 [==============================] - trainLoss: 16.4868  Val_loss: 13.0135 \n",
      "Epoch 87/500\n",
      "383/383 [==============================] - trainLoss: 16.6828  Val_loss: 13.0022 \n",
      "Epoch 88/500\n",
      "383/383 [==============================] - trainLoss: 16.5069  Val_loss: 12.9752 \n",
      "Epoch 89/500\n",
      "383/383 [==============================] - trainLoss: 16.3656  Val_loss: 12.9911 \n",
      "Epoch 90/500\n",
      "383/383 [==============================] - trainLoss: 16.2460  Val_loss: 12.9702 \n",
      "Epoch 91/500\n",
      "383/383 [==============================] - trainLoss: 16.2867  Val_loss: 12.9544 \n",
      "WARNING:tensorflow:No training configuration found in save file: the model was *not* compiled. Compile it manually.\n",
      "WARNING:tensorflow:No training configuration found in save file: the model was *not* compiled. Compile it manually.\n",
      "WARNING:tensorflow:No training configuration found in save file: the model was *not* compiled. Compile it manually.\n",
      "WARNING:tensorflow:No training configuration found in save file: the model was *not* compiled. Compile it manually.\n",
      "Elapsed/s:  59.43795299530029\n",
      "Final training loss:  tf.Tensor(16.286701, shape=(), dtype=float32)\n",
      "best val loss:  tf.Tensor(12.934335, shape=(), dtype=float32)\n",
      "{0: {'parameters': {'codings_size': 208, 'n_hidden_classifier': 1, 'N': 100, 'beta': 15, 'n_neurons': 516, 'n_neurons_classifier': 43, 'learning_rate': 0.001, 'n_hidden': 1}, 'val_loss': <tf.Tensor: shape=(), dtype=float32, numpy=12.73137>}, 1: {'parameters': {'codings_size': 207, 'n_hidden_classifier': 1, 'N': 50, 'beta': 1, 'n_neurons': 152, 'n_neurons_classifier': 40, 'learning_rate': 0.001, 'n_hidden': 1}, 'val_loss': <tf.Tensor: shape=(), dtype=float32, numpy=12.758459>}, 2: {'parameters': {'codings_size': 177, 'n_hidden_classifier': 2, 'N': 50, 'beta': 15, 'n_neurons': 398, 'n_neurons_classifier': 80, 'learning_rate': 0.0001, 'n_hidden': 1}, 'val_loss': <tf.Tensor: shape=(), dtype=float32, numpy=13.014389>}, 3: {'parameters': {'codings_size': 57, 'n_hidden_classifier': 2, 'N': 1, 'beta': 15, 'n_neurons': 516, 'n_neurons_classifier': 43, 'learning_rate': 0.001, 'n_hidden': 1}, 'val_loss': <tf.Tensor: shape=(), dtype=float32, numpy=12.97913>}, 4: {'parameters': {'codings_size': 126, 'n_hidden_classifier': 2, 'N': 100, 'beta': 1, 'n_neurons': 422, 'n_neurons_classifier': 52, 'learning_rate': 0.0001, 'n_hidden': 2}, 'val_loss': <tf.Tensor: shape=(), dtype=float32, numpy=13.502145>}, 5: {'parameters': {'codings_size': 230, 'n_hidden_classifier': 2, 'N': 50, 'beta': 1, 'n_neurons': 380, 'n_neurons_classifier': 57, 'learning_rate': 0.0005, 'n_hidden': 2}, 'val_loss': <tf.Tensor: shape=(), dtype=float32, numpy=12.909143>}, 6: {'parameters': {'codings_size': 94, 'n_hidden_classifier': 1, 'N': 150, 'beta': 1, 'n_neurons': 422, 'n_neurons_classifier': 41, 'learning_rate': 0.0001, 'n_hidden': 2}, 'val_loss': <tf.Tensor: shape=(), dtype=float32, numpy=13.702262>}, 7: {'parameters': {'codings_size': 207, 'n_hidden_classifier': 2, 'N': 100, 'beta': 15, 'n_neurons': 264, 'n_neurons_classifier': 80, 'learning_rate': 0.0005, 'n_hidden': 1}, 'val_loss': <tf.Tensor: shape=(), dtype=float32, numpy=13.019903>}, 8: {'parameters': {'codings_size': 211, 'n_hidden_classifier': 2, 'N': 1, 'beta': 10, 'n_neurons': 152, 'n_neurons_classifier': 40, 'learning_rate': 0.0001, 'n_hidden': 2}, 'val_loss': <tf.Tensor: shape=(), dtype=float32, numpy=13.367176>}, 9: {'parameters': {'codings_size': 255, 'n_hidden_classifier': 1, 'N': 50, 'beta': 10, 'n_neurons': 152, 'n_neurons_classifier': 52, 'learning_rate': 0.0005, 'n_hidden': 1}, 'val_loss': <tf.Tensor: shape=(), dtype=float32, numpy=12.934335>}}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:No training configuration found in save file: the model was *not* compiled. Compile it manually.\n",
      "WARNING:tensorflow:No training configuration found in save file: the model was *not* compiled. Compile it manually.\n",
      "WARNING:tensorflow:No training configuration found in save file: the model was *not* compiled. Compile it manually.\n",
      "WARNING:tensorflow:No training configuration found in save file: the model was *not* compiled. Compile it manually.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[(0,\n",
       "  {'parameters': {'N': 100,\n",
       "    'beta': 15,\n",
       "    'codings_size': 208,\n",
       "    'learning_rate': 0.001,\n",
       "    'n_hidden': 1,\n",
       "    'n_hidden_classifier': 1,\n",
       "    'n_neurons': 516,\n",
       "    'n_neurons_classifier': 43},\n",
       "   'val_loss': <tf.Tensor: shape=(), dtype=float32, numpy=12.73137>}),\n",
       " (1,\n",
       "  {'parameters': {'N': 50,\n",
       "    'beta': 1,\n",
       "    'codings_size': 207,\n",
       "    'learning_rate': 0.001,\n",
       "    'n_hidden': 1,\n",
       "    'n_hidden_classifier': 1,\n",
       "    'n_neurons': 152,\n",
       "    'n_neurons_classifier': 40},\n",
       "   'val_loss': <tf.Tensor: shape=(), dtype=float32, numpy=12.758459>}),\n",
       " (5,\n",
       "  {'parameters': {'N': 50,\n",
       "    'beta': 1,\n",
       "    'codings_size': 230,\n",
       "    'learning_rate': 0.0005,\n",
       "    'n_hidden': 2,\n",
       "    'n_hidden_classifier': 2,\n",
       "    'n_neurons': 380,\n",
       "    'n_neurons_classifier': 57},\n",
       "   'val_loss': <tf.Tensor: shape=(), dtype=float32, numpy=12.909143>}),\n",
       " (9,\n",
       "  {'parameters': {'N': 50,\n",
       "    'beta': 10,\n",
       "    'codings_size': 255,\n",
       "    'learning_rate': 0.0005,\n",
       "    'n_hidden': 1,\n",
       "    'n_hidden_classifier': 1,\n",
       "    'n_neurons': 152,\n",
       "    'n_neurons_classifier': 52},\n",
       "   'val_loss': <tf.Tensor: shape=(), dtype=float32, numpy=12.934335>}),\n",
       " (3,\n",
       "  {'parameters': {'N': 1,\n",
       "    'beta': 15,\n",
       "    'codings_size': 57,\n",
       "    'learning_rate': 0.001,\n",
       "    'n_hidden': 1,\n",
       "    'n_hidden_classifier': 2,\n",
       "    'n_neurons': 516,\n",
       "    'n_neurons_classifier': 43},\n",
       "   'val_loss': <tf.Tensor: shape=(), dtype=float32, numpy=12.97913>}),\n",
       " (2,\n",
       "  {'parameters': {'N': 50,\n",
       "    'beta': 15,\n",
       "    'codings_size': 177,\n",
       "    'learning_rate': 0.0001,\n",
       "    'n_hidden': 1,\n",
       "    'n_hidden_classifier': 2,\n",
       "    'n_neurons': 398,\n",
       "    'n_neurons_classifier': 80},\n",
       "   'val_loss': <tf.Tensor: shape=(), dtype=float32, numpy=13.014389>}),\n",
       " (7,\n",
       "  {'parameters': {'N': 100,\n",
       "    'beta': 15,\n",
       "    'codings_size': 207,\n",
       "    'learning_rate': 0.0005,\n",
       "    'n_hidden': 1,\n",
       "    'n_hidden_classifier': 2,\n",
       "    'n_neurons': 264,\n",
       "    'n_neurons_classifier': 80},\n",
       "   'val_loss': <tf.Tensor: shape=(), dtype=float32, numpy=13.019903>}),\n",
       " (8,\n",
       "  {'parameters': {'N': 1,\n",
       "    'beta': 10,\n",
       "    'codings_size': 211,\n",
       "    'learning_rate': 0.0001,\n",
       "    'n_hidden': 2,\n",
       "    'n_hidden_classifier': 2,\n",
       "    'n_neurons': 152,\n",
       "    'n_neurons_classifier': 40},\n",
       "   'val_loss': <tf.Tensor: shape=(), dtype=float32, numpy=13.367176>}),\n",
       " (4,\n",
       "  {'parameters': {'N': 100,\n",
       "    'beta': 1,\n",
       "    'codings_size': 126,\n",
       "    'learning_rate': 0.0001,\n",
       "    'n_hidden': 2,\n",
       "    'n_hidden_classifier': 2,\n",
       "    'n_neurons': 422,\n",
       "    'n_neurons_classifier': 52},\n",
       "   'val_loss': <tf.Tensor: shape=(), dtype=float32, numpy=13.502145>}),\n",
       " (6,\n",
       "  {'parameters': {'N': 150,\n",
       "    'beta': 1,\n",
       "    'codings_size': 94,\n",
       "    'learning_rate': 0.0001,\n",
       "    'n_hidden': 2,\n",
       "    'n_hidden_classifier': 1,\n",
       "    'n_neurons': 422,\n",
       "    'n_neurons_classifier': 41},\n",
       "   'val_loss': <tf.Tensor: shape=(), dtype=float32, numpy=13.702262>})]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result,variational_encoder,variational_decoder,classifier,y_distribution = hyperparameter_search_labelled(param_distribs,500,10,n_iter=10)\n",
    "result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Best KL-based model has val nloglik of 12.73137. Epochs = 66\n",
    "\n",
    "{'parameters': {'N': 100,\n",
    "    'beta': 15,\n",
    "    'codings_size': 208,\n",
    "    'learning_rate': 0.001,\n",
    "    'n_hidden': 1,\n",
    "    'n_hidden_classifier': 1,\n",
    "    'n_neurons': 516,\n",
    "    'n_neurons_classifier': 43},\n",
    "   'val_loss': <tf.Tensor: shape=(), dtype=float32, numpy=12.73137>}\n",
    "     "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Single run # "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "variational_encoder,variational_decoder,classifier,y_distribution,model = build_model_only_labelled(n_hidden=1, \n",
    "                    n_neurons=516,input_shape=input_shape,beta=15,n_hidden_classifier=1,\n",
    "              n_neurons_classifier=43,N=100,codings_size=208)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0/150\n",
      "WARNING:tensorflow:Layer full_model_only_labelled_1 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because it's dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "320/383 [========================>.....] - Loss for batch: 611.4309WARNING:tensorflow:Layer concatenate_3 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because it's dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "383/383 [==============================] - trainLoss: 611.4309  Val_loss: 24.9826 \n",
      "Epoch 1/150\n",
      "383/383 [==============================] - trainLoss: 193.4082  Val_loss: 22.4472 \n",
      "Epoch 2/150\n",
      "383/383 [==============================] - trainLoss: 135.0973  Val_loss: 21.5446 \n",
      "Epoch 3/150\n",
      "383/383 [==============================] - trainLoss: 116.4249  Val_loss: 20.7219 \n",
      "Epoch 4/150\n",
      "383/383 [==============================] - trainLoss: 103.1366  Val_loss: 19.8937 \n",
      "Epoch 5/150\n",
      "383/383 [==============================] - trainLoss: 97.5240  Val_loss: 19.0715 \n",
      "Epoch 6/150\n",
      "383/383 [==============================] - trainLoss: 91.7798  Val_loss: 18.4052 \n",
      "Epoch 7/150\n",
      "383/383 [==============================] - trainLoss: 84.6630  Val_loss: 17.7689 \n",
      "Epoch 8/150\n",
      "383/383 [==============================] - trainLoss: 80.7554  Val_loss: 17.0924 \n",
      "Epoch 9/150\n",
      "383/383 [==============================] - trainLoss: 78.0345  Val_loss: 16.4407 \n",
      "Epoch 10/150\n",
      "383/383 [==============================] - trainLoss: 76.2658  Val_loss: 16.0168 \n",
      "Epoch 11/150\n",
      "383/383 [==============================] - trainLoss: 71.8081  Val_loss: 15.7211 \n",
      "Epoch 12/150\n",
      "383/383 [==============================] - trainLoss: 69.7961  Val_loss: 15.3994 \n",
      "Epoch 13/150\n",
      "383/383 [==============================] - trainLoss: 66.4787  Val_loss: 15.0847 \n",
      "Epoch 14/150\n",
      "383/383 [==============================] - trainLoss: 64.3716  Val_loss: 14.8559 \n",
      "Epoch 15/150\n",
      "383/383 [==============================] - trainLoss: 63.3549  Val_loss: 14.6137 \n",
      "Epoch 16/150\n",
      "383/383 [==============================] - trainLoss: 61.6459  Val_loss: 14.4789 \n",
      "Epoch 17/150\n",
      "383/383 [==============================] - trainLoss: 58.0959  Val_loss: 14.2463 \n",
      "Epoch 18/150\n",
      "383/383 [==============================] - trainLoss: 57.3996  Val_loss: 14.1239 \n",
      "Epoch 19/150\n",
      "383/383 [==============================] - trainLoss: 55.4338  Val_loss: 14.0317 \n",
      "Epoch 20/150\n",
      "383/383 [==============================] - trainLoss: 53.5078  Val_loss: 14.0666 \n",
      "Epoch 21/150\n",
      "383/383 [==============================] - trainLoss: 53.8652  Val_loss: 14.0607 \n",
      "Epoch 22/150\n",
      "383/383 [==============================] - trainLoss: 51.9172  Val_loss: 13.9506 \n",
      "Epoch 23/150\n",
      "383/383 [==============================] - trainLoss: 50.9850  Val_loss: 13.7667 \n",
      "Epoch 24/150\n",
      "383/383 [==============================] - trainLoss: 49.1251  Val_loss: 13.7367 \n",
      "Epoch 25/150\n",
      "383/383 [==============================] - trainLoss: 48.7094  Val_loss: 13.6724 \n",
      "Epoch 26/150\n",
      "383/383 [==============================] - trainLoss: 46.9090  Val_loss: 13.5378 \n",
      "Epoch 27/150\n",
      "383/383 [==============================] - trainLoss: 47.3274  Val_loss: 13.4952 \n",
      "Epoch 28/150\n",
      "383/383 [==============================] - trainLoss: 47.1926  Val_loss: 13.5888 \n",
      "Epoch 29/150\n",
      "383/383 [==============================] - trainLoss: 45.7338  Val_loss: 13.5413 \n",
      "Epoch 30/150\n",
      "383/383 [==============================] - trainLoss: 44.1310  Val_loss: 13.3940 \n",
      "Epoch 31/150\n",
      "383/383 [==============================] - trainLoss: 43.4547  Val_loss: 13.3512 \n",
      "Epoch 32/150\n",
      "383/383 [==============================] - trainLoss: 42.2718  Val_loss: 13.3314 \n",
      "Epoch 33/150\n",
      "383/383 [==============================] - trainLoss: 43.1499  Val_loss: 13.2966 \n",
      "Epoch 34/150\n",
      "383/383 [==============================] - trainLoss: 41.4642  Val_loss: 13.2951 \n",
      "Epoch 35/150\n",
      "383/383 [==============================] - trainLoss: 41.7342  Val_loss: 13.2807 \n",
      "Epoch 36/150\n",
      "383/383 [==============================] - trainLoss: 40.6989  Val_loss: 13.2060 \n",
      "Epoch 37/150\n",
      "383/383 [==============================] - trainLoss: 39.4150  Val_loss: 13.0799 \n",
      "Epoch 38/150\n",
      "383/383 [==============================] - trainLoss: 39.5117  Val_loss: 13.1863 \n",
      "Epoch 39/150\n",
      "383/383 [==============================] - trainLoss: 39.4434  Val_loss: 13.2283 \n",
      "Epoch 40/150\n",
      "383/383 [==============================] - trainLoss: 38.5748  Val_loss: 13.2137 \n",
      "Epoch 41/150\n",
      "383/383 [==============================] - trainLoss: 39.1552  Val_loss: 13.1054 \n",
      "Epoch 42/150\n",
      "383/383 [==============================] - trainLoss: 37.9839  Val_loss: 13.0827 \n",
      "Epoch 43/150\n",
      "383/383 [==============================] - trainLoss: 37.2289  Val_loss: 13.0861 \n",
      "Epoch 44/150\n",
      "383/383 [==============================] - trainLoss: 36.8576  Val_loss: 13.1312 \n",
      "Epoch 45/150\n",
      "383/383 [==============================] - trainLoss: 36.8565  Val_loss: 13.1036 \n",
      "Epoch 46/150\n",
      "383/383 [==============================] - trainLoss: 35.3267  Val_loss: 12.9595 \n",
      "Epoch 47/150\n",
      "383/383 [==============================] - trainLoss: 35.2493  Val_loss: 12.8728 \n",
      "Epoch 48/150\n",
      "383/383 [==============================] - trainLoss: 35.1301  Val_loss: 12.9179 \n",
      "Epoch 49/150\n",
      "383/383 [==============================] - trainLoss: 34.7273  Val_loss: 13.0549 \n",
      "Epoch 50/150\n",
      "383/383 [==============================] - trainLoss: 34.7801  Val_loss: 13.2273 \n",
      "Epoch 51/150\n",
      "383/383 [==============================] - trainLoss: 33.6765  Val_loss: 13.0783 \n",
      "Epoch 52/150\n",
      "383/383 [==============================] - trainLoss: 33.8942  Val_loss: 12.9557 \n",
      "Epoch 53/150\n",
      "383/383 [==============================] - trainLoss: 33.4769  Val_loss: 12.9880 \n",
      "Epoch 54/150\n",
      "383/383 [==============================] - trainLoss: 32.3598  Val_loss: 12.9714 \n",
      "Epoch 55/150\n",
      "383/383 [==============================] - trainLoss: 32.5043  Val_loss: 12.9997 \n",
      "Epoch 56/150\n",
      "383/383 [==============================] - trainLoss: 32.3359  Val_loss: 12.9755 \n",
      "WARNING:tensorflow:No training configuration found in save file: the model was *not* compiled. Compile it manually.\n",
      "WARNING:tensorflow:No training configuration found in save file: the model was *not* compiled. Compile it manually.\n",
      "WARNING:tensorflow:No training configuration found in save file: the model was *not* compiled. Compile it manually.\n",
      "WARNING:tensorflow:No training configuration found in save file: the model was *not* compiled. Compile it manually.\n",
      "Elapsed/s:  36.603230714797974\n",
      "Final training loss:  tf.Tensor(32.33592, shape=(), dtype=float32)\n",
      "best val loss:  tf.Tensor(12.872842, shape=(), dtype=float32)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "([<tf.Tensor: shape=(), dtype=float32, numpy=611.4309>,\n",
       "  <tf.Tensor: shape=(), dtype=float32, numpy=193.40825>,\n",
       "  <tf.Tensor: shape=(), dtype=float32, numpy=135.0973>,\n",
       "  <tf.Tensor: shape=(), dtype=float32, numpy=116.42486>,\n",
       "  <tf.Tensor: shape=(), dtype=float32, numpy=103.136566>,\n",
       "  <tf.Tensor: shape=(), dtype=float32, numpy=97.52397>,\n",
       "  <tf.Tensor: shape=(), dtype=float32, numpy=91.77984>,\n",
       "  <tf.Tensor: shape=(), dtype=float32, numpy=84.663>,\n",
       "  <tf.Tensor: shape=(), dtype=float32, numpy=80.755394>,\n",
       "  <tf.Tensor: shape=(), dtype=float32, numpy=78.03452>,\n",
       "  <tf.Tensor: shape=(), dtype=float32, numpy=76.26581>,\n",
       "  <tf.Tensor: shape=(), dtype=float32, numpy=71.808075>,\n",
       "  <tf.Tensor: shape=(), dtype=float32, numpy=69.796135>,\n",
       "  <tf.Tensor: shape=(), dtype=float32, numpy=66.47869>,\n",
       "  <tf.Tensor: shape=(), dtype=float32, numpy=64.37159>,\n",
       "  <tf.Tensor: shape=(), dtype=float32, numpy=63.35486>,\n",
       "  <tf.Tensor: shape=(), dtype=float32, numpy=61.64589>,\n",
       "  <tf.Tensor: shape=(), dtype=float32, numpy=58.0959>,\n",
       "  <tf.Tensor: shape=(), dtype=float32, numpy=57.3996>,\n",
       "  <tf.Tensor: shape=(), dtype=float32, numpy=55.43378>,\n",
       "  <tf.Tensor: shape=(), dtype=float32, numpy=53.507774>,\n",
       "  <tf.Tensor: shape=(), dtype=float32, numpy=53.86522>,\n",
       "  <tf.Tensor: shape=(), dtype=float32, numpy=51.917187>,\n",
       "  <tf.Tensor: shape=(), dtype=float32, numpy=50.985>,\n",
       "  <tf.Tensor: shape=(), dtype=float32, numpy=49.125126>,\n",
       "  <tf.Tensor: shape=(), dtype=float32, numpy=48.709423>,\n",
       "  <tf.Tensor: shape=(), dtype=float32, numpy=46.908985>,\n",
       "  <tf.Tensor: shape=(), dtype=float32, numpy=47.32744>,\n",
       "  <tf.Tensor: shape=(), dtype=float32, numpy=47.192593>,\n",
       "  <tf.Tensor: shape=(), dtype=float32, numpy=45.73381>,\n",
       "  <tf.Tensor: shape=(), dtype=float32, numpy=44.131004>,\n",
       "  <tf.Tensor: shape=(), dtype=float32, numpy=43.454704>,\n",
       "  <tf.Tensor: shape=(), dtype=float32, numpy=42.271774>,\n",
       "  <tf.Tensor: shape=(), dtype=float32, numpy=43.149937>,\n",
       "  <tf.Tensor: shape=(), dtype=float32, numpy=41.46418>,\n",
       "  <tf.Tensor: shape=(), dtype=float32, numpy=41.734154>,\n",
       "  <tf.Tensor: shape=(), dtype=float32, numpy=40.698933>,\n",
       "  <tf.Tensor: shape=(), dtype=float32, numpy=39.415>,\n",
       "  <tf.Tensor: shape=(), dtype=float32, numpy=39.511654>,\n",
       "  <tf.Tensor: shape=(), dtype=float32, numpy=39.443386>,\n",
       "  <tf.Tensor: shape=(), dtype=float32, numpy=38.574753>,\n",
       "  <tf.Tensor: shape=(), dtype=float32, numpy=39.15519>,\n",
       "  <tf.Tensor: shape=(), dtype=float32, numpy=37.983852>,\n",
       "  <tf.Tensor: shape=(), dtype=float32, numpy=37.228886>,\n",
       "  <tf.Tensor: shape=(), dtype=float32, numpy=36.85762>,\n",
       "  <tf.Tensor: shape=(), dtype=float32, numpy=36.85651>,\n",
       "  <tf.Tensor: shape=(), dtype=float32, numpy=35.326664>,\n",
       "  <tf.Tensor: shape=(), dtype=float32, numpy=35.249302>,\n",
       "  <tf.Tensor: shape=(), dtype=float32, numpy=35.13007>,\n",
       "  <tf.Tensor: shape=(), dtype=float32, numpy=34.72731>,\n",
       "  <tf.Tensor: shape=(), dtype=float32, numpy=34.780094>,\n",
       "  <tf.Tensor: shape=(), dtype=float32, numpy=33.676487>,\n",
       "  <tf.Tensor: shape=(), dtype=float32, numpy=33.89418>,\n",
       "  <tf.Tensor: shape=(), dtype=float32, numpy=33.476936>,\n",
       "  <tf.Tensor: shape=(), dtype=float32, numpy=32.359776>,\n",
       "  <tf.Tensor: shape=(), dtype=float32, numpy=32.50432>,\n",
       "  <tf.Tensor: shape=(), dtype=float32, numpy=32.33592>],\n",
       " <tf.Tensor: shape=(), dtype=float32, numpy=12.872842>)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fit_model(X_train_omics_labelled, train_set_labelled_y,150,X_valid_omics, valid_set_labelled_y,\n",
    "              10,variational_encoder=variational_encoder,variational_decoder=variational_decoder,\n",
    "             classifier=classifier,y_distribution=y_distribution,model=model,\n",
    "          Sampling=Sampling,y_dist=y_dist,batch_size=32,learning_rate=0.001,codings_size=208,valid_set=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test_nlog_lik = tf.Tensor(13.166075, shape=(), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "test_nlog_lik = -validation_log_lik_sampling(test_set_labelled_y,X_test_omics.to_numpy(),\n",
    "                                    variational_decoder=variational_decoder,codings_size=208,samples=2000)\n",
    "print(\"test_nlog_lik = \" + str(test_nlog_lik))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
