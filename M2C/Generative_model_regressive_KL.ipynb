{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "assert sys.version_info >= (3, 5)\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "assert tf.__version__ >= \"2.0\"\n",
    "import numpy as np\n",
    "import time\n",
    "K = keras.backend\n",
    "import pandas as pd\n",
    "import math\n",
    "import os\n",
    "from sklearn.metrics import r2_score\n",
    "from scipy.stats import uniform,randint,norm\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import scipy.stats\n",
    "from sklearn.preprocessing import MinMaxScaler \n",
    "\n",
    "tf.random.set_seed(42)\n",
    "np.random.seed(42)\n",
    "random_state=42"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_path = 'save_path'\n",
    "os.chdir(save_path)\n",
    "\n",
    "X_train_omics_unlabelled = pd.read_csv(\"X_train_omics_unlabelled.csv\",index_col=0)\n",
    "X_train_omics_labelled = pd.read_csv(\"X_train_omics_labelled.csv\",index_col=0)\n",
    "X_test_omics= pd.read_csv(\"X_test_omics.csv\",index_col=0)\n",
    "X_valid_omics= pd.read_csv(\"X_valid_omics.csv\",index_col=0)\n",
    "features = np.load(\"feature_selection.npy\",allow_pickle=True)\n",
    "\n",
    "train_set_labelled_y= pd.read_csv(\"train_set_labelled_y.csv\",index_col=0)\n",
    "test_set_labelled_y= pd.read_csv(\"test_set_labelled_y.csv\",index_col=0)\n",
    "valid_set_labelled_y= pd.read_csv(\"valid_set_labelled_y.csv\",index_col=0)\n",
    "\n",
    "X_train_omics_unlabelled = X_train_omics_unlabelled[features]\n",
    "X_train_omics_labelled = X_train_omics_labelled[features]\n",
    "X_test_omics = X_test_omics[features]\n",
    "X_valid_omics = X_valid_omics[features]\n",
    "\n",
    "train_set_labelled_c= pd.read_csv(\"train_set_labelled_c.csv\",index_col=0)\n",
    "train_set_unlabelled_c= pd.read_csv(\"train_set_unlabelled_c.csv\",index_col=0)\n",
    "test_set_labelled_c= pd.read_csv(\"test_set_labelled_c.csv\",index_col=0)\n",
    "valid_set_labelled_c= pd.read_csv(\"valid_set_labelled_c.csv\",index_col=0)\n",
    "\n",
    "\n",
    "train_set_labelled_c = train_set_labelled_c[[\"age\",\"male\"]]\n",
    "train_set_unlabelled_c = train_set_unlabelled_c[[\"age\",\"male\"]]\n",
    "test_set_labelled_c = test_set_labelled_c[[\"age\",\"male\"]]\n",
    "valid_set_labelled_c = valid_set_labelled_c[[\"age\",\"male\"]]\n",
    "\n",
    "scaler = MinMaxScaler()\n",
    "train_set_labelled_y = scaler.fit_transform(train_set_labelled_y)\n",
    "valid_set_labelled_y = scaler.transform(valid_set_labelled_y)\n",
    "test_set_labelled_y = scaler.transform(test_set_labelled_y)\n",
    "\n",
    "valid_set_labelled_y[np.where(valid_set_labelled_y >1)] = 1\n",
    "test_set_labelled_y[np.where(test_set_labelled_y >1)] = 1\n",
    "\n",
    "\n",
    "train_set_labelled_c[\"age\"] = scaler.fit_transform(train_set_labelled_c[[\"age\"]])\n",
    "train_set_unlabelled_c[\"age\"] = scaler.transform(train_set_unlabelled_c[[\"age\"]])\n",
    "test_set_labelled_c[\"age\"] = scaler.transform(test_set_labelled_c[[\"age\"]])\n",
    "valid_set_labelled_c[\"age\"] = scaler.transform(valid_set_labelled_c[[\"age\"]])\n",
    "\n",
    "train_set_labelled_c[\"age\"] = scaler.fit_transform(train_set_labelled_c[[\"age\"]])\n",
    "train_set_unlabelled_c[\"age\"] = scaler.transform(train_set_unlabelled_c[[\"age\"]])\n",
    "test_set_labelled_c[\"age\"] = scaler.transform(test_set_labelled_c[[\"age\"]])\n",
    "valid_set_labelled_c[\"age\"] = scaler.transform(valid_set_labelled_c[[\"age\"]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_model_path = 'model_path'\n",
    "os.chdir(save_model_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_shape = X_train_omics_labelled.shape[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Custom parts # "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Helpful functions ##"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def validation_log_lik_sampling(y_val,x_val,c_val,variational_decoder,codings_size,samples=200):\n",
    "\n",
    "    \"\"\"\n",
    "    Samples a value of z for the expectation, and calculates something proportional to loglikelihood.\n",
    "    \n",
    "    The more samples of z, the better the MC approximation to loglik, but the longer it takes to compute.\n",
    "    \n",
    "    This is how we do our evaluation on the validation and also test set. \n",
    "    \n",
    "    We look at the ability to generate x given y i.e. loglik(x|y,c)\"\"\"\n",
    "    \n",
    "    x_val_len = len(x_val)\n",
    "    expectation = 0\n",
    "    for i in range(samples):\n",
    "        z = np.random.normal(loc=0,scale=1,size=codings_size*x_val_len).reshape(x_val_len,codings_size)\n",
    "        x_pred = variational_decoder([z,y_val,c_val])\n",
    "        diff = (x_val-x_pred)**2\n",
    "        pdf = K.sum(diff,axis=-1)\n",
    "        pdf = K.exp(-pdf)\n",
    "        expectation += pdf \n",
    "    expectation = expectation / samples\n",
    "    lik = tf.math.log(expectation)\n",
    "    lik = K.mean(lik)    \n",
    "    return lik\n",
    "\n",
    "def create_batch(x_label, y_label, x_unlabel, c_label,c_unlabel, batch_s=32):\n",
    "    '''\n",
    "    Creates batches of labelled and unlabelled data. The total number of points in both batches is equal to batch_s. \n",
    "    Thanks to Omer Nivron for help with this.\n",
    "    \n",
    "    '''\n",
    "    proportion_labelled = x_label.shape[0]/(x_label.shape[0] + x_unlabel.shape[0])\n",
    "    \n",
    "    shape_label = x_label.shape[0]\n",
    "    label_per_batch = int(np.ceil(proportion_labelled*batch_s))\n",
    "    batch_idx_la = np.random.choice(list(range(shape_label)), label_per_batch)\n",
    "    batch_x_la = (x_label.iloc[batch_idx_la, :])\n",
    "    batch_y_la = (y_label[batch_idx_la,:])\n",
    "    batch_c_la = (c_label.iloc[batch_idx_la,:])\n",
    "\n",
    "    \n",
    "    shape_unlabel = x_unlabel.shape[0]\n",
    "    unlabel_per_batch = batch_s - label_per_batch\n",
    "    batch_idx_un = np.random.choice(list(range(shape_unlabel)), unlabel_per_batch)\n",
    "    batch_x_un = (x_unlabel.iloc[batch_idx_un, :])\n",
    "    batch_c_un = (c_unlabel.iloc[batch_idx_un,:])\n",
    "\n",
    "    \n",
    "    del batch_idx_la,batch_idx_un\n",
    "            \n",
    "    return batch_x_la, batch_y_la,batch_x_un,batch_c_la,batch_c_un\n",
    "\n",
    "\n",
    "def progress_bar(iteration, total, size=30):\n",
    "    \"\"\"Progress bar for training\"\"\"\n",
    "    running = iteration < total\n",
    "    c = \">\" if running else \"=\"\n",
    "    p = (size - 1) * iteration // total\n",
    "    fmt = \"{{:-{}d}}/{{}} [{{}}]\".format(len(str(total)))\n",
    "    params = [iteration, total, \"=\" * p + c + \".\" * (size - p - 1)]\n",
    "    return fmt.format(*params)\n",
    "\n",
    "def print_status_bar(iteration, total, loss, metrics=None, size=30):\n",
    "    \"\"\"Status bar for training\"\"\"\n",
    "    metrics = \" - \".join([\"Loss for batch: {:.4f}\".format(loss)])\n",
    "    end = \"\" if iteration < total else \"\\n\"\n",
    "    print(\"\\r{} - {}\".format(progress_bar(iteration, total), metrics), end=end)\n",
    "    \n",
    "def print_status_bar_epoch(iteration, total, training_loss_for_epoch,val_loss, metrics=None, size=30):\n",
    "    \"\"\"Status bar for training (end of epoch)\"\"\"\n",
    "    metrics = \" - \".join(\n",
    "        [\"trainLoss: {:.4f}  Val_loss: {:.4f} \".format(\n",
    "            training_loss_for_epoch,val_loss)]\n",
    "    )\n",
    "    end = \"\" if iteration < total else \"\\n\"\n",
    "    print(\"\\r{} - {}\".format(progress_bar(iteration, total), metrics), end=end)\n",
    "    \n",
    "    \n",
    "def list_average(list_of_loss):\n",
    "    return sum(list_of_loss)/len(list_of_loss)\n",
    " \n",
    "\n",
    "def gaussian_pdf(array,mean,sigma):\n",
    "    part1 = tf.math.divide(tf.constant(np.array(1.0).reshape(1,-1),dtype=\"float32\"),sigma*(2*math.pi)**0.5)\n",
    "    part2 = K.exp(-0.5*tf.math.divide((array-mean),sigma)**2)\n",
    "    return part1*part2\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model ##"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Custom components ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Sampling(keras.layers.Layer):\n",
    "    \"\"\"reparameterization trick\"\"\"\n",
    "    def call(self, inputs):\n",
    "        mean, log_var = inputs\n",
    "        return K.random_normal(tf.shape(log_var)) * K.exp(log_var/2) + mean  \n",
    "    \n",
    "    \n",
    "class y_dist(keras.layers.Layer):\n",
    "\n",
    "    \"\"\"\n",
    "    Custom layer that is used to learn the parameters of the distribution over y.\n",
    "    \n",
    "    Outputs a loss. The loss is used for training. The loss is a GMM loss.\n",
    "    The mean of this is then taken to provide a per batch loss. \n",
    "        \n",
    "    \"\"\"\n",
    "    def __init__(self,**kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        \n",
    "        \n",
    "    def build(self,batch_input_shape):\n",
    "        self.q1 = self.add_weight(name=\"q1\",shape=[1,1],initializer=\"random_normal\",trainable=True)\n",
    "        self.q2 = self.add_weight(name=\"q2\",shape=[1,1],initializer=\"random_normal\",trainable=True)\n",
    "        self.mu1 = self.add_weight(name=\"mu1\",shape=[1,1],initializer=\"random_normal\",trainable=True)\n",
    "        self.mu2 = self.add_weight(name=\"mu2\",shape=[1,1],initializer=\"random_normal\",trainable=True)\n",
    "        self.mu3 = self.add_weight(name=\"mu3\",shape=[1,1],initializer=\"random_normal\",trainable=True)\n",
    "        self.tau1 = self.add_weight(name=\"tau1\",shape=[1,1],initializer=\"random_normal\",trainable=True)\n",
    "        self.tau2 = self.add_weight(name=\"tau2\",shape=[1,1],initializer=\"random_normal\",trainable=True)\n",
    "        self.tau3 = self.add_weight(name=\"tau3\",shape=[1,1],initializer=\"random_normal\",trainable=True)\n",
    "\n",
    "        super().build(batch_input_shape)\n",
    "    \n",
    "    def call(self,X):\n",
    "        concatenated = tf.concat([self.q1,self.q2,tf.constant(np.array(0.0).reshape(1,-1),dtype=\"float32\")],axis=-1)\n",
    "        p = K.exp(concatenated)\n",
    "        p = tf.math.divide(p,K.sum(p))\n",
    "        sigma_concatenated = tf.concat([self.tau1,self.tau2,self.tau3],axis=-1)\n",
    "        sigma = K.exp(sigma_concatenated)\n",
    "        likelihood = p[0][0]*gaussian_pdf(X,mean=self.mu1,sigma=sigma[0][0])+p[0][1]*gaussian_pdf(X,mean=self.mu2,sigma=sigma[0][1])+p[0][2]*gaussian_pdf(X,mean=self.mu3,sigma=sigma[0][2]) \n",
    "        loglik = tf.math.log(likelihood)\n",
    "        loss = -loglik\n",
    "        loss = tf.reduce_mean(loss)\n",
    "        return loss\n",
    "    \n",
    "    def compute_output_shape(self,batch_input_shape):\n",
    "        return tf.TensorShape(1)    \n",
    "    \n",
    "    \n",
    "class FullModel(keras.models.Model):\n",
    "    \"\"\"\n",
    "    This is the full model. For KL. This is used for training purposes.\n",
    "    \n",
    "    It requires an encoder, decoder, classifier and y_distribution model to be already defined (as can be done with \n",
    "    the build_model function).\n",
    "    \n",
    "    It returns the nloglik i.e. the loss. \n",
    "    \n",
    "    This loss can then be used in gradient descent and be minimised wrt parameters (of the four component models).\n",
    "    \n",
    "    At test time, you will call which of the component models you want to use (as opposed to trying to \"call\" this \n",
    "    FullModel which you won't want to do as its purpose is just to calculate the nloglik for training).\n",
    "    \n",
    "    \"\"\"\n",
    "    def __init__(self,N_parameter,beta,variational_encoder,variational_decoder,classifier,y_distribution,\n",
    "                 **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.encoder = variational_encoder\n",
    "        self.decoder = variational_decoder\n",
    "        self.classifier = classifier  \n",
    "        self.y_distribution = y_distribution\n",
    "        self.N = N_parameter\n",
    "        self.beta = beta\n",
    "    def call(self,inputs):\n",
    "        \"\"\"Inputs is a list, as such:\n",
    "            inputs[0] is labelled X \n",
    "            inputs[1] is labelled y \n",
    "            inputs[2] is unlabelled X\n",
    "            inputs[3] is labelled c\n",
    "            inputs[4] is unlabelled c\"\"\"\n",
    "        \n",
    "        X_labelled = inputs[0]\n",
    "        y_labelled = inputs[1]\n",
    "        X_unlabelled = inputs[2]\n",
    "        c_labelled = inputs[3]\n",
    "        c_unlabelled = inputs[4]\n",
    "        \n",
    "        ############### LABELLED CASE #################\n",
    "        \n",
    "        codings_mean,codings_log_var,codings = self.encoder([X_labelled,y_labelled,c_labelled])\n",
    "        y_pred_mean,y_pred_log_var,y_pred = self.classifier([X_labelled,c_labelled])\n",
    "        reconstructions = self.decoder([codings,y_labelled,c_labelled])\n",
    "\n",
    "        #LOSSES#\n",
    "        recon_loss = labelled_loss_reconstruction(codings_log_var=codings_log_var,x=X_labelled,x_decoded_mean=reconstructions,\n",
    "                                                  codings_mean=codings_mean,beta=self.beta)\n",
    "        cls_loss = labelled_cls_loss(y=y_labelled,y_pred_mean=y_pred_mean,y_pred_log_var=y_pred_log_var,N=self.N)\n",
    "        y_dist_loss1 = self.y_distribution(y_labelled)\n",
    "        labelled_loss = recon_loss + cls_loss + y_dist_loss1\n",
    "\n",
    "        ############### UNLABELLED CASE #################\n",
    "        y_pred_mean_unlabel,y_pred_log_var_unlabel,y_pred_unlabel = self.classifier([X_unlabelled,c_unlabelled])\n",
    "        codings_mean,codings_log_var,codings = self.encoder([X_unlabelled,y_pred_unlabel,c_unlabelled])\n",
    "        reconstructions_un = self.decoder([codings,y_pred_unlabel,c_unlabelled])\n",
    "        \n",
    "        #LOSSES#                                  \n",
    " \n",
    "        \n",
    "        unlabelled_recon_loss = unlabelled_loss_reconstruction(codings_log_var=codings_log_var,codings_mean=codings_mean,\n",
    "                                    y_pred_log_var=y_pred_log_var_unlabel,\n",
    "                                    y_pred_mean= y_pred_mean_unlabel,y_upper_bound=2,y_lower_bound= -1,                         \n",
    "                                    x=X_unlabelled, x_decoded_mean=reconstructions_un,beta=self.beta,\n",
    "                                    )\n",
    "\n",
    "        y_dist_loss = self.y_distribution(y_pred_unlabel)\n",
    "        unlabelled_loss = unlabelled_recon_loss + y_dist_loss\n",
    "        \n",
    "        ############### ALL LOSSES #######################\n",
    "        \n",
    "        loss = labelled_loss + unlabelled_loss\n",
    "        return loss  \n",
    "\n",
    "    \n",
    "\n",
    "def build_model(n_hidden=1, n_neurons=723,input_shape=input_shape,beta=1,n_hidden_classifier=1,\n",
    "              n_neurons_classifier=300,N=30,codings_size=50):\n",
    "    \n",
    "    \"\"\"\n",
    "    Builds deep generative model.\n",
    "    \n",
    "    Parameters specify the architecture. Architecture is such that encoder and decoder have same number of nodes and hidden\n",
    "    layers. Done for simplicity. Classifier has its own architecture.\n",
    "    \n",
    "    Returns encoder,decoder,y_distribution, classifier and overall model. These can be used downstream.\n",
    "    \n",
    "    e.g. variational_encoder,variational_decoder,classifier,y_distribution,model = build_model_mmd(n_hidden=1, n_neurons=723,input_shape=input_shape,beta=1,n_hidden_classifier=1,\n",
    "              n_neurons_classifier=300,N=30,codings_size=50)\n",
    "    \"\"\"\n",
    "       \n",
    "    ########## ENCODER ###############\n",
    "    \n",
    "    x_in = keras.layers.Input(shape=[input_shape])\n",
    "    y_in = keras.layers.Input(shape=[1])\n",
    "    c_in = keras.layers.Input(shape=[2])\n",
    "    z = keras.layers.concatenate([x_in,y_in,c_in])\n",
    "    for layer in range(n_hidden):\n",
    "        z = keras.layers.Dense(n_neurons,activation=\"elu\",kernel_initializer=\"he_normal\")(z)\n",
    "        z = keras.layers.Dropout(0.3)(z)\n",
    "\n",
    "    codings_mean = keras.layers.Dense(codings_size)(z)\n",
    "    codings_log_var = keras.layers.Dense(codings_size)(z)\n",
    "    codings = Sampling()([codings_mean, codings_log_var])\n",
    "    variational_encoder = keras.models.Model(\n",
    "        inputs=[x_in,y_in,c_in], outputs=[codings_mean, codings_log_var, codings])\n",
    "    \n",
    "    \n",
    "    ########## DECODER ###############\n",
    "\n",
    "    latent = keras.layers.Input(shape=[codings_size])\n",
    "    l_merged = keras.layers.concatenate([latent,y_in,c_in])\n",
    "    x = l_merged\n",
    "    for layer in range(n_hidden):\n",
    "        x = keras.layers.Dense(n_neurons, activation=\"elu\",kernel_initializer=\"he_normal\")(x)\n",
    "        x = keras.layers.Dropout(0.3)(x)\n",
    "    x_out = keras.layers.Dense(input_shape,activation=\"sigmoid\")(x) \n",
    "    variational_decoder = keras.models.Model(inputs=[latent,y_in,c_in], outputs=[x_out])\n",
    "    \n",
    "    \n",
    "    ########### CLASSIFIER ############\n",
    "    \n",
    "    y_classifier = keras.layers.concatenate([x_in,c_in])\n",
    "    for layer in range(n_hidden_classifier):\n",
    "        y_classifier = keras.layers.Dense(n_neurons_classifier, activation=\"elu\",kernel_initializer=\"he_normal\")(y_classifier)\n",
    "        y_classifier = keras.layers.Dropout(rate=0.3)(y_classifier)\n",
    "        \n",
    "    y_pred_mean = keras.layers.Dense(1)(y_classifier) \n",
    "    y_pred_log_var = keras.layers.Dense(1)(y_classifier) \n",
    "    y_pred = Sampling()([y_pred_mean, y_pred_log_var])\n",
    "\n",
    "    classifier = keras.models.Model(inputs=[x_in,c_in], outputs=[y_pred_mean,y_pred_log_var,y_pred])\n",
    "    \n",
    "    \n",
    "    ############ Y DISTRIBUTION #############\n",
    "    \n",
    "    loss = y_dist()(y_in)\n",
    "    y_distribution = keras.models.Model(inputs=[y_in],outputs=[loss])\n",
    "    \n",
    "    \n",
    "    ########## FULL MODEL #############\n",
    "    \n",
    "    model = FullModel(N_parameter=N,beta=beta,variational_encoder=variational_encoder,\n",
    "                  variational_decoder=variational_decoder,classifier=classifier,y_distribution=y_distribution\n",
    "                     )\n",
    "    \n",
    "    return variational_encoder,variational_decoder,classifier,y_distribution,model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loss functions ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def custom_mse(x,x_decoded_mean):\n",
    "    \"\"\"returns column of squared errors. Length of column is number of samples.\"\"\"\n",
    "    diff = (x-x_decoded_mean)**2\n",
    "    return K.sum(diff,axis=-1) /2 \n",
    "\n",
    "def labelled_loss_reconstruction(codings_log_var,codings_mean,x, x_decoded_mean,beta=1):\n",
    "    \"\"\"Labelled data. This is the labelled loss.\"\"\"\n",
    "    recon_loss = custom_mse(x, x_decoded_mean)\n",
    "    kl_loss = - 0.5 * K.sum(1 + codings_log_var - K.square(codings_mean) - K.exp(codings_log_var), axis=-1)\n",
    "    #kl loss gives vector of one value for each sample in batch \n",
    "    return K.mean(recon_loss + beta*kl_loss)\n",
    "\n",
    "def unlabelled_loss_reconstruction(x,x_decoded_mean,codings_log_var,codings_mean,y_pred_log_var,y_pred_mean,\n",
    "                                   y_upper_bound=2,y_lower_bound=-1,beta=1):\n",
    "    \"\"\"Unlabelled data. This is the reconstruction loss for the unlabelled portion. \n",
    "        The extra loss, dubbed the integral loss, is also specified here. The definition of this function follow below.\"\"\"\n",
    "    \n",
    "    recon_loss = custom_mse(x,x_decoded_mean)\n",
    "    kl_loss = - 0.5 * K.sum(1 + codings_log_var - K.square(codings_mean) - K.exp(codings_log_var), axis=-1)\n",
    "    #now need to do the expectation with respect to y. Let's just use montecarlo with n=1 for now. Just like we do\n",
    "    #for the expecatation with respect to z\n",
    "    loss = K.mean(beta*kl_loss + recon_loss)\n",
    "\n",
    "    integral_loss = unlabelled_integral_loss(y_pred_log_var,y_pred_mean,y_upper_bound,y_lower_bound) \n",
    "    #returns vector of one value per sample in batch\n",
    "\n",
    "    return K.mean(loss) + K.mean(integral_loss)\n",
    "    \n",
    "\n",
    "def unlabelled_integral_loss(y_pred_log_var,y_pred_mean,y_upper_bound=2, y_lower_bound=-1):\n",
    "    \"\"\"Unlabelled data. This is the loss resulting from the integral that arises in the loss function. I have derived\n",
    "    it in my notes.\"\"\"\n",
    "    sigma = K.exp(y_pred_log_var/2)\n",
    "    term1 = 0.5* tf.math.log(1/(2**0.5 * (math.pi)**0.5*sigma)) * tf.math.erf((y_upper_bound-y_pred_mean)/(2**0.5 *sigma))\n",
    "    term2 = -0.25*tf.math.erf((y_upper_bound-y_pred_mean)/(2**0.5 *sigma))\n",
    "    term3 = (y_upper_bound-y_pred_mean)*K.exp(-((y_upper_bound-y_pred_mean)**2/(2*sigma**2)))/(2**1.5 *math.pi**0.5 *sigma)\n",
    "    part1 = term1 + term2 + term3\n",
    "    \n",
    "    term1 = 0.5* tf.math.log(1/(2**0.5 * (math.pi)**0.5*sigma)) * tf.math.erf((y_lower_bound-y_pred_mean)/(2**0.5 *sigma))\n",
    "    term2 = -0.25*tf.math.erf((y_lower_bound-y_pred_mean)/(2**0.5 *sigma))\n",
    "    term3 = (y_lower_bound-y_pred_mean)*K.exp(-((y_lower_bound-y_pred_mean)**2/(2*sigma**2)))/(2**1.5 *math.pi**0.5 *sigma)\n",
    "    part2 = term1 + term2 + term3\n",
    "    \n",
    "    integral = part1-part2\n",
    "    loss = integral\n",
    "    return loss\n",
    "\n",
    "def labelled_cls_loss(y,y_pred_mean,y_pred_log_var,N=383): \n",
    "    \"\"\"Labelled data only. This is the loss function for the part concerning learning the distibution q(y|x). \n",
    "        This loss function depends on the mean AND the sigma, hence both are included. This can be derived from\n",
    "        taking the log of the normal distribution pdf.\n",
    "        \n",
    "        N is the number of labelled data points in the training set. It is used with alpha to weight this loss term, so\n",
    "        that the model learns q(y|x) using the labelled training data. \n",
    "    \"\"\"\n",
    "    alpha=0.1*N\n",
    "    sigma = K.exp(y_pred_log_var/2)\n",
    "    diff = (y-y_pred_mean)**2\n",
    "    regression_loss = tf.math.divide(diff,(2*sigma**2))\n",
    "    loss = regression_loss + 0.5*y_pred_log_var\n",
    "    return alpha*K.mean(loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training functions ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "@tf.function\n",
    "def train_step(inputs):\n",
    "    \"\"\"Decorated train_step function which applies a gradient update to the parameters\"\"\"\n",
    "    with tf.GradientTape() as tape:\n",
    "        loss = model(inputs,training=True)\n",
    "        loss = tf.add_n([loss] + model.losses)\n",
    "    gradients = tape.gradient(loss, model.trainable_variables)\n",
    "    optimizer.apply_gradients(zip(gradients, model.trainable_variables))\n",
    "    return loss\n",
    "\n",
    "def fit_model(X_train_la, y_train_la, X_train_un,c_train_la,c_train_un,epochs,X_valid_la, y_valid_la,c_valid_la,\n",
    "              patience,variational_encoder,variational_decoder,\n",
    "             classifier,y_distribution,model,Sampling=Sampling,y_dist=y_dist,batch_size=32,learning_rate=0.001,codings_size=50,\n",
    "             valid_set=True):\n",
    "\n",
    "    \"\"\"\n",
    "    Fits a single model. Gets the validation loss too if valid set exists. \n",
    "    And includes a version of early stopping, given by the patience.\n",
    "    Progress bars are shown too.\n",
    "    Number of epochs are specified by the parameter epochs.\n",
    "    \n",
    "    Need to pass in all the custom components. Maybe could put them in a dictionary for cleanliness.\n",
    "    \n",
    "    Valid set is True or False depending if you have one. If you don't, the model at the end of training is saved.\n",
    "    You must still pass in dummy valid sets even if valid_set=False.\n",
    "    \n",
    "    Returns list of training loss, and the minimum validation loss. It also saves the best encoder, decoder and\n",
    "    regressor so they can be used. \n",
    "    \n",
    "    e.g. usage fit_model(X_train_omics_labelled, train_set_labelled_y, X_train_omics_unlabelled,50,X_valid_omics, valid_set_labelled_y,\n",
    "              10,variational_encoder=variational_encoder,variational_decoder=variational_decoder,\n",
    "             classifier=classifier,y_distribution=y_distribution,model=model,\n",
    "          Sampling=Sampling,y_dist=y_dist,batch_size=32,learning_rate=0.001,codings_size=50,valid_set=True)\n",
    "    \"\"\"\n",
    "    if valid_set is True:\n",
    "    \n",
    "        start = time.time()\n",
    "        history = []\n",
    "        K.clear_session()\n",
    "\n",
    "        @tf.function\n",
    "        def train_step(inputs):\n",
    "            with tf.GradientTape() as tape:\n",
    "                loss = model(inputs,training=True)\n",
    "                loss = tf.add_n([loss] + model.losses)\n",
    "            gradients = tape.gradient(loss, model.trainable_variables)\n",
    "            optimizer.apply_gradients(zip(gradients, model.trainable_variables))\n",
    "            return loss\n",
    "\n",
    "        validation_loss = []\n",
    "        optimizer=keras.optimizers.Adam(learning_rate=learning_rate)\n",
    "        batch_loss = []\n",
    "        batches_per_epoch = int(np.floor((X_train_la.shape[0] + X_train_un.shape[0])/batch_size))\n",
    "\n",
    "        for epoch in range(epochs):\n",
    "\n",
    "                print(\"Epoch {}/{}\".format(epoch,epochs))\n",
    "\n",
    "                for i in range(batches_per_epoch):\n",
    "\n",
    "                    batch_x_la, batch_y_la, batch_x_un,batch_c_la,batch_c_un= create_batch(\n",
    "                        X_train_la, y_train_la, X_train_un,c_train_la,c_train_un,batch_size)\n",
    "\n",
    "                    inputs = [batch_x_la.to_numpy(),batch_y_la,batch_x_un.to_numpy(),batch_c_la.to_numpy(),\n",
    "                             batch_c_un.to_numpy()]\n",
    "                    loss = train_step(inputs)\n",
    "                    batch_loss.append(loss)\n",
    "                    average_batch_loss = list_average(batch_loss)\n",
    "                    print_status_bar(i*batch_size,X_train_la.shape[0] + X_train_un.shape[0],average_batch_loss)\n",
    "\n",
    "                training_loss_for_epoch = list_average(batch_loss)\n",
    "                batch_loss = []\n",
    "                history.append(training_loss_for_epoch)\n",
    "                val_loss = -validation_log_lik_sampling(y_valid_la,X_valid_la.to_numpy(),c_valid_la.to_numpy(),\n",
    "                                                        variational_decoder=variational_decoder,codings_size=codings_size)\n",
    "\n",
    "                validation_loss.append(val_loss)\n",
    "                print_status_bar_epoch(X_train_la.shape[0] + X_train_un.shape[0]\n",
    "                                 ,(X_train_la.shape[0] + X_train_un.shape[0]),training_loss_for_epoch,val_loss )\n",
    "\n",
    "                #callback for early stopping\n",
    "                if epoch <= patience - 1:\n",
    "\n",
    "                    if epoch == 0:\n",
    "\n",
    "                        variational_encoder.save(\"variational_encoder.h5\")\n",
    "                        variational_decoder.save(\"variational_decoder.h5\")\n",
    "                        classifier.save(\"classifier.h5\")\n",
    "                        y_distribution.save(\"y_distribution.h5\")\n",
    "\n",
    "                    else:\n",
    "                        if all(val_loss<i for i in validation_loss[:-1]) is True:\n",
    "                            variational_encoder.save(\"variational_encoder.h5\")\n",
    "                            variational_decoder.save(\"variational_decoder.h5\")\n",
    "                            classifier.save(\"classifier.h5\")\n",
    "                            y_distribution.save(\"y_distribution.h5\")\n",
    "                #this statement means at least a model is saved. Because if the best model was before epoch > patience-1,\n",
    "                #then the statement below won't save any model, which is undesirable as we need to load a model. \n",
    "\n",
    "                if epoch > patience - 1:\n",
    "\n",
    "                    latest_val_loss = validation_loss[-patience:]\n",
    "                    if all(val_loss<i for i in latest_val_loss[:-2]) is True:\n",
    "                        variational_encoder.save(\"variational_encoder.h5\")\n",
    "                        variational_decoder.save(\"variational_decoder.h5\")\n",
    "                        classifier.save(\"classifier.h5\")\n",
    "                        y_distribution.save(\"y_distribution.h5\")\n",
    "                    if all(i>latest_val_loss[0] for i in latest_val_loss[1:]) is True:\n",
    "                        break     \n",
    "\n",
    "        #load best model#\n",
    "        variational_encoder = keras.models.load_model(\"variational_encoder.h5\", custom_objects={\n",
    "           \"Sampling\": Sampling\n",
    "        })\n",
    "        variational_decoder = keras.models.load_model(\"variational_decoder.h5\")\n",
    "        classifier = keras.models.load_model(\"classifier.h5\", custom_objects={\n",
    "           \"Sampling\": Sampling\n",
    "        })    \n",
    "        y_distribution = keras.models.load_model(\"y_distribution.h5\", custom_objects={\n",
    "           \"y_dist\": y_dist\n",
    "        })    \n",
    "\n",
    "        done = time.time()\n",
    "        elapsed = done-start\n",
    "        print(\"Elapsed/s: \",elapsed)\n",
    "        print(\"Final training loss: \",training_loss_for_epoch)\n",
    "        print(\"best val loss: \", min(validation_loss))\n",
    "        \n",
    "        return history, min(validation_loss)\n",
    "\n",
    "        \n",
    "    else:\n",
    "        \n",
    "        start = time.time()\n",
    "        history = []\n",
    "        K.clear_session()\n",
    "\n",
    "        @tf.function\n",
    "        def train_step(inputs):\n",
    "            with tf.GradientTape() as tape:\n",
    "                loss = model(inputs,training=True)\n",
    "                loss = tf.add_n([loss] + model.losses)\n",
    "            gradients = tape.gradient(loss, model.trainable_variables)\n",
    "            optimizer.apply_gradients(zip(gradients, model.trainable_variables))\n",
    "            return loss\n",
    "\n",
    "        optimizer=keras.optimizers.Adam(learning_rate=learning_rate)\n",
    "        batch_loss = []\n",
    "        batches_per_epoch = int(np.floor((X_train_la.shape[0] + X_train_un.shape[0])/batch_size))        \n",
    "        val_loss = 0\n",
    "\n",
    "        for epoch in range(epochs):\n",
    "\n",
    "                print(\"Epoch {}/{}\".format(epoch,epochs))\n",
    "                for i in range(batches_per_epoch):\n",
    "\n",
    "                    batch_x_la, batch_y_la, batch_x_un,batch_c_la,batch_c_un= create_batch(\n",
    "                        X_train_la, y_train_la, X_train_un,c_train_la,c_train_un,batch_size)\n",
    "\n",
    "                    inputs = [batch_x_la.to_numpy(),batch_y_la,batch_x_un.to_numpy(),batch_c_la.to_numpy(),\n",
    "                             batch_c_un.to_numpy()]\n",
    "                    loss = train_step(inputs)\n",
    "                    batch_loss.append(loss)\n",
    "                    average_batch_loss = list_average(batch_loss)\n",
    "                    print_status_bar(i*batch_size,X_train_la.shape[0] + X_train_un.shape[0],average_batch_loss)\n",
    "\n",
    "                training_loss_for_epoch = list_average(batch_loss)\n",
    "                batch_loss = []\n",
    "                history.append(training_loss_for_epoch)\n",
    "                print_status_bar_epoch(X_train_la.shape[0] + X_train_un.shape[0]\n",
    "                                 ,(X_train_la.shape[0] + X_train_un.shape[0]),training_loss_for_epoch,val_loss )\n",
    "        \n",
    "\n",
    "        variational_encoder.save(\"variational_encoder.h5\")\n",
    "        variational_decoder.save(\"variational_decoder.h5\")\n",
    "        classifier.save(\"classifier.h5\")\n",
    "        y_distribution.save(\"y_distribution.h5\")\n",
    "        \n",
    "        #load best model#\n",
    "        variational_encoder = keras.models.load_model(\"variational_encoder.h5\", custom_objects={\n",
    "           \"Sampling\": Sampling\n",
    "        })\n",
    "        variational_decoder = keras.models.load_model(\"variational_decoder.h5\")\n",
    "        classifier = keras.models.load_model(\"classifier.h5\", custom_objects={\n",
    "           \"Sampling\": Sampling\n",
    "        })     \n",
    "        y_distribution = keras.models.load_model(\"y_distribution.h5\", custom_objects={\n",
    "           \"y_dist\": y_dist\n",
    "        })    \n",
    "\n",
    "        done = time.time()\n",
    "        elapsed = done-start\n",
    "        print(\"Elapsed/s: \",elapsed)\n",
    "        print(\"Final training loss: \",training_loss_for_epoch)\n",
    "        \n",
    "    \n",
    "        return history\n",
    "\n",
    "\n",
    "def fit_model_search(X_train_la, y_train_la, X_train_un,c_train_la, c_train_un, epochs,X_valid_la, y_valid_la,c_valid_la,\n",
    "              patience,variational_encoder,variational_decoder,\n",
    "             classifier,y_distribution,model,Sampling=Sampling,y_dist=y_dist,batch_size=32,learning_rate=0.001,\n",
    "                    codings_size=50):\n",
    "\n",
    "    \"\"\"\n",
    "    Use for hyperparameter search. \n",
    "    \n",
    "    Fits the model. Gets the validation loss too. And includes a version of early stopping, given by the patience.\n",
    "    Progress bars are shown too.\n",
    "    Number of epochs are specified by the parameter epochs.\n",
    "    \n",
    "    Need to pass in all the custom components. Maybe could put them in a dictionary for cleanliness.\n",
    "    \n",
    "    Returns list of training loss, and the minimum validation loss. It also saves the best encoder, decoder and\n",
    "    regressor so they can be used. \n",
    "    \n",
    "    \"\"\"\n",
    "    \n",
    "    start = time.time()\n",
    "    history = []   \n",
    "       \n",
    "    @tf.function\n",
    "    def train_step(inputs):\n",
    "        with tf.GradientTape() as tape:\n",
    "            loss = model(inputs,training=True)\n",
    "            loss = tf.add_n([loss] + model.losses)\n",
    "        gradients = tape.gradient(loss, model.trainable_variables)\n",
    "        optimizer.apply_gradients(zip(gradients, model.trainable_variables))\n",
    "        return loss\n",
    "    \n",
    "    validation_loss = []\n",
    "    optimizer=keras.optimizers.Adam(learning_rate=learning_rate)\n",
    "    batch_loss = []    \n",
    "    batches_per_epoch = int(np.floor((X_train_la.shape[0] + X_train_un.shape[0])/batch_size))\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "            \n",
    "            print(\"Epoch {}/{}\".format(epoch,epochs))\n",
    "            \n",
    "            for i in range(batches_per_epoch):\n",
    "                \n",
    "                batch_x_la, batch_y_la, batch_x_un,batch_c_la,batch_c_un= create_batch(\n",
    "                    X_train_la, y_train_la, X_train_un,c_train_la,c_train_un,batch_size)\n",
    "\n",
    "                inputs = [batch_x_la.to_numpy(),batch_y_la,batch_x_un.to_numpy(),batch_c_la.to_numpy(),\n",
    "                         batch_c_un.to_numpy()]\n",
    "                loss = train_step(inputs)\n",
    "                batch_loss.append(loss)                \n",
    "                average_batch_loss = list_average(batch_loss)                \n",
    "                print_status_bar(i*batch_size,X_train_la.shape[0] + X_train_un.shape[0],average_batch_loss)\n",
    "            \n",
    "            training_loss_for_epoch = list_average(batch_loss)\n",
    "            batch_loss = []                \n",
    "            history.append(training_loss_for_epoch)            \n",
    "            val_loss = -validation_log_lik_sampling(y_valid_la,X_valid_la.to_numpy(),c_valid_la.to_numpy(),\n",
    "                                                    variational_decoder=variational_decoder,codings_size=codings_size)\n",
    "            \n",
    "            validation_loss.append(val_loss)            \n",
    "            print_status_bar_epoch(X_train_la.shape[0] + X_train_un.shape[0]\n",
    "                             ,(X_train_la.shape[0] + X_train_un.shape[0]),training_loss_for_epoch,val_loss )\n",
    "            \n",
    "            #callback for early stopping\n",
    "            \n",
    "            if epoch <= patience - 1:\n",
    "                \n",
    "                if epoch == 0:\n",
    "                \n",
    "                    variational_encoder.save(\"variational_encoder_intermediate.h5\")\n",
    "                    variational_decoder.save(\"variational_decoder_intermediate.h5\")\n",
    "                    classifier.save(\"classifier_intermediate.h5\")\n",
    "                    y_distribution.save(\"y_distribution_intermediate.h5\")\n",
    "                    \n",
    "                else:\n",
    "                    if all(val_loss<i for i in validation_loss[:-1]) is True:\n",
    "                        variational_encoder.save(\"variational_encoder_intermediate.h5\")\n",
    "                        variational_decoder.save(\"variational_decoder_intermediate.h5\")\n",
    "                        classifier.save(\"classifier_intermediate.h5\")\n",
    "                        y_distribution.save(\"y_distribution_intermediate.h5\")\n",
    "            #this statement means at least a model is saved. Because if the best model was before epoch > patience-1,\n",
    "            #then the statement below won't save any model, which is undesirable as we need to load a model. \n",
    "            \n",
    "            if epoch > patience - 1:\n",
    "                                \n",
    "                latest_val_loss = validation_loss[-patience:]\n",
    "                if all(val_loss<i for i in latest_val_loss[:-1]) is True:\n",
    "                    variational_encoder.save(\"variational_encoder_intermediate.h5\")\n",
    "                    variational_decoder.save(\"variational_decoder_intermediate.h5\")\n",
    "                    classifier.save(\"classifier_intermediate.h5\")\n",
    "                    y_distribution.save(\"y_distribution_intermediate.h5\")\n",
    "                if all(i>latest_val_loss[0] for i in latest_val_loss[1:]) is True:\n",
    "                    break     \n",
    "    \n",
    "    #load best model#\n",
    "    variational_encoder = keras.models.load_model(\"variational_encoder_intermediate.h5\", custom_objects={\n",
    "       \"Sampling\": Sampling\n",
    "    })\n",
    "    variational_decoder = keras.models.load_model(\"variational_decoder_intermediate.h5\")\n",
    "    classifier = keras.models.load_model(\"classifier_intermediate.h5\", custom_objects={\n",
    "           \"Sampling\": Sampling\n",
    "        })    \n",
    "    y_distribution = keras.models.load_model(\"y_distribution_intermediate.h5\", custom_objects={\n",
    "       \"y_dist\": y_dist\n",
    "    })    \n",
    "                \n",
    "    done = time.time()\n",
    "    elapsed = done-start\n",
    "    print(\"Elapsed/s: \",elapsed)\n",
    "    print(\"Final training loss: \",training_loss_for_epoch)\n",
    "    print(\"best val loss: \", min(validation_loss))\n",
    "    \n",
    "    return history, min(validation_loss)\n",
    "\n",
    "def hyperparameter_search(param_distribs,epochs,patience,n_iter,X_train_la=X_train_omics_labelled, \n",
    "                          y_train_la=train_set_labelled_y, X_train_un=X_train_omics_unlabelled,c_train_la=train_set_labelled_c,\n",
    "                              c_train_un=train_set_unlabelled_c,c_valid_la=valid_set_labelled_c,\n",
    "                          X_valid_la=X_valid_omics, y_valid_la=valid_set_labelled_y):\n",
    "    \n",
    "    \"\"\"\n",
    "    Performs hyperparameter, random search. Assesses performance by determining the score on the validation set. \n",
    "    \n",
    "    Saves best models (encoder, decoder and regressor) and returns these. These can then be used downstream.\n",
    "    \n",
    "    Also returns dictionary of the search results.\n",
    "    \n",
    "    Param_distribs of the form: \n",
    "            param_distribs = {\n",
    "            \"n_hidden\": [1],\n",
    "            \"n_hidden_classifier\": [1],\n",
    "            \"beta\": [1],\n",
    "            \"n_neurons\": randint.rvs(50,1000-49,size=20,random_state=random_state).tolist(),\n",
    "           \"n_neurons_classifier\": randint.rvs(49,1000-49,size=20,random_state=random_state).tolist(),\n",
    "            \"codings_size\": randint.rvs(50,290-50,size=30,random_state=random_state).tolist(),\n",
    "            \"N\" :randint.rvs().tolist(),\n",
    "            \"learning_rate\" : ....\n",
    "            #\"codings_size\": [50]}\n",
    "            \n",
    "    There must be a value for every parameter. If you know the value you want to use, set it in the param_distribs\n",
    "    dictionary.\n",
    "    \n",
    "    Patience must be less than the number of epochs.\n",
    "    \n",
    "    e.g. result,variational_encoder,variational_decoder,classifier,y_distribution =\n",
    "            hyperparameter_search_mmd(param_distribs,500,10,n_iter=10)\n",
    "\n",
    "            \n",
    "    \"\"\"\n",
    "\n",
    "    np.random.seed(42) #needs to be here so that everything that follows is consistent\n",
    "\n",
    "    min_val_loss = []\n",
    "    master = {}\n",
    "\n",
    "    for i in range(n_iter): \n",
    "        K.clear_session()\n",
    "        master[i] = {}\n",
    "        master[i][\"parameters\"] = {}\n",
    "        \n",
    "        N= np.random.choice(param_distribs[\"N\"])\n",
    "        learning_rate= np.random.choice(param_distribs[\"learning_rate\"])\n",
    "        beta= np.random.choice(param_distribs[\"beta\"])\n",
    "        n_neurons =np.random.choice(param_distribs[\"n_neurons\"]) \n",
    "        n_neurons_classifier =np.random.choice(param_distribs[\"n_neurons_classifier\"]) \n",
    "        n_hidden  =np.random.choice(param_distribs[\"n_hidden\"]) \n",
    "        n_hidden_classifier  =np.random.choice(param_distribs[\"n_hidden_classifier\"]) \n",
    "        codings_size =np.random.choice(param_distribs[\"codings_size\"]) \n",
    "       \n",
    "        master[i][\"parameters\"][\"N\"] = N\n",
    "        master[i][\"parameters\"][\"learning_rate\"] = learning_rate\n",
    "        master[i][\"parameters\"][\"beta\"] = beta\n",
    "        master[i][\"parameters\"][\"n_neurons\"] = n_neurons\n",
    "        master[i][\"parameters\"][\"n_neurons_classifier\"] = n_neurons_classifier\n",
    "        master[i][\"parameters\"][\"n_hidden\"] = n_hidden\n",
    "        master[i][\"parameters\"][\"n_hidden_classifier\"] = n_hidden_classifier\n",
    "        master[i][\"parameters\"][\"codings_size\"] = codings_size\n",
    "\n",
    "        \n",
    "        variational_encoder,variational_decoder,classifier,y_distribution,model = build_model(n_hidden=n_hidden,       \n",
    "                                       n_neurons=n_neurons,beta=beta,n_hidden_classifier=n_hidden_classifier,\n",
    "                                        n_neurons_classifier=n_neurons_classifier,N=N,codings_size=codings_size)\n",
    "        \n",
    "                \n",
    "        history,val_loss = fit_model_search(X_train_la=X_train_la, y_train_la=y_train_la, \n",
    "                                 X_train_un=X_train_un, epochs=epochs,X_valid_la=X_valid_la, \n",
    "                                 y_valid_la=y_valid_la,patience=patience,variational_encoder=variational_encoder,\n",
    "                                variational_decoder=variational_decoder, classifier=classifier,\n",
    "                                y_distribution=y_distribution,model=model,Sampling=Sampling,y_dist=y_dist,\n",
    "                                            batch_size=32,learning_rate=learning_rate,codings_size=codings_size,\n",
    "                                            \n",
    "                                c_train_la=c_train_la, c_train_un = c_train_un,c_valid_la=c_valid_la\n",
    "                                           )        \n",
    "\n",
    "        master[i][\"val_loss\"] = val_loss\n",
    "        min_val_loss.append(val_loss)\n",
    "\n",
    "        #If val loss is lowest, save this model. \n",
    "        if val_loss <=  min(min_val_loss):\n",
    "            os.rename(\"variational_encoder_intermediate.h5\",\"variational_encoder.h5\")\n",
    "            os.rename(\"variational_decoder_intermediate.h5\",\"variational_decoder.h5\")\n",
    "            os.rename(\"classifier_intermediate.h5\",\"classifier.h5\")\n",
    "            os.rename(\"y_distribution_intermediate.h5\",\"y_distribution.h5\")\n",
    "\n",
    "        print(master)\n",
    "            \n",
    "    #load best model#\n",
    "    variational_encoder = keras.models.load_model(\"variational_encoder.h5\", custom_objects={\n",
    "       \"Sampling\": Sampling\n",
    "    })\n",
    "    variational_decoder = keras.models.load_model(\"variational_decoder.h5\")\n",
    "    classifier = keras.models.load_model(\"classifier.h5\", custom_objects={\n",
    "           \"Sampling\": Sampling\n",
    "        }) \n",
    "    y_distribution = keras.models.load_model(\"y_distribution.h5\", custom_objects={\n",
    "       \"y_dist\": y_dist\n",
    "    })    \n",
    "\n",
    "    result = sorted(master.items(), key=lambda item: item[1][\"val_loss\"])\n",
    "    return result,variational_encoder,variational_decoder,classifier,y_distribution\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hyperparameter Search #"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "param_distribs = {\n",
    "            \"n_hidden\": [1,2],\n",
    "            \"n_hidden_classifier\": [1,2],\n",
    "            \"beta\": [1,10,15],\n",
    "    #\"n_neurons\": [300,500],\n",
    "   # \"n_neurons_classifier\": [50,100,150],\n",
    "            \"n_neurons\": randint.rvs(50,600-49,size=20,random_state=random_state).tolist(),\n",
    "           \"n_neurons_classifier\": randint.rvs(20,120-20,size=20,random_state=random_state).tolist(),\n",
    "            \"codings_size\": randint.rvs(20,290-20,size=30,random_state=random_state).tolist(),\n",
    "   # \"codings_size\": [20,50,70],\n",
    "            \"N\" :[0.1,1,10\n",
    "                 \n",
    "                 \n",
    "            ],\n",
    "            \"learning_rate\" : [0.001,0.0005],\n",
    "            #\"codings_size\": [120,60]\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0/80\n",
      "WARNING:tensorflow:Layer full_model is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because it's dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "2240/2278 [============================>.] - Loss for batch: 75.8509WARNING:tensorflow:Layer concatenate_1 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because it's dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "2278/2278 [==============================] - trainLoss: 75.8509  Val_loss: 17.0957 \n",
      "Epoch 1/80\n",
      "2278/2278 [==============================] - trainLoss: 30.9249  Val_loss: 15.3245 \n",
      "Epoch 2/80\n",
      "2278/2278 [==============================] - trainLoss: 22.3708  Val_loss: 14.4601 \n",
      "Epoch 3/80\n",
      "2278/2278 [==============================] - trainLoss: 17.8403  Val_loss: 13.9109 \n",
      "Epoch 4/80\n",
      "2278/2278 [==============================] - trainLoss: 14.9277  Val_loss: 13.7224 \n",
      "Epoch 5/80\n",
      "2278/2278 [==============================] - trainLoss: 12.2999  Val_loss: 13.4797 \n",
      "Epoch 6/80\n",
      "2278/2278 [==============================] - trainLoss: 10.8440  Val_loss: 13.3504 \n",
      "Epoch 7/80\n",
      "2278/2278 [==============================] - trainLoss: 9.5820  Val_loss: 13.3424 \n",
      "Epoch 8/80\n",
      "2278/2278 [==============================] - trainLoss: 8.6738  Val_loss: 13.1876 \n",
      "Epoch 9/80\n",
      "2278/2278 [==============================] - trainLoss: 7.9365  Val_loss: 13.1203 \n",
      "Epoch 10/80\n",
      "2278/2278 [==============================] - trainLoss: 7.2896  Val_loss: 13.0753 \n",
      "Epoch 11/80\n",
      "2278/2278 [==============================] - trainLoss: 6.7460  Val_loss: 13.0649 \n",
      "Epoch 12/80\n",
      "2278/2278 [==============================] - trainLoss: 6.3168  Val_loss: 13.0610 \n",
      "Epoch 13/80\n",
      "2278/2278 [==============================] - trainLoss: 6.0787  Val_loss: 12.8503 \n",
      "Epoch 14/80\n",
      "2278/2278 [==============================] - trainLoss: 5.6366  Val_loss: 12.9907 \n",
      "Epoch 15/80\n",
      "2278/2278 [==============================] - trainLoss: 5.2593  Val_loss: 13.1007 \n",
      "Epoch 16/80\n",
      "2278/2278 [==============================] - trainLoss: 5.1208  Val_loss: 12.8798 \n",
      "Epoch 17/80\n",
      "2278/2278 [==============================] - trainLoss: 5.0472  Val_loss: 12.7935 \n",
      "Epoch 18/80\n",
      "2278/2278 [==============================] - trainLoss: 4.7548  Val_loss: 12.9088 \n",
      "Epoch 19/80\n",
      "2278/2278 [==============================] - trainLoss: 4.4284  Val_loss: 12.9213 \n",
      "Epoch 20/80\n",
      "2278/2278 [==============================] - trainLoss: 4.2386  Val_loss: 12.9080 \n",
      "Epoch 21/80\n",
      "2278/2278 [==============================] - trainLoss: 4.1978  Val_loss: 13.0184 \n",
      "Epoch 22/80\n",
      "2278/2278 [==============================] - trainLoss: 4.0963  Val_loss: 13.0206 \n",
      "Epoch 23/80\n",
      "2278/2278 [==============================] - trainLoss: 3.8929  Val_loss: 12.9623 \n",
      "Epoch 24/80\n",
      "2278/2278 [==============================] - trainLoss: 3.7432  Val_loss: 12.8720 \n",
      "Epoch 25/80\n",
      "2278/2278 [==============================] - trainLoss: 3.6230  Val_loss: 13.0030 \n",
      "Epoch 26/80\n",
      "2278/2278 [==============================] - trainLoss: 3.7768  Val_loss: 12.9801 \n",
      "WARNING:tensorflow:No training configuration found in save file: the model was *not* compiled. Compile it manually.\n",
      "WARNING:tensorflow:No training configuration found in save file: the model was *not* compiled. Compile it manually.\n",
      "WARNING:tensorflow:No training configuration found in save file: the model was *not* compiled. Compile it manually.\n",
      "WARNING:tensorflow:No training configuration found in save file: the model was *not* compiled. Compile it manually.\n",
      "Elapsed/s:  39.46562671661377\n",
      "Final training loss:  tf.Tensor(3.7767768, shape=(), dtype=float32)\n",
      "best val loss:  tf.Tensor(12.793457, shape=(), dtype=float32)\n",
      "{0: {'val_loss': <tf.Tensor: shape=(), dtype=float32, numpy=12.793457>, 'parameters': {'learning_rate': 0.0005, 'beta': 1, 'codings_size': 169, 'n_neurons_classifier': 72, 'N': 10.0, 'n_hidden_classifier': 1, 'n_neurons': 137, 'n_hidden': 2}}}\n",
      "Epoch 0/80\n",
      "WARNING:tensorflow:Layer full_model is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because it's dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "2240/2278 [============================>.] - Loss for batch: nanWARNING:tensorflow:Layer concatenate_1 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because it's dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "2278/2278 [==============================] - trainLoss: nan  Val_loss: nan \n",
      "Epoch 1/80\n",
      "2278/2278 [==============================] - trainLoss: nan  Val_loss: nan \n",
      "Epoch 2/80\n",
      "2278/2278 [==============================] - trainLoss: nan  Val_loss: nan \n",
      "Epoch 3/80\n",
      "2278/2278 [==============================] - trainLoss: nan  Val_loss: nan \n",
      "Epoch 4/80\n",
      "2278/2278 [==============================] - trainLoss: nan  Val_loss: nan \n",
      "Epoch 5/80\n",
      "2278/2278 [==============================] - trainLoss: nan  Val_loss: nan \n",
      "Epoch 6/80\n",
      "2278/2278 [==============================] - trainLoss: nan  Val_loss: nan \n",
      "Epoch 7/80\n",
      "2278/2278 [==============================] - trainLoss: nan  Val_loss: nan \n",
      "Epoch 8/80\n",
      "2278/2278 [==============================] - trainLoss: nan  Val_loss: nan \n",
      "Epoch 9/80\n",
      "2278/2278 [==============================] - trainLoss: nan  Val_loss: nan \n",
      "Epoch 10/80\n",
      "2278/2278 [==============================] - trainLoss: nan  Val_loss: nan \n",
      "Epoch 11/80\n",
      "2278/2278 [==============================] - trainLoss: nan  Val_loss: nan \n",
      "Epoch 12/80\n",
      "2278/2278 [==============================] - trainLoss: nan  Val_loss: nan \n",
      "Epoch 13/80\n",
      "2278/2278 [==============================] - trainLoss: nan  Val_loss: nan \n",
      "Epoch 14/80\n",
      "2278/2278 [==============================] - trainLoss: nan  Val_loss: nan \n",
      "Epoch 15/80\n",
      "2278/2278 [==============================] - trainLoss: nan  Val_loss: nan \n",
      "Epoch 16/80\n",
      "2278/2278 [==============================] - trainLoss: nan  Val_loss: nan \n",
      "Epoch 17/80\n",
      "2278/2278 [==============================] - trainLoss: nan  Val_loss: nan \n",
      "Epoch 18/80\n",
      "2278/2278 [==============================] - trainLoss: nan  Val_loss: nan \n",
      "Epoch 19/80\n",
      "2278/2278 [==============================] - trainLoss: nan  Val_loss: nan \n",
      "Epoch 20/80\n",
      "2278/2278 [==============================] - trainLoss: nan  Val_loss: nan \n",
      "Epoch 21/80\n",
      "2278/2278 [==============================] - trainLoss: nan  Val_loss: nan \n",
      "Epoch 22/80\n",
      "2278/2278 [==============================] - trainLoss: nan  Val_loss: nan \n",
      "Epoch 23/80\n",
      "2278/2278 [==============================] - trainLoss: nan  Val_loss: nan \n",
      "Epoch 24/80\n",
      "2278/2278 [==============================] - trainLoss: nan  Val_loss: nan \n",
      "Epoch 25/80\n",
      "2278/2278 [==============================] - trainLoss: nan  Val_loss: nan \n",
      "Epoch 26/80\n",
      "2278/2278 [==============================] - trainLoss: nan  Val_loss: nan \n",
      "Epoch 27/80\n",
      "2278/2278 [==============================] - trainLoss: nan  Val_loss: nan \n",
      "Epoch 28/80\n",
      "2278/2278 [==============================] - trainLoss: nan  Val_loss: nan \n",
      "Epoch 29/80\n",
      "2278/2278 [==============================] - trainLoss: nan  Val_loss: nan \n",
      "Epoch 30/80\n",
      "2278/2278 [==============================] - trainLoss: nan  Val_loss: nan \n",
      "Epoch 31/80\n",
      "2278/2278 [==============================] - trainLoss: nan  Val_loss: nan \n",
      "Epoch 32/80\n",
      "2278/2278 [==============================] - trainLoss: nan  Val_loss: nan \n",
      "Epoch 33/80\n",
      "2278/2278 [==============================] - trainLoss: nan  Val_loss: nan \n",
      "Epoch 34/80\n",
      "2278/2278 [==============================] - trainLoss: nan  Val_loss: nan \n",
      "Epoch 35/80\n",
      "2278/2278 [==============================] - trainLoss: nan  Val_loss: nan \n",
      "Epoch 36/80\n",
      "2278/2278 [==============================] - trainLoss: nan  Val_loss: nan \n",
      "Epoch 37/80\n",
      "2278/2278 [==============================] - trainLoss: nan  Val_loss: nan \n",
      "Epoch 38/80\n",
      "2278/2278 [==============================] - trainLoss: nan  Val_loss: nan \n",
      "Epoch 39/80\n",
      "2278/2278 [==============================] - trainLoss: nan  Val_loss: nan \n",
      "Epoch 40/80\n",
      "2278/2278 [==============================] - trainLoss: nan  Val_loss: nan \n",
      "Epoch 41/80\n",
      "2278/2278 [==============================] - trainLoss: nan  Val_loss: nan \n",
      "Epoch 42/80\n",
      "2278/2278 [==============================] - trainLoss: nan  Val_loss: nan \n",
      "Epoch 43/80\n",
      "2278/2278 [==============================] - trainLoss: nan  Val_loss: nan \n",
      "Epoch 44/80\n",
      "2278/2278 [==============================] - trainLoss: nan  Val_loss: nan \n",
      "Epoch 45/80\n",
      "2278/2278 [==============================] - trainLoss: nan  Val_loss: nan \n",
      "Epoch 46/80\n",
      "2278/2278 [==============================] - trainLoss: nan  Val_loss: nan \n",
      "Epoch 47/80\n",
      "2278/2278 [==============================] - trainLoss: nan  Val_loss: nan \n",
      "Epoch 48/80\n",
      "2278/2278 [==============================] - trainLoss: nan  Val_loss: nan \n",
      "Epoch 49/80\n",
      "2278/2278 [==============================] - trainLoss: nan  Val_loss: nan \n",
      "Epoch 50/80\n",
      "2278/2278 [==============================] - trainLoss: nan  Val_loss: nan \n",
      "Epoch 51/80\n",
      "2278/2278 [==============================] - trainLoss: nan  Val_loss: nan \n",
      "Epoch 52/80\n",
      "2278/2278 [==============================] - trainLoss: nan  Val_loss: nan \n",
      "Epoch 53/80\n",
      "2278/2278 [==============================] - trainLoss: nan  Val_loss: nan \n",
      "Epoch 54/80\n",
      "2278/2278 [==============================] - trainLoss: nan  Val_loss: nan \n",
      "Epoch 55/80\n",
      "2278/2278 [==============================] - trainLoss: nan  Val_loss: nan \n",
      "Epoch 56/80\n",
      "2278/2278 [==============================] - trainLoss: nan  Val_loss: nan \n",
      "Epoch 57/80\n",
      "2278/2278 [==============================] - trainLoss: nan  Val_loss: nan \n",
      "Epoch 58/80\n",
      "2278/2278 [==============================] - trainLoss: nan  Val_loss: nan \n",
      "Epoch 59/80\n",
      "2278/2278 [==============================] - trainLoss: nan  Val_loss: nan \n",
      "Epoch 60/80\n",
      "2278/2278 [==============================] - trainLoss: nan  Val_loss: nan \n",
      "Epoch 61/80\n",
      "2278/2278 [==============================] - trainLoss: nan  Val_loss: nan \n",
      "Epoch 62/80\n",
      "2278/2278 [==============================] - trainLoss: nan  Val_loss: nan \n",
      "Epoch 63/80\n",
      "2278/2278 [==============================] - trainLoss: nan  Val_loss: nan \n",
      "Epoch 64/80\n",
      "2278/2278 [==============================] - trainLoss: nan  Val_loss: nan \n",
      "Epoch 65/80\n",
      "2278/2278 [==============================] - trainLoss: nan  Val_loss: nan \n",
      "Epoch 66/80\n",
      "2278/2278 [==============================] - trainLoss: nan  Val_loss: nan \n",
      "Epoch 67/80\n",
      "2278/2278 [==============================] - trainLoss: nan  Val_loss: nan \n",
      "Epoch 68/80\n",
      "2278/2278 [==============================] - trainLoss: nan  Val_loss: nan \n",
      "Epoch 69/80\n",
      "2278/2278 [==============================] - trainLoss: nan  Val_loss: nan \n",
      "Epoch 70/80\n",
      "2278/2278 [==============================] - trainLoss: nan  Val_loss: nan \n",
      "Epoch 71/80\n",
      "2278/2278 [==============================] - trainLoss: nan  Val_loss: nan \n",
      "Epoch 72/80\n",
      "2278/2278 [==============================] - trainLoss: nan  Val_loss: nan \n",
      "Epoch 73/80\n",
      "2278/2278 [==============================] - trainLoss: nan  Val_loss: nan \n",
      "Epoch 74/80\n",
      "2278/2278 [==============================] - trainLoss: nan  Val_loss: nan \n",
      "Epoch 75/80\n",
      "2278/2278 [==============================] - trainLoss: nan  Val_loss: nan \n",
      "Epoch 76/80\n",
      "2278/2278 [==============================] - trainLoss: nan  Val_loss: nan \n",
      "Epoch 77/80\n",
      "2278/2278 [==============================] - trainLoss: nan  Val_loss: nan \n",
      "Epoch 78/80\n",
      "2278/2278 [==============================] - trainLoss: nan  Val_loss: nan \n",
      "Epoch 79/80\n",
      "2278/2278 [==============================] - trainLoss: nan  Val_loss: nan \n",
      "WARNING:tensorflow:No training configuration found in save file: the model was *not* compiled. Compile it manually.\n",
      "WARNING:tensorflow:No training configuration found in save file: the model was *not* compiled. Compile it manually.\n",
      "WARNING:tensorflow:No training configuration found in save file: the model was *not* compiled. Compile it manually.\n",
      "WARNING:tensorflow:No training configuration found in save file: the model was *not* compiled. Compile it manually.\n",
      "Elapsed/s:  103.98434972763062\n",
      "Final training loss:  tf.Tensor(nan, shape=(), dtype=float32)\n",
      "best val loss:  tf.Tensor(nan, shape=(), dtype=float32)\n",
      "{0: {'val_loss': <tf.Tensor: shape=(), dtype=float32, numpy=12.793457>, 'parameters': {'learning_rate': 0.0005, 'beta': 1, 'codings_size': 169, 'n_neurons_classifier': 72, 'N': 10.0, 'n_hidden_classifier': 1, 'n_neurons': 137, 'n_hidden': 2}}, 1: {'val_loss': <tf.Tensor: shape=(), dtype=float32, numpy=nan>, 'parameters': {'learning_rate': 0.001, 'beta': 10, 'codings_size': 107, 'n_neurons_classifier': 22, 'N': 1.0, 'n_hidden_classifier': 2, 'n_neurons': 152, 'n_hidden': 2}}}\n",
      "Epoch 0/80\n",
      "WARNING:tensorflow:Layer full_model is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because it's dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "2240/2278 [============================>.] - Loss for batch: 46.0857WARNING:tensorflow:Layer concatenate_1 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because it's dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "2278/2278 [==============================] - trainLoss: 46.0857  Val_loss: 18.5956 \n",
      "Epoch 1/80\n",
      "2278/2278 [==============================] - trainLoss: 20.6682  Val_loss: 16.0113 \n",
      "Epoch 2/80\n",
      "2278/2278 [==============================] - trainLoss: 14.9502  Val_loss: 14.8944 \n",
      "Epoch 3/80\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2278/2278 [==============================] - trainLoss: 12.1314  Val_loss: 14.0312 \n",
      "Epoch 4/80\n",
      "2278/2278 [==============================] - trainLoss: 10.1310  Val_loss: 13.6579 \n",
      "Epoch 5/80\n",
      "2278/2278 [==============================] - trainLoss: 8.9583  Val_loss: 13.4673 \n",
      "Epoch 6/80\n",
      "2278/2278 [==============================] - trainLoss: 8.0147  Val_loss: 13.1984 \n",
      "Epoch 7/80\n",
      "2278/2278 [==============================] - trainLoss: 7.1472  Val_loss: 13.2864 \n",
      "Epoch 8/80\n",
      "2278/2278 [==============================] - trainLoss: 6.4436  Val_loss: 13.1683 \n",
      "Epoch 9/80\n",
      "2278/2278 [==============================] - trainLoss: 6.0111  Val_loss: 12.9958 \n",
      "Epoch 10/80\n",
      "2278/2278 [==============================] - trainLoss: 5.4482  Val_loss: 13.0205 \n",
      "Epoch 11/80\n",
      "2278/2278 [==============================] - trainLoss: 5.0670  Val_loss: 13.0146 \n",
      "Epoch 12/80\n",
      "2278/2278 [==============================] - trainLoss: 4.8046  Val_loss: 12.9494 \n",
      "Epoch 13/80\n",
      "2278/2278 [==============================] - trainLoss: 4.7060  Val_loss: 12.9200 \n",
      "Epoch 14/80\n",
      "2278/2278 [==============================] - trainLoss: 4.3756  Val_loss: 12.8651 \n",
      "Epoch 15/80\n",
      "2278/2278 [==============================] - trainLoss: 4.2510  Val_loss: 12.6832 \n",
      "Epoch 16/80\n",
      "2278/2278 [==============================] - trainLoss: 4.1722  Val_loss: 12.7114 \n",
      "Epoch 17/80\n",
      "2278/2278 [==============================] - trainLoss: 3.9658  Val_loss: 12.7534 \n",
      "Epoch 18/80\n",
      "2278/2278 [==============================] - trainLoss: 3.8702  Val_loss: 12.7041 \n",
      "Epoch 19/80\n",
      "2278/2278 [==============================] - trainLoss: 3.8667  Val_loss: 12.7718 \n",
      "Epoch 20/80\n",
      "2278/2278 [==============================] - trainLoss: 3.7041  Val_loss: 12.8869 \n",
      "Epoch 21/80\n",
      "2278/2278 [==============================] - trainLoss: 3.5547  Val_loss: 12.5492 \n",
      "Epoch 22/80\n",
      "2278/2278 [==============================] - trainLoss: 3.5778  Val_loss: 12.8707 \n",
      "Epoch 23/80\n",
      "2278/2278 [==============================] - trainLoss: 3.3971  Val_loss: 12.6696 \n",
      "Epoch 24/80\n",
      "2278/2278 [==============================] - trainLoss: 3.4877  Val_loss: 12.7864 \n",
      "Epoch 25/80\n",
      "2278/2278 [==============================] - trainLoss: 3.3555  Val_loss: 12.8057 \n",
      "Epoch 26/80\n",
      "2278/2278 [==============================] - trainLoss: 3.3950  Val_loss: 12.5856 \n",
      "Epoch 27/80\n",
      "2278/2278 [==============================] - trainLoss: 3.3032  Val_loss: 12.3987 \n",
      "Epoch 28/80\n",
      "2278/2278 [==============================] - trainLoss: 3.1326  Val_loss: 12.7940 \n",
      "Epoch 29/80\n",
      "2278/2278 [==============================] - trainLoss: 3.3142  Val_loss: 12.7556 \n",
      "Epoch 30/80\n",
      "2278/2278 [==============================] - trainLoss: 3.1496  Val_loss: 12.6355 \n",
      "Epoch 31/80\n",
      "2278/2278 [==============================] - trainLoss: 3.2102  Val_loss: 12.6824 \n",
      "Epoch 32/80\n",
      "2278/2278 [==============================] - trainLoss: 3.3031  Val_loss: 12.4826 \n",
      "Epoch 33/80\n",
      "2278/2278 [==============================] - trainLoss: 2.9716  Val_loss: 12.7556 \n",
      "Epoch 34/80\n",
      "2278/2278 [==============================] - trainLoss: 3.1561  Val_loss: 12.6529 \n",
      "Epoch 35/80\n",
      "2278/2278 [==============================] - trainLoss: 3.0285  Val_loss: 12.7535 \n",
      "Epoch 36/80\n",
      "2278/2278 [==============================] - trainLoss: 3.0119  Val_loss: 12.6726 \n",
      "WARNING:tensorflow:No training configuration found in save file: the model was *not* compiled. Compile it manually.\n",
      "WARNING:tensorflow:No training configuration found in save file: the model was *not* compiled. Compile it manually.\n",
      "WARNING:tensorflow:No training configuration found in save file: the model was *not* compiled. Compile it manually.\n",
      "WARNING:tensorflow:No training configuration found in save file: the model was *not* compiled. Compile it manually.\n",
      "Elapsed/s:  46.1694815158844\n",
      "Final training loss:  tf.Tensor(3.0119221, shape=(), dtype=float32)\n",
      "best val loss:  tf.Tensor(12.398746, shape=(), dtype=float32)\n",
      "{0: {'val_loss': <tf.Tensor: shape=(), dtype=float32, numpy=12.793457>, 'parameters': {'learning_rate': 0.0005, 'beta': 1, 'codings_size': 169, 'n_neurons_classifier': 72, 'N': 10.0, 'n_hidden_classifier': 1, 'n_neurons': 137, 'n_hidden': 2}}, 1: {'val_loss': <tf.Tensor: shape=(), dtype=float32, numpy=nan>, 'parameters': {'learning_rate': 0.001, 'beta': 10, 'codings_size': 107, 'n_neurons_classifier': 22, 'N': 1.0, 'n_hidden_classifier': 2, 'n_neurons': 152, 'n_hidden': 2}}, 2: {'val_loss': <tf.Tensor: shape=(), dtype=float32, numpy=12.398746>, 'parameters': {'learning_rate': 0.001, 'beta': 1, 'codings_size': 123, 'n_neurons_classifier': 21, 'N': 10.0, 'n_hidden_classifier': 1, 'n_neurons': 422, 'n_hidden': 1}}}\n",
      "Epoch 0/80\n",
      "WARNING:tensorflow:Layer full_model is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because it's dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "2240/2278 [============================>.] - Loss for batch: 90.4830WARNING:tensorflow:Layer concatenate_1 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because it's dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "2278/2278 [==============================] - trainLoss: 90.4830  Val_loss: 19.6848 \n",
      "Epoch 1/80\n",
      "2278/2278 [==============================] - trainLoss: 38.6087  Val_loss: 17.5816 \n",
      "Epoch 2/80\n",
      "2278/2278 [==============================] - trainLoss: 28.6396  Val_loss: 15.8723 \n",
      "Epoch 3/80\n",
      "2278/2278 [==============================] - trainLoss: 22.2311  Val_loss: 15.0074 \n",
      "Epoch 4/80\n",
      "2278/2278 [==============================] - trainLoss: 18.3462  Val_loss: 14.3295 \n",
      "Epoch 5/80\n",
      "2278/2278 [==============================] - trainLoss: 15.8077  Val_loss: 13.8808 \n",
      "Epoch 6/80\n",
      "2278/2278 [==============================] - trainLoss: 13.8303  Val_loss: 13.7387 \n",
      "Epoch 7/80\n",
      "2278/2278 [==============================] - trainLoss: 12.3473  Val_loss: 13.6565 \n",
      "Epoch 8/80\n",
      "2278/2278 [==============================] - trainLoss: 11.1933  Val_loss: 13.3752 \n",
      "Epoch 9/80\n",
      "2278/2278 [==============================] - trainLoss: 10.1619  Val_loss: 13.3986 \n",
      "Epoch 10/80\n",
      "2278/2278 [==============================] - trainLoss: 9.4856  Val_loss: 13.4367 \n",
      "Epoch 11/80\n",
      "2278/2278 [==============================] - trainLoss: 8.6964  Val_loss: 13.1593 \n",
      "Epoch 12/80\n",
      "2278/2278 [==============================] - trainLoss: 8.1100  Val_loss: 13.2265 \n",
      "Epoch 13/80\n",
      "2278/2278 [==============================] - trainLoss: 7.7476  Val_loss: 13.2593 \n",
      "Epoch 14/80\n",
      "2278/2278 [==============================] - trainLoss: 7.1032  Val_loss: 13.0019 \n",
      "Epoch 15/80\n",
      "2278/2278 [==============================] - trainLoss: 6.8574  Val_loss: 13.1385 \n",
      "Epoch 16/80\n",
      "2278/2278 [==============================] - trainLoss: 6.4416  Val_loss: 13.0824 \n",
      "Epoch 17/80\n",
      "2278/2278 [==============================] - trainLoss: 6.1325  Val_loss: 13.0615 \n",
      "Epoch 18/80\n",
      "2278/2278 [==============================] - trainLoss: 5.9223  Val_loss: 13.1256 \n",
      "Epoch 19/80\n",
      "2278/2278 [==============================] - trainLoss: 5.5920  Val_loss: 13.1986 \n",
      "Epoch 20/80\n",
      "2278/2278 [==============================] - trainLoss: 5.2057  Val_loss: 12.9342 \n",
      "Epoch 21/80\n",
      "2278/2278 [==============================] - trainLoss: 5.2189  Val_loss: 12.9371 \n",
      "Epoch 22/80\n",
      "2278/2278 [==============================] - trainLoss: 4.8901  Val_loss: 12.9427 \n",
      "Epoch 23/80\n",
      "2278/2278 [==============================] - trainLoss: 4.6659  Val_loss: 13.1018 \n",
      "Epoch 24/80\n",
      "2278/2278 [==============================] - trainLoss: 4.6796  Val_loss: 12.8163 \n",
      "Epoch 25/80\n",
      "2278/2278 [==============================] - trainLoss: 4.4554  Val_loss: 13.0948 \n",
      "Epoch 26/80\n",
      "2278/2278 [==============================] - trainLoss: 4.3643  Val_loss: 13.2174 \n",
      "Epoch 27/80\n",
      "2278/2278 [==============================] - trainLoss: 4.2674  Val_loss: 12.8742 \n",
      "Epoch 28/80\n",
      "2278/2278 [==============================] - trainLoss: 4.0943  Val_loss: 12.8897 \n",
      "Epoch 29/80\n",
      "2278/2278 [==============================] - trainLoss: 3.9423  Val_loss: 13.1322 \n",
      "Epoch 30/80\n",
      "2278/2278 [==============================] - trainLoss: 3.8172  Val_loss: 13.0677 \n",
      "Epoch 31/80\n",
      "2278/2278 [==============================] - trainLoss: 3.7578  Val_loss: 13.0672 \n",
      "Epoch 32/80\n",
      "2278/2278 [==============================] - trainLoss: 3.7435  Val_loss: 12.9168 \n",
      "Epoch 33/80\n",
      "2278/2278 [==============================] - trainLoss: 3.5393  Val_loss: 12.7165 \n",
      "Epoch 34/80\n",
      "2278/2278 [==============================] - trainLoss: 3.4718  Val_loss: 12.9394 \n",
      "Epoch 35/80\n",
      "2278/2278 [==============================] - trainLoss: 3.3991  Val_loss: 13.1707 \n",
      "Epoch 36/80\n",
      "2278/2278 [==============================] - trainLoss: 3.4533  Val_loss: 12.8039 \n",
      "Epoch 37/80\n",
      "2278/2278 [==============================] - trainLoss: 3.4738  Val_loss: 13.6314 \n",
      "Epoch 38/80\n",
      "2278/2278 [==============================] - trainLoss: 3.2704  Val_loss: 12.9385 \n",
      "Epoch 39/80\n",
      "2278/2278 [==============================] - trainLoss: 3.3857  Val_loss: 12.9309 \n",
      "Epoch 40/80\n",
      "2278/2278 [==============================] - trainLoss: 3.3067  Val_loss: 12.7219 \n",
      "Epoch 41/80\n",
      "2278/2278 [==============================] - trainLoss: 3.1975  Val_loss: 12.9222 \n",
      "Epoch 42/80\n",
      "2278/2278 [==============================] - trainLoss: 3.1532  Val_loss: 12.8923 \n",
      "WARNING:tensorflow:No training configuration found in save file: the model was *not* compiled. Compile it manually.\n",
      "WARNING:tensorflow:No training configuration found in save file: the model was *not* compiled. Compile it manually.\n",
      "WARNING:tensorflow:No training configuration found in save file: the model was *not* compiled. Compile it manually.\n",
      "WARNING:tensorflow:No training configuration found in save file: the model was *not* compiled. Compile it manually.\n",
      "Elapsed/s:  58.70131754875183\n",
      "Final training loss:  tf.Tensor(3.1532211, shape=(), dtype=float32)\n",
      "best val loss:  tf.Tensor(12.716526, shape=(), dtype=float32)\n",
      "{0: {'val_loss': <tf.Tensor: shape=(), dtype=float32, numpy=12.793457>, 'parameters': {'learning_rate': 0.0005, 'beta': 1, 'codings_size': 169, 'n_neurons_classifier': 72, 'N': 10.0, 'n_hidden_classifier': 1, 'n_neurons': 137, 'n_hidden': 2}}, 1: {'val_loss': <tf.Tensor: shape=(), dtype=float32, numpy=nan>, 'parameters': {'learning_rate': 0.001, 'beta': 10, 'codings_size': 107, 'n_neurons_classifier': 22, 'N': 1.0, 'n_hidden_classifier': 2, 'n_neurons': 152, 'n_hidden': 2}}, 2: {'val_loss': <tf.Tensor: shape=(), dtype=float32, numpy=12.398746>, 'parameters': {'learning_rate': 0.001, 'beta': 1, 'codings_size': 123, 'n_neurons_classifier': 21, 'N': 10.0, 'n_hidden_classifier': 1, 'n_neurons': 422, 'n_hidden': 1}}, 3: {'val_loss': <tf.Tensor: shape=(), dtype=float32, numpy=12.716526>, 'parameters': {'learning_rate': 0.0005, 'beta': 1, 'codings_size': 119, 'n_neurons_classifier': 71, 'N': 10.0, 'n_hidden_classifier': 2, 'n_neurons': 398, 'n_hidden': 2}}}\n",
      "Epoch 0/80\n",
      "WARNING:tensorflow:Layer full_model is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because it's dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "2240/2278 [============================>.] - Loss for batch: 812.4078WARNING:tensorflow:Layer concatenate_1 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because it's dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "2278/2278 [==============================] - trainLoss: 812.4078  Val_loss: 20.1088 \n",
      "Epoch 1/80\n",
      "2278/2278 [==============================] - trainLoss: 287.1788  Val_loss: 17.4669 \n",
      "Epoch 2/80\n",
      "2278/2278 [==============================] - trainLoss: 203.4015  Val_loss: 15.6804 \n",
      "Epoch 3/80\n",
      "2278/2278 [==============================] - trainLoss: 151.6182  Val_loss: 14.7950 \n",
      "Epoch 4/80\n",
      "2278/2278 [==============================] - trainLoss: 119.4717  Val_loss: 14.3745 \n",
      "Epoch 5/80\n",
      "2278/2278 [==============================] - trainLoss: 96.1515  Val_loss: 14.3148 \n",
      "Epoch 6/80\n",
      "2278/2278 [==============================] - trainLoss: 83.8543  Val_loss: 13.9538 \n",
      "Epoch 7/80\n",
      "2278/2278 [==============================] - trainLoss: 70.1440  Val_loss: 13.4311 \n",
      "Epoch 8/80\n",
      "2278/2278 [==============================] - trainLoss: 61.9673  Val_loss: 13.5370 \n",
      "Epoch 9/80\n",
      "2278/2278 [==============================] - trainLoss: 54.3320  Val_loss: 13.5199 \n",
      "Epoch 10/80\n",
      "2278/2278 [==============================] - trainLoss: 48.9220  Val_loss: 13.2011 \n",
      "Epoch 11/80\n",
      "2278/2278 [==============================] - trainLoss: 44.2910  Val_loss: 13.4916 \n",
      "Epoch 12/80\n",
      "2278/2278 [==============================] - trainLoss: 39.4299  Val_loss: 13.7691 \n",
      "Epoch 13/80\n",
      "2278/2278 [==============================] - trainLoss: 36.4381  Val_loss: 13.3901 \n",
      "Epoch 14/80\n",
      "2278/2278 [==============================] - trainLoss: 32.8300  Val_loss: 13.4676 \n",
      "Epoch 15/80\n",
      "2278/2278 [==============================] - trainLoss: 30.5529  Val_loss: 12.8540 \n",
      "Epoch 16/80\n",
      "2278/2278 [==============================] - trainLoss: 28.7882  Val_loss: 13.2802 \n",
      "Epoch 17/80\n",
      "2278/2278 [==============================] - trainLoss: 25.6428  Val_loss: 13.2613 \n",
      "Epoch 18/80\n",
      "2278/2278 [==============================] - trainLoss: 24.4253  Val_loss: 13.6041 \n",
      "Epoch 19/80\n",
      "2278/2278 [==============================] - trainLoss: 22.4621  Val_loss: 13.0889 \n",
      "Epoch 20/80\n",
      "2278/2278 [==============================] - trainLoss: 21.1366  Val_loss: 13.0894 \n",
      "Epoch 21/80\n",
      "2278/2278 [==============================] - trainLoss: 19.7962  Val_loss: 13.4157 \n",
      "Epoch 22/80\n",
      "2278/2278 [==============================] - trainLoss: 18.9654  Val_loss: 13.0851 \n",
      "Epoch 23/80\n",
      "2278/2278 [==============================] - trainLoss: 17.9126  Val_loss: 13.2334 \n",
      "Epoch 24/80\n",
      "2278/2278 [==============================] - trainLoss: 16.7245  Val_loss: 13.1467 \n",
      "WARNING:tensorflow:No training configuration found in save file: the model was *not* compiled. Compile it manually.\n",
      "WARNING:tensorflow:No training configuration found in save file: the model was *not* compiled. Compile it manually.\n",
      "WARNING:tensorflow:No training configuration found in save file: the model was *not* compiled. Compile it manually.\n",
      "WARNING:tensorflow:No training configuration found in save file: the model was *not* compiled. Compile it manually.\n",
      "Elapsed/s:  34.59279680252075\n",
      "Final training loss:  tf.Tensor(16.724504, shape=(), dtype=float32)\n",
      "best val loss:  tf.Tensor(12.854044, shape=(), dtype=float32)\n",
      "{0: {'val_loss': <tf.Tensor: shape=(), dtype=float32, numpy=12.793457>, 'parameters': {'learning_rate': 0.0005, 'beta': 1, 'codings_size': 169, 'n_neurons_classifier': 72, 'N': 10.0, 'n_hidden_classifier': 1, 'n_neurons': 137, 'n_hidden': 2}}, 1: {'val_loss': <tf.Tensor: shape=(), dtype=float32, numpy=nan>, 'parameters': {'learning_rate': 0.001, 'beta': 10, 'codings_size': 107, 'n_neurons_classifier': 22, 'N': 1.0, 'n_hidden_classifier': 2, 'n_neurons': 152, 'n_hidden': 2}}, 2: {'val_loss': <tf.Tensor: shape=(), dtype=float32, numpy=12.398746>, 'parameters': {'learning_rate': 0.001, 'beta': 1, 'codings_size': 123, 'n_neurons_classifier': 21, 'N': 10.0, 'n_hidden_classifier': 1, 'n_neurons': 422, 'n_hidden': 1}}, 3: {'val_loss': <tf.Tensor: shape=(), dtype=float32, numpy=12.716526>, 'parameters': {'learning_rate': 0.0005, 'beta': 1, 'codings_size': 119, 'n_neurons_classifier': 71, 'N': 10.0, 'n_hidden_classifier': 2, 'n_neurons': 398, 'n_hidden': 2}}, 4: {'val_loss': <tf.Tensor: shape=(), dtype=float32, numpy=12.854044>, 'parameters': {'learning_rate': 0.0005, 'beta': 15, 'codings_size': 91, 'n_neurons_classifier': 22, 'N': 1.0, 'n_hidden_classifier': 2, 'n_neurons': 508, 'n_hidden': 2}}}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:No training configuration found in save file: the model was *not* compiled. Compile it manually.\n",
      "WARNING:tensorflow:No training configuration found in save file: the model was *not* compiled. Compile it manually.\n",
      "WARNING:tensorflow:No training configuration found in save file: the model was *not* compiled. Compile it manually.\n",
      "WARNING:tensorflow:No training configuration found in save file: the model was *not* compiled. Compile it manually.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[(0,\n",
       "  {'parameters': {'N': 10.0,\n",
       "    'beta': 1,\n",
       "    'codings_size': 169,\n",
       "    'learning_rate': 0.0005,\n",
       "    'n_hidden': 2,\n",
       "    'n_hidden_classifier': 1,\n",
       "    'n_neurons': 137,\n",
       "    'n_neurons_classifier': 72},\n",
       "   'val_loss': <tf.Tensor: shape=(), dtype=float32, numpy=12.793457>}),\n",
       " (1,\n",
       "  {'parameters': {'N': 1.0,\n",
       "    'beta': 10,\n",
       "    'codings_size': 107,\n",
       "    'learning_rate': 0.001,\n",
       "    'n_hidden': 2,\n",
       "    'n_hidden_classifier': 2,\n",
       "    'n_neurons': 152,\n",
       "    'n_neurons_classifier': 22},\n",
       "   'val_loss': <tf.Tensor: shape=(), dtype=float32, numpy=nan>}),\n",
       " (2,\n",
       "  {'parameters': {'N': 10.0,\n",
       "    'beta': 1,\n",
       "    'codings_size': 123,\n",
       "    'learning_rate': 0.001,\n",
       "    'n_hidden': 1,\n",
       "    'n_hidden_classifier': 1,\n",
       "    'n_neurons': 422,\n",
       "    'n_neurons_classifier': 21},\n",
       "   'val_loss': <tf.Tensor: shape=(), dtype=float32, numpy=12.398746>}),\n",
       " (3,\n",
       "  {'parameters': {'N': 10.0,\n",
       "    'beta': 1,\n",
       "    'codings_size': 119,\n",
       "    'learning_rate': 0.0005,\n",
       "    'n_hidden': 2,\n",
       "    'n_hidden_classifier': 2,\n",
       "    'n_neurons': 398,\n",
       "    'n_neurons_classifier': 71},\n",
       "   'val_loss': <tf.Tensor: shape=(), dtype=float32, numpy=12.716526>}),\n",
       " (4,\n",
       "  {'parameters': {'N': 1.0,\n",
       "    'beta': 15,\n",
       "    'codings_size': 91,\n",
       "    'learning_rate': 0.0005,\n",
       "    'n_hidden': 2,\n",
       "    'n_hidden_classifier': 2,\n",
       "    'n_neurons': 508,\n",
       "    'n_neurons_classifier': 22},\n",
       "   'val_loss': <tf.Tensor: shape=(), dtype=float32, numpy=12.854044>})]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result,variational_encoder,variational_decoder,classifier,y_distribution = hyperparameter_search(param_distribs=param_distribs,\n",
    "                                                                        epochs=80,patience=10,n_iter=5)\n",
    "result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Best KL-based model has val nloglik of 12.398746. \n",
    "\n",
    "'parameters': {'N': 10.0,\n",
    "    'beta': 1,\n",
    "    'codings_size': 123,\n",
    "    'learning_rate': 0.001,\n",
    "    'n_hidden': 1,\n",
    "    'n_hidden_classifier': 1,\n",
    "    'n_neurons': 422,\n",
    "    'n_neurons_classifier': 21},\n",
    "   'val_loss': <tf.Tensor: shape=(), dtype=float32, numpy=12.398746"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Single run # "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "variational_encoder,variational_decoder,classifier,y_distribution,model = build_model(n_hidden=1, n_neurons=422,input_shape=input_shape,beta=1,n_hidden_classifier=1,\n",
    "              n_neurons_classifier=21,N=10,codings_size=123)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0/100\n",
      "WARNING:tensorflow:Layer full_model is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because it's dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "2240/2278 [============================>.] - Loss for batch: 36.3465WARNING:tensorflow:Layer concatenate_1 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because it's dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "2278/2278 [==============================] - trainLoss: 36.3465  Val_loss: 17.5259 \n",
      "Epoch 1/100\n",
      "2278/2278 [==============================] - trainLoss: 17.3172  Val_loss: 15.0695 \n",
      "Epoch 2/100\n",
      "2278/2278 [==============================] - trainLoss: 13.4429  Val_loss: 14.1935 \n",
      "Epoch 3/100\n",
      "2278/2278 [==============================] - trainLoss: 11.0726  Val_loss: 13.9215 \n",
      "Epoch 4/100\n",
      "2278/2278 [==============================] - trainLoss: 9.5394  Val_loss: 13.6584 \n",
      "Epoch 5/100\n",
      "2278/2278 [==============================] - trainLoss: 8.4240  Val_loss: 13.3760 \n",
      "Epoch 6/100\n",
      "2278/2278 [==============================] - trainLoss: 7.3463  Val_loss: 13.2381 \n",
      "Epoch 7/100\n",
      "2278/2278 [==============================] - trainLoss: 6.6752  Val_loss: 13.2357 \n",
      "Epoch 8/100\n",
      "2278/2278 [==============================] - trainLoss: 6.1428  Val_loss: 13.1244 \n",
      "Epoch 9/100\n",
      "2278/2278 [==============================] - trainLoss: 5.7523  Val_loss: 12.9639 \n",
      "Epoch 10/100\n",
      "2278/2278 [==============================] - trainLoss: 5.3309  Val_loss: 12.9399 \n",
      "Epoch 11/100\n",
      "2278/2278 [==============================] - trainLoss: 5.0741  Val_loss: 12.9745 \n",
      "Epoch 12/100\n",
      "2278/2278 [==============================] - trainLoss: 4.6312  Val_loss: 12.8637 \n",
      "Epoch 13/100\n",
      "2278/2278 [==============================] - trainLoss: 4.5103  Val_loss: 12.8343 \n",
      "Epoch 14/100\n",
      "2278/2278 [==============================] - trainLoss: 4.1471  Val_loss: 12.9311 \n",
      "Epoch 15/100\n",
      "2278/2278 [==============================] - trainLoss: 4.1413  Val_loss: 12.7417 \n",
      "Epoch 16/100\n",
      "2278/2278 [==============================] - trainLoss: 4.0190  Val_loss: 12.9249 \n",
      "Epoch 17/100\n",
      "2278/2278 [==============================] - trainLoss: 3.7375  Val_loss: 12.7306 \n",
      "Epoch 18/100\n",
      "2278/2278 [==============================] - trainLoss: 3.6865  Val_loss: 12.7484 \n",
      "Epoch 19/100\n",
      "2278/2278 [==============================] - trainLoss: 3.4794  Val_loss: 12.8354 \n",
      "Epoch 20/100\n",
      "2278/2278 [==============================] - trainLoss: 3.5126  Val_loss: 12.8590 \n",
      "Epoch 21/100\n",
      "2278/2278 [==============================] - trainLoss: 3.4384  Val_loss: 12.8021 \n",
      "Epoch 22/100\n",
      "2278/2278 [==============================] - trainLoss: 3.3551  Val_loss: 12.6595 \n",
      "Epoch 23/100\n",
      "2278/2278 [==============================] - trainLoss: 3.3126  Val_loss: 12.5918 \n",
      "Epoch 24/100\n",
      "2278/2278 [==============================] - trainLoss: 3.3056  Val_loss: 12.6234 \n",
      "Epoch 25/100\n",
      "2278/2278 [==============================] - trainLoss: 3.2659  Val_loss: 12.5700 \n",
      "Epoch 26/100\n",
      "2278/2278 [==============================] - trainLoss: 3.2297  Val_loss: 12.5455 \n",
      "Epoch 27/100\n",
      "2278/2278 [==============================] - trainLoss: 3.1152  Val_loss: 12.5569 \n",
      "Epoch 28/100\n",
      "2278/2278 [==============================] - trainLoss: 3.1846  Val_loss: 12.5976 \n",
      "Epoch 29/100\n",
      "2278/2278 [==============================] - trainLoss: 3.2001  Val_loss: 12.7662 \n",
      "Epoch 30/100\n",
      "2278/2278 [==============================] - trainLoss: 3.0771  Val_loss: 12.5858 \n",
      "Epoch 31/100\n",
      "2278/2278 [==============================] - trainLoss: 3.0726  Val_loss: 12.6161 \n",
      "Epoch 32/100\n",
      "2278/2278 [==============================] - trainLoss: 3.1516  Val_loss: 12.7560 \n",
      "Epoch 33/100\n",
      "2278/2278 [==============================] - trainLoss: 3.0336  Val_loss: 12.5141 \n",
      "Epoch 34/100\n",
      "2278/2278 [==============================] - trainLoss: 3.0214  Val_loss: 12.5676 \n",
      "Epoch 35/100\n",
      "2278/2278 [==============================] - trainLoss: 3.0135  Val_loss: 12.5544 \n",
      "Epoch 36/100\n",
      "2278/2278 [==============================] - trainLoss: 3.1077  Val_loss: 12.7638 \n",
      "Epoch 37/100\n",
      "2278/2278 [==============================] - trainLoss: 2.9502  Val_loss: 12.8016 \n",
      "Epoch 38/100\n",
      "2278/2278 [==============================] - trainLoss: 3.1503  Val_loss: 12.6080 \n",
      "Epoch 39/100\n",
      "2278/2278 [==============================] - trainLoss: 2.9422  Val_loss: 12.8400 \n",
      "Epoch 40/100\n",
      "2278/2278 [==============================] - trainLoss: 2.9727  Val_loss: 12.4142 \n",
      "Epoch 41/100\n",
      "2278/2278 [==============================] - trainLoss: 2.9300  Val_loss: 12.6412 \n",
      "Epoch 42/100\n",
      "2278/2278 [==============================] - trainLoss: 3.0131  Val_loss: 12.4349 \n",
      "Epoch 43/100\n",
      "2278/2278 [==============================] - trainLoss: 2.9080  Val_loss: 12.4407 \n",
      "Epoch 44/100\n",
      "2278/2278 [==============================] - trainLoss: 3.0138  Val_loss: 12.7255 \n",
      "Epoch 45/100\n",
      "2278/2278 [==============================] - trainLoss: 3.0533  Val_loss: 12.4475 \n",
      "Epoch 46/100\n",
      "2278/2278 [==============================] - trainLoss: 3.0536  Val_loss: 12.6259 \n",
      "Epoch 47/100\n",
      "2278/2278 [==============================] - trainLoss: 2.8296  Val_loss: 12.6951 \n",
      "Epoch 48/100\n",
      "2278/2278 [==============================] - trainLoss: 2.9579  Val_loss: 12.4597 \n",
      "Epoch 49/100\n",
      "2278/2278 [==============================] - trainLoss: 2.8668  Val_loss: 12.3593 \n",
      "Epoch 50/100\n",
      "2278/2278 [==============================] - trainLoss: 2.9648  Val_loss: 12.6554 \n",
      "Epoch 51/100\n",
      "2278/2278 [==============================] - trainLoss: 3.0057  Val_loss: 12.4858 \n",
      "Epoch 52/100\n",
      "2278/2278 [==============================] - trainLoss: 2.8607  Val_loss: 12.3312 \n",
      "Epoch 53/100\n",
      "2278/2278 [==============================] - trainLoss: 2.7674  Val_loss: 12.5470 \n",
      "Epoch 54/100\n",
      "2278/2278 [==============================] - trainLoss: 3.0269  Val_loss: 12.4970 \n",
      "Epoch 55/100\n",
      "2278/2278 [==============================] - trainLoss: 2.9759  Val_loss: 12.5394 \n",
      "Epoch 56/100\n",
      "2278/2278 [==============================] - trainLoss: 2.8163  Val_loss: 12.5806 \n",
      "Epoch 57/100\n",
      "2278/2278 [==============================] - trainLoss: 2.8485  Val_loss: 12.5804 \n",
      "Epoch 58/100\n",
      "2278/2278 [==============================] - trainLoss: 2.7234  Val_loss: 12.6180 \n",
      "Epoch 59/100\n",
      "2278/2278 [==============================] - trainLoss: 2.7643  Val_loss: 12.4964 \n",
      "Epoch 60/100\n",
      "2278/2278 [==============================] - trainLoss: 2.7552  Val_loss: 12.5534 \n",
      "Epoch 61/100\n",
      "2278/2278 [==============================] - trainLoss: 2.7733  Val_loss: 12.4904 \n",
      "WARNING:tensorflow:No training configuration found in save file: the model was *not* compiled. Compile it manually.\n",
      "WARNING:tensorflow:No training configuration found in save file: the model was *not* compiled. Compile it manually.\n",
      "WARNING:tensorflow:No training configuration found in save file: the model was *not* compiled. Compile it manually.\n",
      "WARNING:tensorflow:No training configuration found in save file: the model was *not* compiled. Compile it manually.\n",
      "Elapsed/s:  75.89681100845337\n",
      "Final training loss:  tf.Tensor(2.773266, shape=(), dtype=float32)\n",
      "best val loss:  tf.Tensor(12.331202, shape=(), dtype=float32)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "([<tf.Tensor: shape=(), dtype=float32, numpy=36.346508>,\n",
       "  <tf.Tensor: shape=(), dtype=float32, numpy=17.317219>,\n",
       "  <tf.Tensor: shape=(), dtype=float32, numpy=13.442852>,\n",
       "  <tf.Tensor: shape=(), dtype=float32, numpy=11.072579>,\n",
       "  <tf.Tensor: shape=(), dtype=float32, numpy=9.539424>,\n",
       "  <tf.Tensor: shape=(), dtype=float32, numpy=8.423951>,\n",
       "  <tf.Tensor: shape=(), dtype=float32, numpy=7.3463244>,\n",
       "  <tf.Tensor: shape=(), dtype=float32, numpy=6.675222>,\n",
       "  <tf.Tensor: shape=(), dtype=float32, numpy=6.1428337>,\n",
       "  <tf.Tensor: shape=(), dtype=float32, numpy=5.752289>,\n",
       "  <tf.Tensor: shape=(), dtype=float32, numpy=5.3308945>,\n",
       "  <tf.Tensor: shape=(), dtype=float32, numpy=5.0740943>,\n",
       "  <tf.Tensor: shape=(), dtype=float32, numpy=4.631156>,\n",
       "  <tf.Tensor: shape=(), dtype=float32, numpy=4.51026>,\n",
       "  <tf.Tensor: shape=(), dtype=float32, numpy=4.147092>,\n",
       "  <tf.Tensor: shape=(), dtype=float32, numpy=4.1413116>,\n",
       "  <tf.Tensor: shape=(), dtype=float32, numpy=4.018955>,\n",
       "  <tf.Tensor: shape=(), dtype=float32, numpy=3.7374825>,\n",
       "  <tf.Tensor: shape=(), dtype=float32, numpy=3.6865363>,\n",
       "  <tf.Tensor: shape=(), dtype=float32, numpy=3.479358>,\n",
       "  <tf.Tensor: shape=(), dtype=float32, numpy=3.5126057>,\n",
       "  <tf.Tensor: shape=(), dtype=float32, numpy=3.4383872>,\n",
       "  <tf.Tensor: shape=(), dtype=float32, numpy=3.3550692>,\n",
       "  <tf.Tensor: shape=(), dtype=float32, numpy=3.312581>,\n",
       "  <tf.Tensor: shape=(), dtype=float32, numpy=3.3056107>,\n",
       "  <tf.Tensor: shape=(), dtype=float32, numpy=3.2658842>,\n",
       "  <tf.Tensor: shape=(), dtype=float32, numpy=3.2297387>,\n",
       "  <tf.Tensor: shape=(), dtype=float32, numpy=3.115235>,\n",
       "  <tf.Tensor: shape=(), dtype=float32, numpy=3.184578>,\n",
       "  <tf.Tensor: shape=(), dtype=float32, numpy=3.2000806>,\n",
       "  <tf.Tensor: shape=(), dtype=float32, numpy=3.0771382>,\n",
       "  <tf.Tensor: shape=(), dtype=float32, numpy=3.0725818>,\n",
       "  <tf.Tensor: shape=(), dtype=float32, numpy=3.151602>,\n",
       "  <tf.Tensor: shape=(), dtype=float32, numpy=3.0336452>,\n",
       "  <tf.Tensor: shape=(), dtype=float32, numpy=3.0214398>,\n",
       "  <tf.Tensor: shape=(), dtype=float32, numpy=3.0135422>,\n",
       "  <tf.Tensor: shape=(), dtype=float32, numpy=3.1077468>,\n",
       "  <tf.Tensor: shape=(), dtype=float32, numpy=2.9502463>,\n",
       "  <tf.Tensor: shape=(), dtype=float32, numpy=3.1503375>,\n",
       "  <tf.Tensor: shape=(), dtype=float32, numpy=2.942151>,\n",
       "  <tf.Tensor: shape=(), dtype=float32, numpy=2.9727235>,\n",
       "  <tf.Tensor: shape=(), dtype=float32, numpy=2.9300127>,\n",
       "  <tf.Tensor: shape=(), dtype=float32, numpy=3.0130582>,\n",
       "  <tf.Tensor: shape=(), dtype=float32, numpy=2.9080296>,\n",
       "  <tf.Tensor: shape=(), dtype=float32, numpy=3.0137918>,\n",
       "  <tf.Tensor: shape=(), dtype=float32, numpy=3.0532556>,\n",
       "  <tf.Tensor: shape=(), dtype=float32, numpy=3.0535903>,\n",
       "  <tf.Tensor: shape=(), dtype=float32, numpy=2.8296454>,\n",
       "  <tf.Tensor: shape=(), dtype=float32, numpy=2.9579475>,\n",
       "  <tf.Tensor: shape=(), dtype=float32, numpy=2.8667526>,\n",
       "  <tf.Tensor: shape=(), dtype=float32, numpy=2.9648187>,\n",
       "  <tf.Tensor: shape=(), dtype=float32, numpy=3.0057013>,\n",
       "  <tf.Tensor: shape=(), dtype=float32, numpy=2.8606968>,\n",
       "  <tf.Tensor: shape=(), dtype=float32, numpy=2.767381>,\n",
       "  <tf.Tensor: shape=(), dtype=float32, numpy=3.0269492>,\n",
       "  <tf.Tensor: shape=(), dtype=float32, numpy=2.975883>,\n",
       "  <tf.Tensor: shape=(), dtype=float32, numpy=2.8162634>,\n",
       "  <tf.Tensor: shape=(), dtype=float32, numpy=2.8484929>,\n",
       "  <tf.Tensor: shape=(), dtype=float32, numpy=2.7234194>,\n",
       "  <tf.Tensor: shape=(), dtype=float32, numpy=2.764329>,\n",
       "  <tf.Tensor: shape=(), dtype=float32, numpy=2.755168>,\n",
       "  <tf.Tensor: shape=(), dtype=float32, numpy=2.773266>],\n",
       " <tf.Tensor: shape=(), dtype=float32, numpy=12.331202>)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fit_model(X_train_omics_labelled, train_set_labelled_y, X_train_omics_unlabelled,train_set_labelled_c,\n",
    "          train_set_unlabelled_c,100,X_valid_omics, valid_set_labelled_y,valid_set_labelled_c,\n",
    "              10,variational_encoder=variational_encoder,variational_decoder=variational_decoder,\n",
    "             classifier=classifier,y_distribution=y_distribution,model=model,\n",
    "          Sampling=Sampling,y_dist=y_dist,batch_size=32,learning_rate=0.001,valid_set=True,codings_size=123)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test_nlog_lik = tf.Tensor(12.778836, shape=(), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "test_nlog_lik = -validation_log_lik_sampling(test_set_labelled_y,X_test_omics.to_numpy(),test_set_labelled_c.to_numpy(),\n",
    "                                    variational_decoder=variational_decoder,codings_size=123,samples=2000)\n",
    "print(\"test_nlog_lik = \" + str(test_nlog_lik))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
