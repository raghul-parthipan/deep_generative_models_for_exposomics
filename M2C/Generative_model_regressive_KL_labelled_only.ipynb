{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "assert sys.version_info >= (3, 5)\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "assert tf.__version__ >= \"2.0\"\n",
    "import numpy as np\n",
    "import time\n",
    "K = keras.backend\n",
    "import pandas as pd\n",
    "import math\n",
    "import os\n",
    "from sklearn.metrics import r2_score\n",
    "from scipy.stats import uniform,randint,norm\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import scipy.stats\n",
    "from sklearn.preprocessing import MinMaxScaler \n",
    "\n",
    "\n",
    "#check os.environ ld_library_path is the same here as when I do it in python via terminal, if I get issues\n",
    "\n",
    "#sometimes I can't select the GPU. In this case, try: https://forums.fast.ai/t/tip-limiting-tensorflow-to-one-gpu/1995\n",
    "\n",
    "tf.random.set_seed(42)\n",
    "np.random.seed(42)\n",
    "random_state=42"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_path = 'save_path'\n",
    "os.chdir(save_path)\n",
    "\n",
    "X_train_omics_unlabelled = pd.read_csv(\"X_train_omics_unlabelled.csv\",index_col=0)\n",
    "X_train_omics_labelled = pd.read_csv(\"X_train_omics_labelled.csv\",index_col=0)\n",
    "X_test_omics= pd.read_csv(\"X_test_omics.csv\",index_col=0)\n",
    "X_valid_omics= pd.read_csv(\"X_valid_omics.csv\",index_col=0)\n",
    "features = np.load(\"feature_selection.npy\",allow_pickle=True)\n",
    "\n",
    "train_set_labelled_y= pd.read_csv(\"train_set_labelled_y.csv\",index_col=0)\n",
    "test_set_labelled_y= pd.read_csv(\"test_set_labelled_y.csv\",index_col=0)\n",
    "valid_set_labelled_y= pd.read_csv(\"valid_set_labelled_y.csv\",index_col=0)\n",
    "\n",
    "X_train_omics_unlabelled = X_train_omics_unlabelled[features]\n",
    "X_train_omics_labelled = X_train_omics_labelled[features]\n",
    "X_test_omics = X_test_omics[features]\n",
    "X_valid_omics = X_valid_omics[features]\n",
    "\n",
    "train_set_labelled_c= pd.read_csv(\"train_set_labelled_c.csv\",index_col=0)\n",
    "train_set_unlabelled_c= pd.read_csv(\"train_set_unlabelled_c.csv\",index_col=0)\n",
    "test_set_labelled_c= pd.read_csv(\"test_set_labelled_c.csv\",index_col=0)\n",
    "valid_set_labelled_c= pd.read_csv(\"valid_set_labelled_c.csv\",index_col=0)\n",
    "\n",
    "\n",
    "train_set_labelled_c = train_set_labelled_c[[\"age\",\"male\"]]\n",
    "train_set_unlabelled_c = train_set_unlabelled_c[[\"age\",\"male\"]]\n",
    "test_set_labelled_c = test_set_labelled_c[[\"age\",\"male\"]]\n",
    "valid_set_labelled_c = valid_set_labelled_c[[\"age\",\"male\"]]\n",
    "\n",
    "scaler = MinMaxScaler()\n",
    "train_set_labelled_y = scaler.fit_transform(train_set_labelled_y)\n",
    "valid_set_labelled_y = scaler.transform(valid_set_labelled_y)\n",
    "test_set_labelled_y = scaler.transform(test_set_labelled_y)\n",
    "\n",
    "valid_set_labelled_y[np.where(valid_set_labelled_y >1)] = 1\n",
    "test_set_labelled_y[np.where(test_set_labelled_y >1)] = 1\n",
    "\n",
    "\n",
    "train_set_labelled_c[\"age\"] = scaler.fit_transform(train_set_labelled_c[[\"age\"]])\n",
    "train_set_unlabelled_c[\"age\"] = scaler.transform(train_set_unlabelled_c[[\"age\"]])\n",
    "test_set_labelled_c[\"age\"] = scaler.transform(test_set_labelled_c[[\"age\"]])\n",
    "valid_set_labelled_c[\"age\"] = scaler.transform(valid_set_labelled_c[[\"age\"]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_model_path = 'save_model_path'\n",
    "os.chdir(save_model_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_shape = X_train_omics_labelled.shape[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Custom parts # "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Helpful functions ##"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def validation_log_lik_sampling(y_val,x_val,c_val,variational_decoder,codings_size,samples=200):\n",
    "\n",
    "    \"\"\"\n",
    "    Samples a value of z for the expectation, and calculates something proportional to loglikelihood.\n",
    "    \n",
    "    The more samples of z, the better the MC approximation to loglik, but the longer it takes to compute.\n",
    "    \n",
    "    This is how we do our evaluation on the validation and also test set. \n",
    "    \n",
    "    We look at the ability to generate x given y i.e. loglik(x|y,c)\"\"\"\n",
    "    \n",
    "    x_val_len = len(x_val)\n",
    "    expectation = 0\n",
    "    for i in range(samples):\n",
    "        z = np.random.normal(loc=0,scale=1,size=codings_size*x_val_len).reshape(x_val_len,codings_size)\n",
    "        x_pred = variational_decoder([z,y_val,c_val])\n",
    "        diff = (x_val-x_pred)**2\n",
    "        pdf = K.sum(diff,axis=-1)\n",
    "        pdf = K.exp(-pdf)\n",
    "        expectation += pdf \n",
    "    expectation = expectation / samples\n",
    "    lik = tf.math.log(expectation)\n",
    "    lik = K.mean(lik)    \n",
    "    return lik\n",
    "\n",
    "def create_batch(x_label, y_label, c_label, batch_s=32):\n",
    "    '''\n",
    "    Creates batches of labelled and unlabelled data. The total number of points in both batches is equal to batch_s. \n",
    "    Thanks to Omer Nivron for help with this.\n",
    "    \n",
    "    '''\n",
    "    proportion_labelled = x_label.shape[0]/(x_label.shape[0])\n",
    "    \n",
    "    shape_label = x_label.shape[0]\n",
    "    label_per_batch = int(np.ceil(proportion_labelled*batch_s))\n",
    "    batch_idx_la = np.random.choice(list(range(shape_label)), label_per_batch)\n",
    "    batch_x_la = (x_label.iloc[batch_idx_la, :])\n",
    "    batch_y_la = (y_label[batch_idx_la,:])\n",
    "    batch_c_la = (c_label.iloc[batch_idx_la,:])\n",
    "\n",
    "    \n",
    "    del batch_idx_la\n",
    "            \n",
    "    return batch_x_la, batch_y_la,batch_c_la\n",
    "\n",
    "\n",
    "def progress_bar(iteration, total, size=30):\n",
    "    \"\"\"Progress bar for training\"\"\"\n",
    "    running = iteration < total\n",
    "    c = \">\" if running else \"=\"\n",
    "    p = (size - 1) * iteration // total\n",
    "    fmt = \"{{:-{}d}}/{{}} [{{}}]\".format(len(str(total)))\n",
    "    params = [iteration, total, \"=\" * p + c + \".\" * (size - p - 1)]\n",
    "    return fmt.format(*params)\n",
    "\n",
    "def print_status_bar(iteration, total, loss, metrics=None, size=30):\n",
    "    \"\"\"Status bar for training\"\"\"\n",
    "    metrics = \" - \".join([\"Loss for batch: {:.4f}\".format(loss)])\n",
    "    end = \"\" if iteration < total else \"\\n\"\n",
    "    print(\"\\r{} - {}\".format(progress_bar(iteration, total), metrics), end=end)\n",
    "    \n",
    "def print_status_bar_epoch(iteration, total, training_loss_for_epoch,val_loss, metrics=None, size=30):\n",
    "    \"\"\"Status bar for training (end of epoch)\"\"\"\n",
    "    metrics = \" - \".join(\n",
    "        [\"trainLoss: {:.4f}  Val_loss: {:.4f} \".format(\n",
    "            training_loss_for_epoch,val_loss)]\n",
    "    )\n",
    "    end = \"\" if iteration < total else \"\\n\"\n",
    "    print(\"\\r{} - {}\".format(progress_bar(iteration, total), metrics), end=end)\n",
    "    \n",
    "    \n",
    "def list_average(list_of_loss):\n",
    "    return sum(list_of_loss)/len(list_of_loss)\n",
    " \n",
    "\n",
    "def gaussian_pdf(array,mean,sigma):\n",
    "    part1 = tf.math.divide(tf.constant(np.array(1.0).reshape(1,-1),dtype=\"float32\"),sigma*(2*math.pi)**0.5)\n",
    "    part2 = K.exp(-0.5*tf.math.divide((array-mean),sigma)**2)\n",
    "    return part1*part2\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model ##"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Custom components ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Sampling(keras.layers.Layer):\n",
    "    \"\"\"reparameterization trick\"\"\"\n",
    "    def call(self, inputs):\n",
    "        mean, log_var = inputs\n",
    "        return K.random_normal(tf.shape(log_var)) * K.exp(log_var/2) + mean  \n",
    "    \n",
    "    \n",
    "class y_dist(keras.layers.Layer):\n",
    "\n",
    "    \"\"\"\n",
    "    Custom layer that is used to learn the parameters of the distribution over y.\n",
    "    \n",
    "    Outputs a loss. The loss is used for training. The loss is a GMM loss.\n",
    "    The mean of this is then taken to provide a per batch loss. \n",
    "        \n",
    "    \"\"\"\n",
    "    def __init__(self,**kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        \n",
    "        \n",
    "    def build(self,batch_input_shape):\n",
    "        self.q1 = self.add_weight(name=\"q1\",shape=[1,1],initializer=\"random_normal\",trainable=True)\n",
    "        self.q2 = self.add_weight(name=\"q2\",shape=[1,1],initializer=\"random_normal\",trainable=True)\n",
    "        self.mu1 = self.add_weight(name=\"mu1\",shape=[1,1],initializer=\"random_normal\",trainable=True)\n",
    "        self.mu2 = self.add_weight(name=\"mu2\",shape=[1,1],initializer=\"random_normal\",trainable=True)\n",
    "        self.mu3 = self.add_weight(name=\"mu3\",shape=[1,1],initializer=\"random_normal\",trainable=True)\n",
    "        self.tau1 = self.add_weight(name=\"tau1\",shape=[1,1],initializer=\"random_normal\",trainable=True)\n",
    "        self.tau2 = self.add_weight(name=\"tau2\",shape=[1,1],initializer=\"random_normal\",trainable=True)\n",
    "        self.tau3 = self.add_weight(name=\"tau3\",shape=[1,1],initializer=\"random_normal\",trainable=True)\n",
    "\n",
    "        super().build(batch_input_shape)\n",
    "    \n",
    "    def call(self,X):\n",
    "        concatenated = tf.concat([self.q1,self.q2,tf.constant(np.array(0.0).reshape(1,-1),dtype=\"float32\")],axis=-1)\n",
    "        p = K.exp(concatenated)\n",
    "        p = tf.math.divide(p,K.sum(p))\n",
    "        sigma_concatenated = tf.concat([self.tau1,self.tau2,self.tau3],axis=-1)\n",
    "        sigma = K.exp(sigma_concatenated)\n",
    "        likelihood = p[0][0]*gaussian_pdf(X,mean=self.mu1,sigma=sigma[0][0])+p[0][1]*gaussian_pdf(X,mean=self.mu2,sigma=sigma[0][1])+p[0][2]*gaussian_pdf(X,mean=self.mu3,sigma=sigma[0][2]) \n",
    "        loglik = tf.math.log(likelihood)\n",
    "        loss = -loglik\n",
    "        loss = tf.reduce_mean(loss)\n",
    "        return loss\n",
    "    \n",
    "    def compute_output_shape(self,batch_input_shape):\n",
    "        return tf.TensorShape(1)    \n",
    "    \n",
    "    \n",
    "class FullModel(keras.models.Model):\n",
    "    \"\"\"\n",
    "    This is the model for labelled data only. For KL. This is used for training purposes.\n",
    "    \n",
    "    It requires an encoder, decoder, classifier and y_distribution model to be already defined (as can be done with \n",
    "    the build_model function).\n",
    "    \n",
    "    It returns the nloglik i.e. the loss. \n",
    "    \n",
    "    This loss can then be used in gradient descent and be minimised wrt parameters (of the four component models).\n",
    "    \n",
    "    At test time, you will call which of the component models you want to use (as opposed to trying to \"call\" this \n",
    "    FullModel which you won't want to do as its purpose is just to calculate the nloglik for training).\n",
    "    \n",
    "    \"\"\"\n",
    "    def __init__(self,N_parameter,beta,variational_encoder,variational_decoder,classifier,y_distribution,\n",
    "                 **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.encoder = variational_encoder\n",
    "        self.decoder = variational_decoder\n",
    "        self.classifier = classifier  \n",
    "        self.y_distribution = y_distribution\n",
    "        self.N = N_parameter\n",
    "        self.beta = beta\n",
    "    def call(self,inputs):\n",
    "        \"\"\"Inputs is a list, as such:\n",
    "            inputs[0] is labelled X \n",
    "            inputs[1] is labelled y \n",
    "            inputs[2] is labelled c\n",
    "        \"\"\"\n",
    "        \n",
    "        X_labelled = inputs[0]\n",
    "        y_labelled = inputs[1]\n",
    "        c_labelled = inputs[2]\n",
    "        \n",
    "        ############### LABELLED CASE #################\n",
    "        \n",
    "        codings_mean,codings_log_var,codings = self.encoder([X_labelled,y_labelled,c_labelled])\n",
    "        y_pred_mean,y_pred_log_var,y_pred = self.classifier([X_labelled,c_labelled])\n",
    "        reconstructions = self.decoder([codings,y_labelled,c_labelled])\n",
    "\n",
    "        #LOSSES#\n",
    "        recon_loss = labelled_loss_reconstruction(codings_log_var=codings_log_var,x=X_labelled,x_decoded_mean=reconstructions,\n",
    "                                                  codings_mean=codings_mean,beta=self.beta)\n",
    "        cls_loss = labelled_cls_loss(y=y_labelled,y_pred_mean=y_pred_mean,y_pred_log_var=y_pred_log_var,N=self.N)\n",
    "        y_dist_loss1 = self.y_distribution(y_labelled)\n",
    "        labelled_loss = recon_loss + cls_loss + y_dist_loss1\n",
    "\n",
    "        \n",
    "        ############### ALL LOSSES #######################\n",
    "        \n",
    "        loss = labelled_loss\n",
    "        return loss  \n",
    "\n",
    "    \n",
    "\n",
    "def build_model(n_hidden=1, n_neurons=723,input_shape=input_shape,beta=1,n_hidden_classifier=1,\n",
    "              n_neurons_classifier=300,N=30,codings_size=50):\n",
    "    \n",
    "    \"\"\"\n",
    "    Builds deep generative model.\n",
    "    \n",
    "    Parameters specify the architecture. Architecture is such that encoder and decoder have same number of nodes and hidden\n",
    "    layers. Done for simplicity. Classifier has its own architecture.\n",
    "    \n",
    "    Returns encoder,decoder,y_distribution, classifier and overall model. These can be used downstream.\n",
    "    \n",
    "    e.g. variational_encoder,variational_decoder,classifier,y_distribution,model = build_model_mmd(n_hidden=1, n_neurons=723,input_shape=input_shape,beta=1,n_hidden_classifier=1,\n",
    "              n_neurons_classifier=300,N=30,codings_size=50)\n",
    "    \"\"\"\n",
    "       \n",
    "    ########## ENCODER ###############\n",
    "    \n",
    "    x_in = keras.layers.Input(shape=[input_shape])\n",
    "    y_in = keras.layers.Input(shape=[1])\n",
    "    c_in = keras.layers.Input(shape=[2])\n",
    "    z = keras.layers.concatenate([x_in,y_in,c_in])\n",
    "    for layer in range(n_hidden):\n",
    "        z = keras.layers.Dense(n_neurons,activation=\"elu\",kernel_initializer=\"he_normal\")(z)\n",
    "        z = keras.layers.Dropout(0.3)(z)\n",
    "\n",
    "    codings_mean = keras.layers.Dense(codings_size)(z)\n",
    "    codings_log_var = keras.layers.Dense(codings_size)(z)\n",
    "    codings = Sampling()([codings_mean, codings_log_var])\n",
    "    variational_encoder = keras.models.Model(\n",
    "        inputs=[x_in,y_in,c_in], outputs=[codings_mean, codings_log_var, codings])\n",
    "    \n",
    "    \n",
    "    ########## DECODER ###############\n",
    "\n",
    "    latent = keras.layers.Input(shape=[codings_size])\n",
    "    l_merged = keras.layers.concatenate([latent,y_in,c_in])\n",
    "    x = l_merged\n",
    "    for layer in range(n_hidden):\n",
    "        x = keras.layers.Dense(n_neurons, activation=\"elu\",kernel_initializer=\"he_normal\")(x)\n",
    "        x = keras.layers.Dropout(0.3)(x)\n",
    "    x_out = keras.layers.Dense(input_shape,activation=\"sigmoid\")(x) \n",
    "    variational_decoder = keras.models.Model(inputs=[latent,y_in,c_in], outputs=[x_out])\n",
    "    \n",
    "    \n",
    "    ########### CLASSIFIER ############\n",
    "    \n",
    "    y_classifier = keras.layers.concatenate([x_in,c_in])\n",
    "    for layer in range(n_hidden_classifier):\n",
    "        y_classifier = keras.layers.Dense(n_neurons_classifier, activation=\"elu\",kernel_initializer=\"he_normal\")(y_classifier)\n",
    "        y_classifier = keras.layers.Dropout(rate=0.3)(y_classifier)\n",
    "        \n",
    "    y_pred_mean = keras.layers.Dense(1)(y_classifier) \n",
    "    y_pred_log_var = keras.layers.Dense(1)(y_classifier) \n",
    "    y_pred = Sampling()([y_pred_mean, y_pred_log_var])\n",
    "\n",
    "    classifier = keras.models.Model(inputs=[x_in,c_in], outputs=[y_pred_mean,y_pred_log_var,y_pred])\n",
    "    \n",
    "    \n",
    "    ############ Y DISTRIBUTION #############\n",
    "    \n",
    "    loss = y_dist()(y_in)\n",
    "    y_distribution = keras.models.Model(inputs=[y_in],outputs=[loss])\n",
    "    \n",
    "    \n",
    "    ########## FULL MODEL #############\n",
    "    \n",
    "    model = FullModel(N_parameter=N,beta=beta,variational_encoder=variational_encoder,\n",
    "                  variational_decoder=variational_decoder,classifier=classifier,y_distribution=y_distribution\n",
    "                     )\n",
    "    \n",
    "    return variational_encoder,variational_decoder,classifier,y_distribution,model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loss functions ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def custom_mse(x,x_decoded_mean):\n",
    "    \"\"\"returns column of squared errors. Length of column is number of samples.\"\"\"\n",
    "    diff = (x-x_decoded_mean)**2\n",
    "    return K.sum(diff,axis=-1) /2 \n",
    "\n",
    "def labelled_loss_reconstruction(codings_log_var,codings_mean,x, x_decoded_mean,beta=1):\n",
    "    \"\"\"Labelled data. This is the labelled loss.\"\"\"\n",
    "    recon_loss = custom_mse(x, x_decoded_mean)\n",
    "    kl_loss = - 0.5 * K.sum(1 + codings_log_var - K.square(codings_mean) - K.exp(codings_log_var), axis=-1)\n",
    "    #kl loss gives vector of one value for each sample in batch \n",
    "    return K.mean(recon_loss + beta*kl_loss)\n",
    "\n",
    "\n",
    "def labelled_cls_loss(y,y_pred_mean,y_pred_log_var,N=383): \n",
    "    \"\"\"Labelled data only. This is the loss function for the part concerning learning the distibution q(y|x). \n",
    "        This loss function depends on the mean AND the sigma, hence both are included. This can be derived from\n",
    "        taking the log of the normal distribution pdf.\n",
    "        \n",
    "        N is the number of labelled data points in the training set. It is used with alpha to weight this loss term, so\n",
    "        that the model learns q(y|x) using the labelled training data. \n",
    "    \"\"\"\n",
    "    alpha=0.1*N\n",
    "    sigma = K.exp(y_pred_log_var/2)\n",
    "    diff = (y-y_pred_mean)**2\n",
    "    regression_loss = tf.math.divide(diff,(2*sigma**2))\n",
    "    loss = regression_loss + 0.5*y_pred_log_var\n",
    "    return alpha*K.mean(loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training functions ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "@tf.function\n",
    "def train_step(inputs):\n",
    "    \"\"\"Decorated train_step function which applies a gradient update to the parameters\"\"\"\n",
    "    with tf.GradientTape() as tape:\n",
    "        loss = model(inputs,training=True)\n",
    "        loss = tf.add_n([loss] + model.losses)\n",
    "    gradients = tape.gradient(loss, model.trainable_variables)\n",
    "    optimizer.apply_gradients(zip(gradients, model.trainable_variables))\n",
    "    return loss\n",
    "\n",
    "def fit_model(X_train_la, y_train_la,c_train_la,epochs,X_valid_la, y_valid_la,c_valid_la,\n",
    "              patience,variational_encoder,variational_decoder,\n",
    "             classifier,y_distribution,model,Sampling=Sampling,y_dist=y_dist,batch_size=32,learning_rate=0.001,codings_size=50,\n",
    "             valid_set=True):\n",
    "\n",
    "    \"\"\"\n",
    "    Fits a single model. Gets the validation loss too if valid set exists. \n",
    "    And includes a version of early stopping, given by the patience.\n",
    "    Progress bars are shown too.\n",
    "    Number of epochs are specified by the parameter epochs.\n",
    "    \n",
    "    Need to pass in all the custom components. Maybe could put them in a dictionary for cleanliness.\n",
    "    \n",
    "    Valid set is True or False depending if you have one. If you don't, the model at the end of training is saved.\n",
    "    You must still pass in dummy valid sets even if valid_set=False.\n",
    "    \n",
    "    Returns list of training loss, and the minimum validation loss. It also saves the best encoder, decoder and\n",
    "    regressor so they can be used. \n",
    "    \n",
    "    e.g. usage fit_model(X_train_omics_labelled, train_set_labelled_y, X_train_omics_unlabelled,50,X_valid_omics, valid_set_labelled_y,\n",
    "              10,variational_encoder=variational_encoder,variational_decoder=variational_decoder,\n",
    "             classifier=classifier,y_distribution=y_distribution,model=model,\n",
    "          Sampling=Sampling,y_dist=y_dist,batch_size=32,learning_rate=0.001,codings_size=50,valid_set=True)\n",
    "    \"\"\"\n",
    "    if valid_set is True:\n",
    "    \n",
    "        start = time.time()\n",
    "        history = []\n",
    "        K.clear_session()\n",
    "\n",
    "        @tf.function\n",
    "        def train_step(inputs):\n",
    "            with tf.GradientTape() as tape:\n",
    "                loss = model(inputs,training=True)\n",
    "                loss = tf.add_n([loss] + model.losses)\n",
    "            gradients = tape.gradient(loss, model.trainable_variables)\n",
    "            optimizer.apply_gradients(zip(gradients, model.trainable_variables))\n",
    "            return loss\n",
    "\n",
    "        validation_loss = []\n",
    "        optimizer=keras.optimizers.Adam(learning_rate=learning_rate)\n",
    "        batch_loss = []\n",
    "        batches_per_epoch = int(np.floor((X_train_la.shape[0])/batch_size))\n",
    "\n",
    "        for epoch in range(epochs):\n",
    "\n",
    "                print(\"Epoch {}/{}\".format(epoch,epochs))\n",
    "\n",
    "                for i in range(batches_per_epoch):\n",
    "\n",
    "                    batch_x_la, batch_y_la,batch_c_la= create_batch(\n",
    "                        X_train_la, y_train_la,c_train_la,batch_size)\n",
    "\n",
    "                    inputs = [batch_x_la.to_numpy(),batch_y_la,batch_c_la.to_numpy()]\n",
    "                    loss = train_step(inputs)\n",
    "                    batch_loss.append(loss)\n",
    "                    average_batch_loss = list_average(batch_loss)\n",
    "                    print_status_bar(i*batch_size,X_train_la.shape[0],average_batch_loss)\n",
    "\n",
    "                training_loss_for_epoch = list_average(batch_loss)\n",
    "                batch_loss = []\n",
    "                history.append(training_loss_for_epoch)\n",
    "                val_loss = -validation_log_lik_sampling(y_valid_la,X_valid_la.to_numpy(),c_valid_la.to_numpy(),\n",
    "                                                        variational_decoder=variational_decoder,codings_size=codings_size)\n",
    "\n",
    "                validation_loss.append(val_loss)\n",
    "                print_status_bar_epoch(X_train_la.shape[0]\n",
    "                                 ,(X_train_la.shape[0]),training_loss_for_epoch,val_loss )\n",
    "\n",
    "                #callback for early stopping\n",
    "                if epoch <= patience - 1:\n",
    "\n",
    "                    if epoch == 0:\n",
    "\n",
    "                        variational_encoder.save(\"variational_encoder.h5\")\n",
    "                        variational_decoder.save(\"variational_decoder.h5\")\n",
    "                        classifier.save(\"classifier.h5\")\n",
    "                        y_distribution.save(\"y_distribution.h5\")\n",
    "\n",
    "                    else:\n",
    "                        if all(val_loss<i for i in validation_loss[:-1]) is True:\n",
    "                            variational_encoder.save(\"variational_encoder.h5\")\n",
    "                            variational_decoder.save(\"variational_decoder.h5\")\n",
    "                            classifier.save(\"classifier.h5\")\n",
    "                            y_distribution.save(\"y_distribution.h5\")\n",
    "                #this statement means at least a model is saved. Because if the best model was before epoch > patience-1,\n",
    "                #then the statement below won't save any model, which is undesirable as we need to load a model. \n",
    "\n",
    "                if epoch > patience - 1:\n",
    "\n",
    "                    latest_val_loss = validation_loss[-patience:]\n",
    "                    if all(val_loss<i for i in latest_val_loss[:-2]) is True:\n",
    "                        variational_encoder.save(\"variational_encoder.h5\")\n",
    "                        variational_decoder.save(\"variational_decoder.h5\")\n",
    "                        classifier.save(\"classifier.h5\")\n",
    "                        y_distribution.save(\"y_distribution.h5\")\n",
    "                    if all(i>latest_val_loss[0] for i in latest_val_loss[1:]) is True:\n",
    "                        break     \n",
    "\n",
    "        #load best model#\n",
    "        variational_encoder = keras.models.load_model(\"variational_encoder.h5\", custom_objects={\n",
    "           \"Sampling\": Sampling\n",
    "        })\n",
    "        variational_decoder = keras.models.load_model(\"variational_decoder.h5\")\n",
    "        classifier = keras.models.load_model(\"classifier.h5\", custom_objects={\n",
    "           \"Sampling\": Sampling\n",
    "        })    \n",
    "        y_distribution = keras.models.load_model(\"y_distribution.h5\", custom_objects={\n",
    "           \"y_dist\": y_dist\n",
    "        })    \n",
    "\n",
    "        done = time.time()\n",
    "        elapsed = done-start\n",
    "        print(\"Elapsed/s: \",elapsed)\n",
    "        print(\"Final training loss: \",training_loss_for_epoch)\n",
    "        print(\"best val loss: \", min(validation_loss))\n",
    "        \n",
    "        return history, min(validation_loss)\n",
    "\n",
    "        \n",
    "    else:\n",
    "        \n",
    "        start = time.time()\n",
    "        history = []\n",
    "        K.clear_session()\n",
    "\n",
    "        @tf.function\n",
    "        def train_step(inputs):\n",
    "            with tf.GradientTape() as tape:\n",
    "                loss = model(inputs,training=True)\n",
    "                loss = tf.add_n([loss] + model.losses)\n",
    "            gradients = tape.gradient(loss, model.trainable_variables)\n",
    "            optimizer.apply_gradients(zip(gradients, model.trainable_variables))\n",
    "            return loss\n",
    "        \n",
    "        optimizer=keras.optimizers.Adam(learning_rate=learning_rate)\n",
    "        batch_loss = []\n",
    "        batches_per_epoch = int(np.floor((X_train_la.shape[0])/batch_size))\n",
    "        val_loss = 0\n",
    "\n",
    "        for epoch in range(epochs):\n",
    "\n",
    "                print(\"Epoch {}/{}\".format(epoch,epochs))\n",
    "\n",
    "                for i in range(batches_per_epoch):\n",
    "\n",
    "                    batch_x_la, batch_y_la,batch_c_la= create_batch(\n",
    "                        X_train_la, y_train_la,c_train_la,batch_size)\n",
    "\n",
    "                    inputs = [batch_x_la.to_numpy(),batch_y_la,batch_c_la.to_numpy()]\n",
    "                    loss = train_step(inputs)\n",
    "                    batch_loss.append(loss)\n",
    "                    average_batch_loss = list_average(batch_loss)\n",
    "                    print_status_bar(i*batch_size,X_train_la.shape[0],average_batch_loss)\n",
    "\n",
    "                training_loss_for_epoch = list_average(batch_loss)\n",
    "                batch_loss = []\n",
    "                history.append(training_loss_for_epoch)\n",
    "                print_status_bar_epoch(X_train_la.shape[0]\n",
    "                                 ,(X_train_la.shape[0]),training_loss_for_epoch,val_loss )\n",
    "        \n",
    "\n",
    "        variational_encoder.save(\"variational_encoder.h5\")\n",
    "        variational_decoder.save(\"variational_decoder.h5\")\n",
    "        classifier.save(\"classifier.h5\")\n",
    "        y_distribution.save(\"y_distribution.h5\")\n",
    "        \n",
    "        #load best model#\n",
    "        variational_encoder = keras.models.load_model(\"variational_encoder.h5\", custom_objects={\n",
    "           \"Sampling\": Sampling\n",
    "        })\n",
    "        variational_decoder = keras.models.load_model(\"variational_decoder.h5\")\n",
    "        classifier = keras.models.load_model(\"classifier.h5\", custom_objects={\n",
    "           \"Sampling\": Sampling\n",
    "        })     \n",
    "        y_distribution = keras.models.load_model(\"y_distribution.h5\", custom_objects={\n",
    "           \"y_dist\": y_dist\n",
    "        })    \n",
    "\n",
    "        done = time.time()\n",
    "        elapsed = done-start\n",
    "        print(\"Elapsed/s: \",elapsed)\n",
    "        print(\"Final training loss: \",training_loss_for_epoch)\n",
    "        \n",
    "    \n",
    "        return history\n",
    "\n",
    "\n",
    "def fit_model_search(X_train_la, y_train_la,c_train_la, epochs,X_valid_la, y_valid_la,c_valid_la,\n",
    "              patience,variational_encoder,variational_decoder,\n",
    "             classifier,y_distribution,model,Sampling=Sampling,y_dist=y_dist,batch_size=32,learning_rate=0.001,\n",
    "                    codings_size=50):\n",
    "\n",
    "    \"\"\"\n",
    "    Use for hyperparameter search. \n",
    "    \n",
    "    Fits the model. Gets the validation loss too. And includes a version of early stopping, given by the patience.\n",
    "    Progress bars are shown too.\n",
    "    Number of epochs are specified by the parameter epochs.\n",
    "    \n",
    "    Need to pass in all the custom components. Maybe could put them in a dictionary for cleanliness.\n",
    "    \n",
    "    Returns list of training loss, and the minimum validation loss. It also saves the best encoder, decoder and\n",
    "    regressor so they can be used. \n",
    "    \n",
    "    \"\"\"\n",
    "    \n",
    "    start = time.time()\n",
    "    history = []   \n",
    "       \n",
    "    @tf.function\n",
    "    def train_step(inputs):\n",
    "        with tf.GradientTape() as tape:\n",
    "            loss = model(inputs,training=True)\n",
    "            loss = tf.add_n([loss] + model.losses)\n",
    "        gradients = tape.gradient(loss, model.trainable_variables)\n",
    "        optimizer.apply_gradients(zip(gradients, model.trainable_variables))\n",
    "        return loss\n",
    "    \n",
    "    validation_loss = []\n",
    "    optimizer=keras.optimizers.Adam(learning_rate=learning_rate)\n",
    "    batch_loss = []\n",
    "    batches_per_epoch = int(np.floor((X_train_la.shape[0])/batch_size))\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "\n",
    "            print(\"Epoch {}/{}\".format(epoch,epochs))\n",
    "\n",
    "            for i in range(batches_per_epoch):\n",
    "\n",
    "                batch_x_la, batch_y_la,batch_c_la= create_batch(\n",
    "                    X_train_la, y_train_la,c_train_la,batch_size)\n",
    "\n",
    "                inputs = [batch_x_la.to_numpy(),batch_y_la,batch_c_la.to_numpy()]\n",
    "                loss = train_step(inputs)\n",
    "                batch_loss.append(loss)\n",
    "                average_batch_loss = list_average(batch_loss)\n",
    "                print_status_bar(i*batch_size,X_train_la.shape[0],average_batch_loss)\n",
    "\n",
    "            training_loss_for_epoch = list_average(batch_loss)\n",
    "            batch_loss = []                \n",
    "            history.append(training_loss_for_epoch)            \n",
    "            val_loss = -validation_log_lik_sampling(y_valid_la,X_valid_la.to_numpy(),c_valid_la.to_numpy(),\n",
    "                                                    variational_decoder=variational_decoder,codings_size=codings_size)\n",
    "\n",
    "            validation_loss.append(val_loss)            \n",
    "            print_status_bar_epoch(X_train_la.shape[0] \n",
    "                             ,(X_train_la.shape[0] ),training_loss_for_epoch,val_loss )\n",
    "\n",
    "            #callback for early stopping\n",
    "            \n",
    "            if epoch <= patience - 1:\n",
    "                \n",
    "                if epoch == 0:\n",
    "                \n",
    "                    variational_encoder.save(\"variational_encoder_intermediate.h5\")\n",
    "                    variational_decoder.save(\"variational_decoder_intermediate.h5\")\n",
    "                    classifier.save(\"classifier_intermediate.h5\")\n",
    "                    y_distribution.save(\"y_distribution_intermediate.h5\")\n",
    "                    \n",
    "                else:\n",
    "                    if all(val_loss<i for i in validation_loss[:-1]) is True:\n",
    "                        variational_encoder.save(\"variational_encoder_intermediate.h5\")\n",
    "                        variational_decoder.save(\"variational_decoder_intermediate.h5\")\n",
    "                        classifier.save(\"classifier_intermediate.h5\")\n",
    "                        y_distribution.save(\"y_distribution_intermediate.h5\")\n",
    "            #this statement means at least a model is saved. Because if the best model was before epoch > patience-1,\n",
    "            #then the statement below won't save any model, which is undesirable as we need to load a model. \n",
    "            \n",
    "            if epoch > patience - 1:\n",
    "                                \n",
    "                latest_val_loss = validation_loss[-patience:]\n",
    "                if all(val_loss<i for i in latest_val_loss[:-1]) is True:\n",
    "                    variational_encoder.save(\"variational_encoder_intermediate.h5\")\n",
    "                    variational_decoder.save(\"variational_decoder_intermediate.h5\")\n",
    "                    classifier.save(\"classifier_intermediate.h5\")\n",
    "                    y_distribution.save(\"y_distribution_intermediate.h5\")\n",
    "                if all(i>latest_val_loss[0] for i in latest_val_loss[1:]) is True:\n",
    "                    break     \n",
    "    \n",
    "    #load best model#\n",
    "    variational_encoder = keras.models.load_model(\"variational_encoder_intermediate.h5\", custom_objects={\n",
    "       \"Sampling\": Sampling\n",
    "    })\n",
    "    variational_decoder = keras.models.load_model(\"variational_decoder_intermediate.h5\")\n",
    "    classifier = keras.models.load_model(\"classifier_intermediate.h5\", custom_objects={\n",
    "           \"Sampling\": Sampling\n",
    "        })    \n",
    "    y_distribution = keras.models.load_model(\"y_distribution_intermediate.h5\", custom_objects={\n",
    "       \"y_dist\": y_dist\n",
    "    })    \n",
    "                \n",
    "    done = time.time()\n",
    "    elapsed = done-start\n",
    "    print(\"Elapsed/s: \",elapsed)\n",
    "    print(\"Final training loss: \",training_loss_for_epoch)\n",
    "    print(\"best val loss: \", min(validation_loss))\n",
    "    \n",
    "    return history, min(validation_loss)\n",
    "\n",
    "def hyperparameter_search(param_distribs,epochs,patience,n_iter,X_train_la=X_train_omics_labelled, \n",
    "                          y_train_la=train_set_labelled_y, c_train_la=train_set_labelled_c,\n",
    "                             c_valid_la=valid_set_labelled_c,\n",
    "                          X_valid_la=X_valid_omics, y_valid_la=valid_set_labelled_y):\n",
    "    \n",
    "    \"\"\"\n",
    "    Performs hyperparameter, random search. Assesses performance by determining the score on the validation set. \n",
    "    \n",
    "    Saves best models (encoder, decoder and regressor) and returns these. These can then be used downstream.\n",
    "    \n",
    "    Also returns dictionary of the search results.\n",
    "    \n",
    "    Param_distribs of the form: \n",
    "            param_distribs = {\n",
    "            \"n_hidden\": [1],\n",
    "            \"n_hidden_classifier\": [1],\n",
    "            \"beta\": [1],\n",
    "            \"n_neurons\": randint.rvs(50,1000-49,size=20,random_state=random_state).tolist(),\n",
    "           \"n_neurons_classifier\": randint.rvs(49,1000-49,size=20,random_state=random_state).tolist(),\n",
    "            \"codings_size\": randint.rvs(50,290-50,size=30,random_state=random_state).tolist(),\n",
    "            \"N\" :randint.rvs().tolist(),\n",
    "            \"learning_rate\" : ....\n",
    "            #\"codings_size\": [50]}\n",
    "            \n",
    "    There must be a value for every parameter. If you know the value you want to use, set it in the param_distribs\n",
    "    dictionary.\n",
    "    \n",
    "    Patience must be less than the number of epochs.\n",
    "    \n",
    "    e.g. result,variational_encoder,variational_decoder,classifier,y_distribution =\n",
    "            hyperparameter_search_mmd(param_distribs,500,10,n_iter=10)\n",
    "\n",
    "            \n",
    "    \"\"\"\n",
    "\n",
    "    np.random.seed(42) #needs to be here so that everything that follows is consistent\n",
    "\n",
    "    min_val_loss = []\n",
    "    master = {}\n",
    "\n",
    "    for i in range(n_iter): \n",
    "        K.clear_session()\n",
    "        master[i] = {}\n",
    "        master[i][\"parameters\"] = {}\n",
    "        \n",
    "        N= np.random.choice(param_distribs[\"N\"])\n",
    "        learning_rate= np.random.choice(param_distribs[\"learning_rate\"])\n",
    "        beta= np.random.choice(param_distribs[\"beta\"])\n",
    "        n_neurons =np.random.choice(param_distribs[\"n_neurons\"]) \n",
    "        n_neurons_classifier =np.random.choice(param_distribs[\"n_neurons_classifier\"]) \n",
    "        n_hidden  =np.random.choice(param_distribs[\"n_hidden\"]) \n",
    "        n_hidden_classifier  =np.random.choice(param_distribs[\"n_hidden_classifier\"]) \n",
    "        codings_size =np.random.choice(param_distribs[\"codings_size\"]) \n",
    "       \n",
    "        master[i][\"parameters\"][\"N\"] = N\n",
    "        master[i][\"parameters\"][\"learning_rate\"] = learning_rate\n",
    "        master[i][\"parameters\"][\"beta\"] = beta\n",
    "        master[i][\"parameters\"][\"n_neurons\"] = n_neurons\n",
    "        master[i][\"parameters\"][\"n_neurons_classifier\"] = n_neurons_classifier\n",
    "        master[i][\"parameters\"][\"n_hidden\"] = n_hidden\n",
    "        master[i][\"parameters\"][\"n_hidden_classifier\"] = n_hidden_classifier\n",
    "        master[i][\"parameters\"][\"codings_size\"] = codings_size\n",
    "\n",
    "        \n",
    "        variational_encoder,variational_decoder,classifier,y_distribution,model = build_model(n_hidden=n_hidden,       \n",
    "                                       n_neurons=n_neurons,beta=beta,n_hidden_classifier=n_hidden_classifier,\n",
    "                                        n_neurons_classifier=n_neurons_classifier,N=N,codings_size=codings_size)\n",
    "        \n",
    "                \n",
    "        history,val_loss = fit_model_search(X_train_la=X_train_la, y_train_la=y_train_la, \n",
    "                                epochs=epochs,X_valid_la=X_valid_la, \n",
    "                                 y_valid_la=y_valid_la,patience=patience,variational_encoder=variational_encoder,\n",
    "                                variational_decoder=variational_decoder, classifier=classifier,\n",
    "                                y_distribution=y_distribution,model=model,Sampling=Sampling,y_dist=y_dist,\n",
    "                                            batch_size=32,learning_rate=learning_rate,codings_size=codings_size,\n",
    "                                            \n",
    "                                c_train_la=c_train_la,c_valid_la=c_valid_la\n",
    "                                           )        \n",
    "\n",
    "        master[i][\"val_loss\"] = val_loss\n",
    "        min_val_loss.append(val_loss)\n",
    "\n",
    "        #If val loss is lowest, save this model. \n",
    "        if val_loss <=  min(min_val_loss):\n",
    "            os.rename(\"variational_encoder_intermediate.h5\",\"variational_encoder.h5\")\n",
    "            os.rename(\"variational_decoder_intermediate.h5\",\"variational_decoder.h5\")\n",
    "            os.rename(\"classifier_intermediate.h5\",\"classifier.h5\")\n",
    "            os.rename(\"y_distribution_intermediate.h5\",\"y_distribution.h5\")\n",
    "\n",
    "        print(master)\n",
    "            \n",
    "    #load best model#\n",
    "    variational_encoder = keras.models.load_model(\"variational_encoder.h5\", custom_objects={\n",
    "       \"Sampling\": Sampling\n",
    "    })\n",
    "    variational_decoder = keras.models.load_model(\"variational_decoder.h5\")\n",
    "    classifier = keras.models.load_model(\"classifier.h5\", custom_objects={\n",
    "           \"Sampling\": Sampling\n",
    "        }) \n",
    "    y_distribution = keras.models.load_model(\"y_distribution.h5\", custom_objects={\n",
    "       \"y_dist\": y_dist\n",
    "    })    \n",
    "\n",
    "    result = sorted(master.items(), key=lambda item: item[1][\"val_loss\"])\n",
    "    return result,variational_encoder,variational_decoder,classifier,y_distribution\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hyperparameter Search #"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "param_distribs = {\n",
    "            \"n_hidden\": [1,2],\n",
    "            \"n_hidden_classifier\": [1,2],\n",
    "            \"beta\": [1,10,15],\n",
    "    #\"n_neurons\": [300,500],\n",
    "   # \"n_neurons_classifier\": [50,100,150],\n",
    "            \"n_neurons\": randint.rvs(50,600-49,size=20,random_state=random_state).tolist(),\n",
    "           \"n_neurons_classifier\": randint.rvs(20,120-20,size=20,random_state=random_state).tolist(),\n",
    "            \"codings_size\": randint.rvs(20,290-20,size=30,random_state=random_state).tolist(),\n",
    "   # \"codings_size\": [20,50,70],\n",
    "            \"N\" :[0.1,1,10\n",
    "                 \n",
    "                 \n",
    "            ],\n",
    "            \"learning_rate\" : [0.001,0.0005],\n",
    "            #\"codings_size\": [120,60]\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0/80\n",
      "WARNING:tensorflow:Layer full_model is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because it's dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "320/383 [========================>.....] - Loss for batch: 79.8977WARNING:tensorflow:Layer concatenate_1 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because it's dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "383/383 [==============================] - trainLoss: 79.8977  Val_loss: 24.1627 \n",
      "Epoch 1/80\n",
      "383/383 [==============================] - trainLoss: 36.1987  Val_loss: 22.2070 \n",
      "Epoch 2/80\n",
      "383/383 [==============================] - trainLoss: 24.9521  Val_loss: 20.5597 \n",
      "Epoch 3/80\n",
      "383/383 [==============================] - trainLoss: 19.7999  Val_loss: 19.3432 \n",
      "Epoch 4/80\n",
      "383/383 [==============================] - trainLoss: 17.2666  Val_loss: 18.3373 \n",
      "Epoch 5/80\n",
      "383/383 [==============================] - trainLoss: 15.4706  Val_loss: 17.6391 \n",
      "Epoch 6/80\n",
      "383/383 [==============================] - trainLoss: 14.3393  Val_loss: 17.1207 \n",
      "Epoch 7/80\n",
      "383/383 [==============================] - trainLoss: 13.2417  Val_loss: 16.7186 \n",
      "Epoch 8/80\n",
      "383/383 [==============================] - trainLoss: 12.5334  Val_loss: 16.3880 \n",
      "Epoch 9/80\n",
      "383/383 [==============================] - trainLoss: 11.8572  Val_loss: 16.0581 \n",
      "Epoch 10/80\n",
      "383/383 [==============================] - trainLoss: 11.3439  Val_loss: 15.7923 \n",
      "Epoch 11/80\n",
      "383/383 [==============================] - trainLoss: 11.0066  Val_loss: 15.4993 \n",
      "Epoch 12/80\n",
      "383/383 [==============================] - trainLoss: 10.0730  Val_loss: 15.2406 \n",
      "Epoch 13/80\n",
      "383/383 [==============================] - trainLoss: 9.9324  Val_loss: 15.0226 \n",
      "Epoch 14/80\n",
      "383/383 [==============================] - trainLoss: 9.3073  Val_loss: 14.8592 \n",
      "Epoch 15/80\n",
      "383/383 [==============================] - trainLoss: 8.7999  Val_loss: 14.7857 \n",
      "Epoch 16/80\n",
      "383/383 [==============================] - trainLoss: 8.5583  Val_loss: 14.6866 \n",
      "Epoch 17/80\n",
      "383/383 [==============================] - trainLoss: 8.0025  Val_loss: 14.5890 \n",
      "Epoch 18/80\n",
      "383/383 [==============================] - trainLoss: 7.9359  Val_loss: 14.4328 \n",
      "Epoch 19/80\n",
      "383/383 [==============================] - trainLoss: 7.8160  Val_loss: 14.3220 \n",
      "Epoch 20/80\n",
      "383/383 [==============================] - trainLoss: 7.2556  Val_loss: 14.2082 \n",
      "Epoch 21/80\n",
      "383/383 [==============================] - trainLoss: 7.0255  Val_loss: 14.1852 \n",
      "Epoch 22/80\n",
      "383/383 [==============================] - trainLoss: 6.8987  Val_loss: 14.1297 \n",
      "Epoch 23/80\n",
      "383/383 [==============================] - trainLoss: 6.5838  Val_loss: 14.0007 \n",
      "Epoch 24/80\n",
      "383/383 [==============================] - trainLoss: 6.5291  Val_loss: 13.9240 \n",
      "Epoch 25/80\n",
      "383/383 [==============================] - trainLoss: 6.2096  Val_loss: 13.8898 \n",
      "Epoch 26/80\n",
      "383/383 [==============================] - trainLoss: 5.9034  Val_loss: 13.8881 \n",
      "Epoch 27/80\n",
      "383/383 [==============================] - trainLoss: 5.9043  Val_loss: 13.8357 \n",
      "Epoch 28/80\n",
      "383/383 [==============================] - trainLoss: 5.5477  Val_loss: 13.7378 \n",
      "Epoch 29/80\n",
      "383/383 [==============================] - trainLoss: 5.5435  Val_loss: 13.6728 \n",
      "Epoch 30/80\n",
      "383/383 [==============================] - trainLoss: 5.3820  Val_loss: 13.6280 \n",
      "Epoch 31/80\n",
      "383/383 [==============================] - trainLoss: 5.2554  Val_loss: 13.6339 \n",
      "Epoch 32/80\n",
      "383/383 [==============================] - trainLoss: 4.8170  Val_loss: 13.6546 \n",
      "Epoch 33/80\n",
      "383/383 [==============================] - trainLoss: 4.9435  Val_loss: 13.5851 \n",
      "Epoch 34/80\n",
      "383/383 [==============================] - trainLoss: 4.9025  Val_loss: 13.5342 \n",
      "Epoch 35/80\n",
      "383/383 [==============================] - trainLoss: 4.6913  Val_loss: 13.5010 \n",
      "Epoch 36/80\n",
      "383/383 [==============================] - trainLoss: 4.6005  Val_loss: 13.4660 \n",
      "Epoch 37/80\n",
      "383/383 [==============================] - trainLoss: 4.4802  Val_loss: 13.4329 \n",
      "Epoch 38/80\n",
      "383/383 [==============================] - trainLoss: 4.3539  Val_loss: 13.3734 \n",
      "Epoch 39/80\n",
      "383/383 [==============================] - trainLoss: 4.2571  Val_loss: 13.4169 \n",
      "Epoch 40/80\n",
      "383/383 [==============================] - trainLoss: 4.2902  Val_loss: 13.3824 \n",
      "Epoch 41/80\n",
      "383/383 [==============================] - trainLoss: 4.1758  Val_loss: 13.3550 \n",
      "Epoch 42/80\n",
      "383/383 [==============================] - trainLoss: 4.1040  Val_loss: 13.3292 \n",
      "Epoch 43/80\n",
      "383/383 [==============================] - trainLoss: 3.9678  Val_loss: 13.3965 \n",
      "Epoch 44/80\n",
      "383/383 [==============================] - trainLoss: 3.7416  Val_loss: 13.4200 \n",
      "Epoch 45/80\n",
      "383/383 [==============================] - trainLoss: 3.7038  Val_loss: 13.3694 \n",
      "Epoch 46/80\n",
      "383/383 [==============================] - trainLoss: 3.5057  Val_loss: 13.3570 \n",
      "Epoch 47/80\n",
      "383/383 [==============================] - trainLoss: 3.3622  Val_loss: 13.3736 \n",
      "Epoch 48/80\n",
      "383/383 [==============================] - trainLoss: 3.5896  Val_loss: 13.3581 \n",
      "Epoch 49/80\n",
      "383/383 [==============================] - trainLoss: 3.4284  Val_loss: 13.3165 \n",
      "Epoch 50/80\n",
      "383/383 [==============================] - trainLoss: 3.4151  Val_loss: 13.3004 \n",
      "Epoch 51/80\n",
      "383/383 [==============================] - trainLoss: 3.2629  Val_loss: 13.2128 \n",
      "Epoch 52/80\n",
      "383/383 [==============================] - trainLoss: 3.2390  Val_loss: 13.1745 \n",
      "Epoch 53/80\n",
      "383/383 [==============================] - trainLoss: 3.2606  Val_loss: 13.2465 \n",
      "Epoch 54/80\n",
      "383/383 [==============================] - trainLoss: 3.2487  Val_loss: 13.3083 \n",
      "Epoch 55/80\n",
      "383/383 [==============================] - trainLoss: 3.1274  Val_loss: 13.3750 \n",
      "Epoch 56/80\n",
      "383/383 [==============================] - trainLoss: 3.0621  Val_loss: 13.3115 \n",
      "Epoch 57/80\n",
      "383/383 [==============================] - trainLoss: 2.9449  Val_loss: 13.1783 \n",
      "Epoch 58/80\n",
      "383/383 [==============================] - trainLoss: 3.0610  Val_loss: 13.1232 \n",
      "Epoch 59/80\n",
      "383/383 [==============================] - trainLoss: 2.9922  Val_loss: 13.1406 \n",
      "Epoch 60/80\n",
      "383/383 [==============================] - trainLoss: 2.8742  Val_loss: 13.1501 \n",
      "Epoch 61/80\n",
      "383/383 [==============================] - trainLoss: 2.8400  Val_loss: 13.1559 \n",
      "Epoch 62/80\n",
      "383/383 [==============================] - trainLoss: 2.7427  Val_loss: 13.1846 \n",
      "Epoch 63/80\n",
      "383/383 [==============================] - trainLoss: 2.8343  Val_loss: 13.2187 \n",
      "Epoch 64/80\n",
      "383/383 [==============================] - trainLoss: 2.5719  Val_loss: 13.2150 \n",
      "Epoch 65/80\n",
      "383/383 [==============================] - trainLoss: 2.7468  Val_loss: 13.1330 \n",
      "Epoch 66/80\n",
      "383/383 [==============================] - trainLoss: 2.6820  Val_loss: 13.0622 \n",
      "Epoch 67/80\n",
      "383/383 [==============================] - trainLoss: 2.7773  Val_loss: 13.0653 \n",
      "Epoch 68/80\n",
      "383/383 [==============================] - trainLoss: 2.5746  Val_loss: 13.0698 \n",
      "Epoch 69/80\n",
      "383/383 [==============================] - trainLoss: 2.5442  Val_loss: 13.0450 \n",
      "Epoch 70/80\n",
      "383/383 [==============================] - trainLoss: 2.4903  Val_loss: 13.0047 \n",
      "Epoch 71/80\n",
      "383/383 [==============================] - trainLoss: 2.5043  Val_loss: 13.0185 \n",
      "Epoch 72/80\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "383/383 [==============================] - trainLoss: 2.3550  Val_loss: 13.0279 \n",
      "Epoch 73/80\n",
      "383/383 [==============================] - trainLoss: 2.3296  Val_loss: 13.0977 \n",
      "Epoch 74/80\n",
      "383/383 [==============================] - trainLoss: 2.3722  Val_loss: 13.1333 \n",
      "Epoch 75/80\n",
      "383/383 [==============================] - trainLoss: 2.2489  Val_loss: 13.1284 \n",
      "Epoch 76/80\n",
      "383/383 [==============================] - trainLoss: 2.2444  Val_loss: 13.1460 \n",
      "Epoch 77/80\n",
      "383/383 [==============================] - trainLoss: 2.0861  Val_loss: 13.2129 \n",
      "Epoch 78/80\n",
      "383/383 [==============================] - trainLoss: 2.2374  Val_loss: 13.1894 \n",
      "Epoch 79/80\n",
      "383/383 [==============================] - trainLoss: 2.1428  Val_loss: 13.1225 \n",
      "WARNING:tensorflow:No training configuration found in save file: the model was *not* compiled. Compile it manually.\n",
      "WARNING:tensorflow:No training configuration found in save file: the model was *not* compiled. Compile it manually.\n",
      "WARNING:tensorflow:No training configuration found in save file: the model was *not* compiled. Compile it manually.\n",
      "WARNING:tensorflow:No training configuration found in save file: the model was *not* compiled. Compile it manually.\n",
      "Elapsed/s:  62.3584668636322\n",
      "Final training loss:  tf.Tensor(2.14285, shape=(), dtype=float32)\n",
      "best val loss:  tf.Tensor(13.004663, shape=(), dtype=float32)\n",
      "{0: {'parameters': {'codings_size': 169, 'beta': 1, 'n_neurons': 137, 'learning_rate': 0.0005, 'n_hidden': 2, 'n_hidden_classifier': 1, 'n_neurons_classifier': 72, 'N': 10.0}, 'val_loss': <tf.Tensor: shape=(), dtype=float32, numpy=13.004663>}}\n",
      "Epoch 0/80\n",
      "WARNING:tensorflow:Layer full_model is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because it's dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "320/383 [========================>.....] - Loss for batch: 524.3896WARNING:tensorflow:Layer concatenate_1 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because it's dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "383/383 [==============================] - trainLoss: 524.3896  Val_loss: 22.6959 \n",
      "Epoch 1/80\n",
      "383/383 [==============================] - trainLoss: 165.9093  Val_loss: 19.6233 \n",
      "Epoch 2/80\n",
      "383/383 [==============================] - trainLoss: 109.7104  Val_loss: 17.9681 \n",
      "Epoch 3/80\n",
      "383/383 [==============================] - trainLoss: 86.0622  Val_loss: 17.0921 \n",
      "Epoch 4/80\n",
      "383/383 [==============================] - trainLoss: 68.5776  Val_loss: 16.5026 \n",
      "Epoch 5/80\n",
      "383/383 [==============================] - trainLoss: 65.0013  Val_loss: 15.9010 \n",
      "Epoch 6/80\n",
      "383/383 [==============================] - trainLoss: 56.4443  Val_loss: 15.4117 \n",
      "Epoch 7/80\n",
      "383/383 [==============================] - trainLoss: 51.0549  Val_loss: 15.0438 \n",
      "Epoch 8/80\n",
      "383/383 [==============================] - trainLoss: 48.5640  Val_loss: 14.7588 \n",
      "Epoch 9/80\n",
      "383/383 [==============================] - trainLoss: 44.4712  Val_loss: 14.4705 \n",
      "Epoch 10/80\n",
      "383/383 [==============================] - trainLoss: 41.4438  Val_loss: 14.2521 \n",
      "Epoch 11/80\n",
      "383/383 [==============================] - trainLoss: 39.5398  Val_loss: 14.0609 \n",
      "Epoch 12/80\n",
      "383/383 [==============================] - trainLoss: 37.1937  Val_loss: 13.9689 \n",
      "Epoch 13/80\n",
      "383/383 [==============================] - trainLoss: 33.3959  Val_loss: 13.8284 \n",
      "Epoch 14/80\n",
      "383/383 [==============================] - trainLoss: 31.7648  Val_loss: 13.6620 \n",
      "Epoch 15/80\n",
      "383/383 [==============================] - trainLoss: 30.2794  Val_loss: 13.6410 \n",
      "Epoch 16/80\n",
      "383/383 [==============================] - trainLoss: 27.7488  Val_loss: 13.6458 \n",
      "Epoch 17/80\n",
      "383/383 [==============================] - trainLoss: 26.3460  Val_loss: 13.5485 \n",
      "Epoch 18/80\n",
      "383/383 [==============================] - trainLoss: 25.3539  Val_loss: 13.4501 \n",
      "Epoch 19/80\n",
      "383/383 [==============================] - trainLoss: 23.7949  Val_loss: 13.4569 \n",
      "Epoch 20/80\n",
      "383/383 [==============================] - trainLoss: 22.8393  Val_loss: 13.4537 \n",
      "Epoch 21/80\n",
      "383/383 [==============================] - trainLoss: 21.9204  Val_loss: 13.3623 \n",
      "Epoch 22/80\n",
      "383/383 [==============================] - trainLoss: 21.3337  Val_loss: 13.3463 \n",
      "Epoch 23/80\n",
      "383/383 [==============================] - trainLoss: 21.2980  Val_loss: 13.2917 \n",
      "Epoch 24/80\n",
      "383/383 [==============================] - trainLoss: 19.7632  Val_loss: 13.1673 \n",
      "Epoch 25/80\n",
      "383/383 [==============================] - trainLoss: 18.7671  Val_loss: 13.2686 \n",
      "Epoch 26/80\n",
      "383/383 [==============================] - trainLoss: 17.7891  Val_loss: 13.3494 \n",
      "Epoch 27/80\n",
      "383/383 [==============================] - trainLoss: 17.6789  Val_loss: 13.3431 \n",
      "Epoch 28/80\n",
      "383/383 [==============================] - trainLoss: 16.3821  Val_loss: 13.1679 \n",
      "Epoch 29/80\n",
      "383/383 [==============================] - trainLoss: 16.0881  Val_loss: 13.0855 \n",
      "Epoch 30/80\n",
      "383/383 [==============================] - trainLoss: 15.4995  Val_loss: 13.2144 \n",
      "Epoch 31/80\n",
      "383/383 [==============================] - trainLoss: 14.7739  Val_loss: 13.1678 \n",
      "Epoch 32/80\n",
      "383/383 [==============================] - trainLoss: 14.4753  Val_loss: 13.0686 \n",
      "Epoch 33/80\n",
      "383/383 [==============================] - trainLoss: 13.4140  Val_loss: 13.1001 \n",
      "Epoch 34/80\n",
      "383/383 [==============================] - trainLoss: 13.3143  Val_loss: 13.1479 \n",
      "Epoch 35/80\n",
      "383/383 [==============================] - trainLoss: 13.3616  Val_loss: 13.1305 \n",
      "Epoch 36/80\n",
      "383/383 [==============================] - trainLoss: 12.4705  Val_loss: 13.1391 \n",
      "Epoch 37/80\n",
      "383/383 [==============================] - trainLoss: 12.1646  Val_loss: 12.9563 \n",
      "Epoch 38/80\n",
      "383/383 [==============================] - trainLoss: 11.9766  Val_loss: 13.1744 \n",
      "Epoch 39/80\n",
      "383/383 [==============================] - trainLoss: 11.6411  Val_loss: 13.1104 \n",
      "Epoch 40/80\n",
      "383/383 [==============================] - trainLoss: 11.0248  Val_loss: 12.9844 \n",
      "Epoch 41/80\n",
      "383/383 [==============================] - trainLoss: 11.1237  Val_loss: 13.0929 \n",
      "Epoch 42/80\n",
      "383/383 [==============================] - trainLoss: 10.4949  Val_loss: 13.1511 \n",
      "Epoch 43/80\n",
      "383/383 [==============================] - trainLoss: 10.5191  Val_loss: 12.9208 \n",
      "Epoch 44/80\n",
      "383/383 [==============================] - trainLoss: 10.1177  Val_loss: 12.9765 \n",
      "Epoch 45/80\n",
      "383/383 [==============================] - trainLoss: 9.7480  Val_loss: 13.0377 \n",
      "Epoch 46/80\n",
      "383/383 [==============================] - trainLoss: 9.4355  Val_loss: 13.1430 \n",
      "Epoch 47/80\n",
      "383/383 [==============================] - trainLoss: 9.5036  Val_loss: 13.1652 \n",
      "Epoch 48/80\n",
      "383/383 [==============================] - trainLoss: 9.0586  Val_loss: 12.9718 \n",
      "Epoch 49/80\n",
      "383/383 [==============================] - trainLoss: 8.7082  Val_loss: 12.9940 \n",
      "Epoch 50/80\n",
      "383/383 [==============================] - trainLoss: 8.7176  Val_loss: 12.9864 \n",
      "Epoch 51/80\n",
      "383/383 [==============================] - trainLoss: 8.6506  Val_loss: 12.9404 \n",
      "Epoch 52/80\n",
      "383/383 [==============================] - trainLoss: 8.0118  Val_loss: 13.1909 \n",
      "WARNING:tensorflow:No training configuration found in save file: the model was *not* compiled. Compile it manually.\n",
      "WARNING:tensorflow:No training configuration found in save file: the model was *not* compiled. Compile it manually.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:No training configuration found in save file: the model was *not* compiled. Compile it manually.\n",
      "WARNING:tensorflow:No training configuration found in save file: the model was *not* compiled. Compile it manually.\n",
      "Elapsed/s:  42.21155095100403\n",
      "Final training loss:  tf.Tensor(8.011768, shape=(), dtype=float32)\n",
      "best val loss:  tf.Tensor(12.920776, shape=(), dtype=float32)\n",
      "{0: {'parameters': {'codings_size': 169, 'beta': 1, 'n_neurons': 137, 'learning_rate': 0.0005, 'n_hidden': 2, 'n_hidden_classifier': 1, 'n_neurons_classifier': 72, 'N': 10.0}, 'val_loss': <tf.Tensor: shape=(), dtype=float32, numpy=13.004663>}, 1: {'parameters': {'codings_size': 177, 'beta': 10, 'n_neurons': 152, 'learning_rate': 0.001, 'n_hidden': 2, 'n_hidden_classifier': 2, 'n_neurons_classifier': 40, 'N': 1.0}, 'val_loss': <tf.Tensor: shape=(), dtype=float32, numpy=12.920776>}}\n",
      "Epoch 0/80\n",
      "WARNING:tensorflow:Layer full_model is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because it's dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "320/383 [========================>.....] - Loss for batch: 307.0144WARNING:tensorflow:Layer concatenate_1 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because it's dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "383/383 [==============================] - trainLoss: 307.0144  Val_loss: 25.0334 \n",
      "Epoch 1/80\n",
      "383/383 [==============================] - trainLoss: 87.2553  Val_loss: 21.9813 \n",
      "Epoch 2/80\n",
      "383/383 [==============================] - trainLoss: 62.1521  Val_loss: 20.6658 \n",
      "Epoch 3/80\n",
      "383/383 [==============================] - trainLoss: 51.8780  Val_loss: 19.7215 \n",
      "Epoch 4/80\n",
      "383/383 [==============================] - trainLoss: 45.6463  Val_loss: 18.8350 \n",
      "Epoch 5/80\n",
      "383/383 [==============================] - trainLoss: 42.0577  Val_loss: 18.0810 \n",
      "Epoch 6/80\n",
      "383/383 [==============================] - trainLoss: 39.0582  Val_loss: 17.4441 \n",
      "Epoch 7/80\n",
      "383/383 [==============================] - trainLoss: 36.5668  Val_loss: 16.8328 \n",
      "Epoch 8/80\n",
      "383/383 [==============================] - trainLoss: 33.7754  Val_loss: 16.2641 \n",
      "Epoch 9/80\n",
      "383/383 [==============================] - trainLoss: 32.9851  Val_loss: 15.8725 \n",
      "Epoch 10/80\n",
      "383/383 [==============================] - trainLoss: 30.1765  Val_loss: 15.5575 \n",
      "Epoch 11/80\n",
      "383/383 [==============================] - trainLoss: 29.0788  Val_loss: 15.2826 \n",
      "Epoch 12/80\n",
      "383/383 [==============================] - trainLoss: 27.2375  Val_loss: 15.0265 \n",
      "Epoch 13/80\n",
      "383/383 [==============================] - trainLoss: 26.3832  Val_loss: 14.7564 \n",
      "Epoch 14/80\n",
      "383/383 [==============================] - trainLoss: 24.9943  Val_loss: 14.5730 \n",
      "Epoch 15/80\n",
      "383/383 [==============================] - trainLoss: 24.1536  Val_loss: 14.3227 \n",
      "Epoch 16/80\n",
      "383/383 [==============================] - trainLoss: 22.8963  Val_loss: 14.1803 \n",
      "Epoch 17/80\n",
      "383/383 [==============================] - trainLoss: 22.3735  Val_loss: 13.9884 \n",
      "Epoch 18/80\n",
      "383/383 [==============================] - trainLoss: 21.3082  Val_loss: 13.9670 \n",
      "Epoch 19/80\n",
      "383/383 [==============================] - trainLoss: 21.3274  Val_loss: 13.9910 \n",
      "Epoch 20/80\n",
      "383/383 [==============================] - trainLoss: 19.8018  Val_loss: 13.9875 \n",
      "Epoch 21/80\n",
      "383/383 [==============================] - trainLoss: 19.3055  Val_loss: 13.9134 \n",
      "Epoch 22/80\n",
      "383/383 [==============================] - trainLoss: 18.7779  Val_loss: 13.6659 \n",
      "Epoch 23/80\n",
      "383/383 [==============================] - trainLoss: 18.3442  Val_loss: 13.5263 \n",
      "Epoch 24/80\n",
      "383/383 [==============================] - trainLoss: 16.7545  Val_loss: 13.5322 \n",
      "Epoch 25/80\n",
      "383/383 [==============================] - trainLoss: 16.5303  Val_loss: 13.5821 \n",
      "Epoch 26/80\n",
      "383/383 [==============================] - trainLoss: 16.6199  Val_loss: 13.5302 \n",
      "Epoch 27/80\n",
      "383/383 [==============================] - trainLoss: 15.1413  Val_loss: 13.4696 \n",
      "Epoch 28/80\n",
      "383/383 [==============================] - trainLoss: 14.7891  Val_loss: 13.4076 \n",
      "Epoch 29/80\n",
      "383/383 [==============================] - trainLoss: 14.5109  Val_loss: 13.4264 \n",
      "Epoch 30/80\n",
      "383/383 [==============================] - trainLoss: 13.9134  Val_loss: 13.4245 \n",
      "Epoch 31/80\n",
      "383/383 [==============================] - trainLoss: 13.4710  Val_loss: 13.3834 \n",
      "Epoch 32/80\n",
      "383/383 [==============================] - trainLoss: 13.7935  Val_loss: 13.2887 \n",
      "Epoch 33/80\n",
      "383/383 [==============================] - trainLoss: 13.1868  Val_loss: 13.2800 \n",
      "Epoch 34/80\n",
      "383/383 [==============================] - trainLoss: 12.8157  Val_loss: 13.1975 \n",
      "Epoch 35/80\n",
      "383/383 [==============================] - trainLoss: 12.1897  Val_loss: 13.1842 \n",
      "Epoch 36/80\n",
      "383/383 [==============================] - trainLoss: 11.9672  Val_loss: 13.2454 \n",
      "Epoch 37/80\n",
      "383/383 [==============================] - trainLoss: 11.8884  Val_loss: 13.2420 \n",
      "Epoch 38/80\n",
      "383/383 [==============================] - trainLoss: 11.3512  Val_loss: 13.1678 \n",
      "Epoch 39/80\n",
      "383/383 [==============================] - trainLoss: 11.3204  Val_loss: 13.1244 \n",
      "Epoch 40/80\n",
      "383/383 [==============================] - trainLoss: 11.2635  Val_loss: 13.1125 \n",
      "Epoch 41/80\n",
      "383/383 [==============================] - trainLoss: 10.8870  Val_loss: 13.2714 \n",
      "Epoch 42/80\n",
      "383/383 [==============================] - trainLoss: 10.5994  Val_loss: 13.2694 \n",
      "Epoch 43/80\n",
      "383/383 [==============================] - trainLoss: 10.4338  Val_loss: 13.1592 \n",
      "Epoch 44/80\n",
      "383/383 [==============================] - trainLoss: 9.9091  Val_loss: 12.9977 \n",
      "Epoch 45/80\n",
      "383/383 [==============================] - trainLoss: 9.9583  Val_loss: 12.9311 \n",
      "Epoch 46/80\n",
      "383/383 [==============================] - trainLoss: 9.7297  Val_loss: 12.9394 \n",
      "Epoch 47/80\n",
      "383/383 [==============================] - trainLoss: 9.2613  Val_loss: 12.9880 \n",
      "Epoch 48/80\n",
      "383/383 [==============================] - trainLoss: 9.3023  Val_loss: 12.9631 \n",
      "Epoch 49/80\n",
      "383/383 [==============================] - trainLoss: 9.3638  Val_loss: 12.8997 \n",
      "Epoch 50/80\n",
      "383/383 [==============================] - trainLoss: 9.0037  Val_loss: 12.9318 \n",
      "Epoch 51/80\n",
      "383/383 [==============================] - trainLoss: 8.8728  Val_loss: 12.9667 \n",
      "Epoch 52/80\n",
      "383/383 [==============================] - trainLoss: 8.9217  Val_loss: 12.9332 \n",
      "Epoch 53/80\n",
      "383/383 [==============================] - trainLoss: 8.4417  Val_loss: 12.8957 \n",
      "Epoch 54/80\n",
      "383/383 [==============================] - trainLoss: 8.3280  Val_loss: 13.0397 \n",
      "Epoch 55/80\n",
      "383/383 [==============================] - trainLoss: 8.0343  Val_loss: 13.0263 \n",
      "Epoch 56/80\n",
      "383/383 [==============================] - trainLoss: 7.8747  Val_loss: 12.9042 \n",
      "Epoch 57/80\n",
      "383/383 [==============================] - trainLoss: 7.8923  Val_loss: 12.8633 \n",
      "Epoch 58/80\n",
      "383/383 [==============================] - trainLoss: 7.7942  Val_loss: 12.9441 \n",
      "Epoch 59/80\n",
      "383/383 [==============================] - trainLoss: 7.6658  Val_loss: 12.9582 \n",
      "Epoch 60/80\n",
      "383/383 [==============================] - trainLoss: 7.3278  Val_loss: 12.9868 \n",
      "Epoch 61/80\n",
      "383/383 [==============================] - trainLoss: 7.3166  Val_loss: 12.8279 \n",
      "Epoch 62/80\n",
      "383/383 [==============================] - trainLoss: 7.3367  Val_loss: 12.7797 \n",
      "Epoch 63/80\n",
      "383/383 [==============================] - trainLoss: 6.9763  Val_loss: 12.8471 \n",
      "Epoch 64/80\n",
      "383/383 [==============================] - trainLoss: 7.0522  Val_loss: 12.9092 \n",
      "Epoch 65/80\n",
      "383/383 [==============================] - trainLoss: 6.9210  Val_loss: 12.7806 \n",
      "Epoch 66/80\n",
      "383/383 [==============================] - trainLoss: 6.7959  Val_loss: 12.7329 \n",
      "Epoch 67/80\n",
      "383/383 [==============================] - trainLoss: 6.6357  Val_loss: 12.8041 \n",
      "Epoch 68/80\n",
      "383/383 [==============================] - trainLoss: 6.6484  Val_loss: 12.9243 \n",
      "Epoch 69/80\n",
      "383/383 [==============================] - trainLoss: 6.4760  Val_loss: 12.9656 \n",
      "Epoch 70/80\n",
      "383/383 [==============================] - trainLoss: 6.2668  Val_loss: 12.8870 \n",
      "Epoch 71/80\n",
      "383/383 [==============================] - trainLoss: 6.3934  Val_loss: 12.8581 \n",
      "Epoch 72/80\n",
      "383/383 [==============================] - trainLoss: 6.2495  Val_loss: 12.7622 \n",
      "Epoch 73/80\n",
      "383/383 [==============================] - trainLoss: 6.1088  Val_loss: 12.6494 \n",
      "Epoch 74/80\n",
      "383/383 [==============================] - trainLoss: 5.9468  Val_loss: 12.6473 \n",
      "Epoch 75/80\n",
      "383/383 [==============================] - trainLoss: 5.9657  Val_loss: 12.6724 \n",
      "Epoch 76/80\n",
      "383/383 [==============================] - trainLoss: 5.8496  Val_loss: 12.6747 \n",
      "Epoch 77/80\n",
      "383/383 [==============================] - trainLoss: 5.8076  Val_loss: 12.7760 \n",
      "Epoch 78/80\n",
      "383/383 [==============================] - trainLoss: 5.7730  Val_loss: 12.6389 \n",
      "Epoch 79/80\n",
      "383/383 [==============================] - trainLoss: 5.7038  Val_loss: 12.6034 \n",
      "WARNING:tensorflow:No training configuration found in save file: the model was *not* compiled. Compile it manually.\n",
      "WARNING:tensorflow:No training configuration found in save file: the model was *not* compiled. Compile it manually.\n",
      "WARNING:tensorflow:No training configuration found in save file: the model was *not* compiled. Compile it manually.\n",
      "WARNING:tensorflow:No training configuration found in save file: the model was *not* compiled. Compile it manually.\n",
      "Elapsed/s:  53.08454346656799\n",
      "Final training loss:  tf.Tensor(5.7038045, shape=(), dtype=float32)\n",
      "best val loss:  tf.Tensor(12.603399, shape=(), dtype=float32)\n",
      "{0: {'parameters': {'codings_size': 169, 'beta': 1, 'n_neurons': 137, 'learning_rate': 0.0005, 'n_hidden': 2, 'n_hidden_classifier': 1, 'n_neurons_classifier': 72, 'N': 10.0}, 'val_loss': <tf.Tensor: shape=(), dtype=float32, numpy=13.004663>}, 1: {'parameters': {'codings_size': 177, 'beta': 10, 'n_neurons': 152, 'learning_rate': 0.001, 'n_hidden': 2, 'n_hidden_classifier': 2, 'n_neurons_classifier': 40, 'N': 1.0}, 'val_loss': <tf.Tensor: shape=(), dtype=float32, numpy=12.920776>}, 2: {'parameters': {'codings_size': 149, 'beta': 10, 'n_neurons': 380, 'learning_rate': 0.001, 'n_hidden': 1, 'n_hidden_classifier': 1, 'n_neurons_classifier': 21, 'N': 0.1}, 'val_loss': <tf.Tensor: shape=(), dtype=float32, numpy=12.603399>}}\n",
      "Epoch 0/80\n",
      "WARNING:tensorflow:Layer full_model is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because it's dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "320/383 [========================>.....] - Loss for batch: 445.7355WARNING:tensorflow:Layer concatenate_1 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because it's dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "383/383 [==============================] - trainLoss: 445.7355  Val_loss: 25.6254 \n",
      "Epoch 1/80\n",
      "383/383 [==============================] - trainLoss: 147.8675  Val_loss: 23.4035 \n",
      "Epoch 2/80\n",
      "383/383 [==============================] - trainLoss: 100.0130  Val_loss: 21.8600 \n",
      "Epoch 3/80\n",
      "383/383 [==============================] - trainLoss: 79.6198  Val_loss: 20.7805 \n",
      "Epoch 4/80\n",
      "383/383 [==============================] - trainLoss: 71.6353  Val_loss: 20.0028 \n",
      "Epoch 5/80\n",
      "383/383 [==============================] - trainLoss: 67.9178  Val_loss: 19.5329 \n",
      "Epoch 6/80\n",
      "383/383 [==============================] - trainLoss: 60.9871  Val_loss: 19.1202 \n",
      "Epoch 7/80\n",
      "383/383 [==============================] - trainLoss: 57.1473  Val_loss: 18.6751 \n",
      "Epoch 8/80\n",
      "383/383 [==============================] - trainLoss: 55.4614  Val_loss: 18.3576 \n",
      "Epoch 9/80\n",
      "383/383 [==============================] - trainLoss: 52.5034  Val_loss: 18.0841 \n",
      "Epoch 10/80\n",
      "383/383 [==============================] - trainLoss: 48.9839  Val_loss: 17.7913 \n",
      "Epoch 11/80\n",
      "383/383 [==============================] - trainLoss: 48.0510  Val_loss: 17.5089 \n",
      "Epoch 12/80\n",
      "383/383 [==============================] - trainLoss: 47.7060  Val_loss: 17.3074 \n",
      "Epoch 13/80\n",
      "383/383 [==============================] - trainLoss: 44.2137  Val_loss: 17.0341 \n",
      "Epoch 14/80\n",
      "383/383 [==============================] - trainLoss: 42.4415  Val_loss: 16.7325 \n",
      "Epoch 15/80\n",
      "383/383 [==============================] - trainLoss: 42.3568  Val_loss: 16.4943 \n",
      "Epoch 16/80\n",
      "383/383 [==============================] - trainLoss: 40.1727  Val_loss: 16.1907 \n",
      "Epoch 17/80\n",
      "383/383 [==============================] - trainLoss: 38.6148  Val_loss: 15.9252 \n",
      "Epoch 18/80\n",
      "383/383 [==============================] - trainLoss: 38.7777  Val_loss: 15.7575 \n",
      "Epoch 19/80\n",
      "383/383 [==============================] - trainLoss: 35.6587  Val_loss: 15.5589 \n",
      "Epoch 20/80\n",
      "383/383 [==============================] - trainLoss: 36.1323  Val_loss: 15.3980 \n",
      "Epoch 21/80\n",
      "383/383 [==============================] - trainLoss: 34.4575  Val_loss: 15.2727 \n",
      "Epoch 22/80\n",
      "383/383 [==============================] - trainLoss: 32.4891  Val_loss: 15.1419 \n",
      "Epoch 23/80\n",
      "383/383 [==============================] - trainLoss: 32.3153  Val_loss: 15.0542 \n",
      "Epoch 24/80\n",
      "383/383 [==============================] - trainLoss: 30.9319  Val_loss: 14.9543 \n",
      "Epoch 25/80\n",
      "383/383 [==============================] - trainLoss: 30.1701  Val_loss: 14.8006 \n",
      "Epoch 26/80\n",
      "383/383 [==============================] - trainLoss: 30.9198  Val_loss: 14.7215 \n",
      "Epoch 27/80\n",
      "383/383 [==============================] - trainLoss: 29.6368  Val_loss: 14.5811 \n",
      "Epoch 28/80\n",
      "383/383 [==============================] - trainLoss: 28.3135  Val_loss: 14.4552 \n",
      "Epoch 29/80\n",
      "383/383 [==============================] - trainLoss: 27.3413  Val_loss: 14.3730 \n",
      "Epoch 30/80\n",
      "383/383 [==============================] - trainLoss: 26.9325  Val_loss: 14.3034 \n",
      "Epoch 31/80\n",
      "383/383 [==============================] - trainLoss: 26.1871  Val_loss: 14.2409 \n",
      "Epoch 32/80\n",
      "383/383 [==============================] - trainLoss: 25.2556  Val_loss: 14.2235 \n",
      "Epoch 33/80\n",
      "383/383 [==============================] - trainLoss: 24.9086  Val_loss: 14.1572 \n",
      "Epoch 34/80\n",
      "383/383 [==============================] - trainLoss: 24.8648  Val_loss: 14.0889 \n",
      "Epoch 35/80\n",
      "383/383 [==============================] - trainLoss: 24.2003  Val_loss: 13.9983 \n",
      "Epoch 36/80\n",
      "383/383 [==============================] - trainLoss: 23.3631  Val_loss: 13.9482 \n",
      "Epoch 37/80\n",
      "383/383 [==============================] - trainLoss: 23.2608  Val_loss: 13.9175 \n",
      "Epoch 38/80\n",
      "383/383 [==============================] - trainLoss: 21.9762  Val_loss: 13.8742 \n",
      "Epoch 39/80\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "383/383 [==============================] - trainLoss: 21.6192  Val_loss: 13.9099 \n",
      "Epoch 40/80\n",
      "383/383 [==============================] - trainLoss: 20.6272  Val_loss: 13.9095 \n",
      "Epoch 41/80\n",
      "383/383 [==============================] - trainLoss: 21.0191  Val_loss: 13.8191 \n",
      "Epoch 42/80\n",
      "383/383 [==============================] - trainLoss: 20.0826  Val_loss: 13.7598 \n",
      "Epoch 43/80\n",
      "383/383 [==============================] - trainLoss: 20.0524  Val_loss: 13.7119 \n",
      "Epoch 44/80\n",
      "383/383 [==============================] - trainLoss: 19.8289  Val_loss: 13.6649 \n",
      "Epoch 45/80\n",
      "383/383 [==============================] - trainLoss: 18.9321  Val_loss: 13.6402 \n",
      "Epoch 46/80\n",
      "383/383 [==============================] - trainLoss: 18.7467  Val_loss: 13.6028 \n",
      "Epoch 47/80\n",
      "383/383 [==============================] - trainLoss: 18.8004  Val_loss: 13.6132 \n",
      "Epoch 48/80\n",
      "383/383 [==============================] - trainLoss: 18.4442  Val_loss: 13.6010 \n",
      "Epoch 49/80\n",
      "383/383 [==============================] - trainLoss: 17.8394  Val_loss: 13.6207 \n",
      "Epoch 50/80\n",
      "383/383 [==============================] - trainLoss: 17.4233  Val_loss: 13.6050 \n",
      "Epoch 51/80\n",
      "383/383 [==============================] - trainLoss: 16.9595  Val_loss: 13.5527 \n",
      "Epoch 52/80\n",
      "383/383 [==============================] - trainLoss: 16.2157  Val_loss: 13.4700 \n",
      "Epoch 53/80\n",
      "383/383 [==============================] - trainLoss: 16.4263  Val_loss: 13.4405 \n",
      "Epoch 54/80\n",
      "383/383 [==============================] - trainLoss: 16.5368  Val_loss: 13.4491 \n",
      "Epoch 55/80\n",
      "383/383 [==============================] - trainLoss: 16.3055  Val_loss: 13.3990 \n",
      "Epoch 56/80\n",
      "383/383 [==============================] - trainLoss: 15.3121  Val_loss: 13.3815 \n",
      "Epoch 57/80\n",
      "383/383 [==============================] - trainLoss: 14.9896  Val_loss: 13.3584 \n",
      "Epoch 58/80\n",
      "383/383 [==============================] - trainLoss: 14.9595  Val_loss: 13.3857 \n",
      "Epoch 59/80\n",
      "383/383 [==============================] - trainLoss: 14.6715  Val_loss: 13.4135 \n",
      "Epoch 60/80\n",
      "383/383 [==============================] - trainLoss: 14.2486  Val_loss: 13.4459 \n",
      "Epoch 61/80\n",
      "383/383 [==============================] - trainLoss: 14.5514  Val_loss: 13.4548 \n",
      "Epoch 62/80\n",
      "383/383 [==============================] - trainLoss: 14.3760  Val_loss: 13.4051 \n",
      "Epoch 63/80\n",
      "383/383 [==============================] - trainLoss: 14.0633  Val_loss: 13.3358 \n",
      "Epoch 64/80\n",
      "383/383 [==============================] - trainLoss: 13.6630  Val_loss: 13.2903 \n",
      "Epoch 65/80\n",
      "383/383 [==============================] - trainLoss: 13.6353  Val_loss: 13.2666 \n",
      "Epoch 66/80\n",
      "383/383 [==============================] - trainLoss: 13.1802  Val_loss: 13.2604 \n",
      "Epoch 67/80\n",
      "383/383 [==============================] - trainLoss: 13.0650  Val_loss: 13.2454 \n",
      "Epoch 68/80\n",
      "383/383 [==============================] - trainLoss: 12.7835  Val_loss: 13.2685 \n",
      "Epoch 69/80\n",
      "383/383 [==============================] - trainLoss: 12.5998  Val_loss: 13.2317 \n",
      "Epoch 70/80\n",
      "383/383 [==============================] - trainLoss: 12.3389  Val_loss: 13.1970 \n",
      "Epoch 71/80\n",
      "383/383 [==============================] - trainLoss: 12.2754  Val_loss: 13.1698 \n",
      "Epoch 72/80\n",
      "383/383 [==============================] - trainLoss: 12.2681  Val_loss: 13.1586 \n",
      "Epoch 73/80\n",
      "383/383 [==============================] - trainLoss: 11.7377  Val_loss: 13.1464 \n",
      "Epoch 74/80\n",
      "383/383 [==============================] - trainLoss: 11.9670  Val_loss: 13.1554 \n",
      "Epoch 75/80\n",
      "383/383 [==============================] - trainLoss: 11.5041  Val_loss: 13.1830 \n",
      "Epoch 76/80\n",
      "383/383 [==============================] - trainLoss: 11.3691  Val_loss: 13.1624 \n",
      "Epoch 77/80\n",
      "383/383 [==============================] - trainLoss: 11.1730  Val_loss: 13.1637 \n",
      "Epoch 78/80\n",
      "383/383 [==============================] - trainLoss: 11.2666  Val_loss: 13.1648 \n",
      "Epoch 79/80\n",
      "383/383 [==============================] - trainLoss: 10.8024  Val_loss: 13.1744 \n",
      "WARNING:tensorflow:No training configuration found in save file: the model was *not* compiled. Compile it manually.\n",
      "WARNING:tensorflow:No training configuration found in save file: the model was *not* compiled. Compile it manually.\n",
      "WARNING:tensorflow:No training configuration found in save file: the model was *not* compiled. Compile it manually.\n",
      "WARNING:tensorflow:No training configuration found in save file: the model was *not* compiled. Compile it manually.\n",
      "Elapsed/s:  53.35085320472717\n",
      "Final training loss:  tf.Tensor(10.802425, shape=(), dtype=float32)\n",
      "best val loss:  tf.Tensor(13.14643, shape=(), dtype=float32)\n",
      "{0: {'parameters': {'codings_size': 169, 'beta': 1, 'n_neurons': 137, 'learning_rate': 0.0005, 'n_hidden': 2, 'n_hidden_classifier': 1, 'n_neurons_classifier': 72, 'N': 10.0}, 'val_loss': <tf.Tensor: shape=(), dtype=float32, numpy=13.004663>}, 1: {'parameters': {'codings_size': 177, 'beta': 10, 'n_neurons': 152, 'learning_rate': 0.001, 'n_hidden': 2, 'n_hidden_classifier': 2, 'n_neurons_classifier': 40, 'N': 1.0}, 'val_loss': <tf.Tensor: shape=(), dtype=float32, numpy=12.920776>}, 2: {'parameters': {'codings_size': 149, 'beta': 10, 'n_neurons': 380, 'learning_rate': 0.001, 'n_hidden': 1, 'n_hidden_classifier': 1, 'n_neurons_classifier': 21, 'N': 0.1}, 'val_loss': <tf.Tensor: shape=(), dtype=float32, numpy=12.603399>}, 3: {'parameters': {'codings_size': 149, 'beta': 15, 'n_neurons': 264, 'learning_rate': 0.0005, 'n_hidden': 1, 'n_hidden_classifier': 1, 'n_neurons_classifier': 34, 'N': 1.0}, 'val_loss': <tf.Tensor: shape=(), dtype=float32, numpy=13.14643>}}\n",
      "Epoch 0/80\n",
      "WARNING:tensorflow:Layer full_model is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because it's dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "320/383 [========================>.....] - Loss for batch: 35.5253WARNING:tensorflow:Layer concatenate_1 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because it's dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "383/383 [==============================] - trainLoss: 35.5253  Val_loss: 21.6904 \n",
      "Epoch 1/80\n",
      "383/383 [==============================] - trainLoss: 17.6610  Val_loss: 19.3563 \n",
      "Epoch 2/80\n",
      "383/383 [==============================] - trainLoss: 13.5697  Val_loss: 16.9436 \n",
      "Epoch 3/80\n",
      "383/383 [==============================] - trainLoss: 11.9161  Val_loss: 15.5668 \n",
      "Epoch 4/80\n",
      "383/383 [==============================] - trainLoss: 10.5695  Val_loss: 15.0281 \n",
      "Epoch 5/80\n",
      "383/383 [==============================] - trainLoss: 9.6085  Val_loss: 14.8310 \n",
      "Epoch 6/80\n",
      "383/383 [==============================] - trainLoss: 9.0090  Val_loss: 14.2118 \n",
      "Epoch 7/80\n",
      "383/383 [==============================] - trainLoss: 8.5047  Val_loss: 13.8205 \n",
      "Epoch 8/80\n",
      "383/383 [==============================] - trainLoss: 7.9142  Val_loss: 13.8771 \n",
      "Epoch 9/80\n",
      "383/383 [==============================] - trainLoss: 7.5955  Val_loss: 13.6322 \n",
      "Epoch 10/80\n",
      "383/383 [==============================] - trainLoss: 7.1061  Val_loss: 13.5997 \n",
      "Epoch 11/80\n",
      "383/383 [==============================] - trainLoss: 6.4235  Val_loss: 13.5273 \n",
      "Epoch 12/80\n",
      "383/383 [==============================] - trainLoss: 6.2624  Val_loss: 13.5765 \n",
      "Epoch 13/80\n",
      "383/383 [==============================] - trainLoss: 6.0220  Val_loss: 13.2185 \n",
      "Epoch 14/80\n",
      "383/383 [==============================] - trainLoss: 5.8875  Val_loss: 13.4003 \n",
      "Epoch 15/80\n",
      "383/383 [==============================] - trainLoss: 5.5598  Val_loss: 13.1308 \n",
      "Epoch 16/80\n",
      "383/383 [==============================] - trainLoss: 5.3490  Val_loss: 13.2683 \n",
      "Epoch 17/80\n",
      "383/383 [==============================] - trainLoss: 5.3340  Val_loss: 13.2788 \n",
      "Epoch 18/80\n",
      "383/383 [==============================] - trainLoss: 5.2307  Val_loss: 13.3092 \n",
      "Epoch 19/80\n",
      "383/383 [==============================] - trainLoss: 5.1564  Val_loss: 13.4409 \n",
      "Epoch 20/80\n",
      "383/383 [==============================] - trainLoss: 4.9255  Val_loss: 13.2315 \n",
      "Epoch 21/80\n",
      "383/383 [==============================] - trainLoss: 4.6688  Val_loss: 13.5682 \n",
      "Epoch 22/80\n",
      "383/383 [==============================] - trainLoss: 4.6080  Val_loss: 13.7443 \n",
      "Epoch 23/80\n",
      "383/383 [==============================] - trainLoss: 4.6201  Val_loss: 13.0479 \n",
      "Epoch 24/80\n",
      "383/383 [==============================] - trainLoss: 4.5745  Val_loss: 13.0505 \n",
      "Epoch 25/80\n",
      "383/383 [==============================] - trainLoss: 4.3699  Val_loss: 13.4318 \n",
      "Epoch 26/80\n",
      "383/383 [==============================] - trainLoss: 4.2490  Val_loss: 13.0908 \n",
      "Epoch 27/80\n",
      "383/383 [==============================] - trainLoss: 4.0718  Val_loss: 13.6885 \n",
      "Epoch 28/80\n",
      "383/383 [==============================] - trainLoss: 4.2143  Val_loss: 13.1389 \n",
      "Epoch 29/80\n",
      "383/383 [==============================] - trainLoss: 4.0140  Val_loss: 13.0788 \n",
      "Epoch 30/80\n",
      "383/383 [==============================] - trainLoss: 3.9848  Val_loss: 13.5642 \n",
      "Epoch 31/80\n",
      "383/383 [==============================] - trainLoss: 3.9319  Val_loss: 13.3695 \n",
      "Epoch 32/80\n",
      "383/383 [==============================] - trainLoss: 3.7669  Val_loss: 12.9955 \n",
      "Epoch 33/80\n",
      "383/383 [==============================] - trainLoss: 3.7730  Val_loss: 13.0072 \n",
      "Epoch 34/80\n",
      "383/383 [==============================] - trainLoss: 3.7934  Val_loss: 13.2532 \n",
      "Epoch 35/80\n",
      "383/383 [==============================] - trainLoss: 3.6796  Val_loss: 13.1058 \n",
      "Epoch 36/80\n",
      "383/383 [==============================] - trainLoss: 3.5903  Val_loss: 13.0770 \n",
      "Epoch 37/80\n",
      "383/383 [==============================] - trainLoss: 3.5809  Val_loss: 13.3488 \n",
      "Epoch 38/80\n",
      "383/383 [==============================] - trainLoss: 3.5937  Val_loss: 13.0050 \n",
      "Epoch 39/80\n",
      "383/383 [==============================] - trainLoss: 3.4658  Val_loss: 13.0386 \n",
      "Epoch 40/80\n",
      "383/383 [==============================] - trainLoss: 3.4247  Val_loss: 12.8467 \n",
      "Epoch 41/80\n",
      "383/383 [==============================] - trainLoss: 3.3415  Val_loss: 13.3777 \n",
      "Epoch 42/80\n",
      "383/383 [==============================] - trainLoss: 3.2449  Val_loss: 12.6178 \n",
      "Epoch 43/80\n",
      "383/383 [==============================] - trainLoss: 3.2585  Val_loss: 13.1103 \n",
      "Epoch 44/80\n",
      "383/383 [==============================] - trainLoss: 3.3467  Val_loss: 12.8906 \n",
      "Epoch 45/80\n",
      "383/383 [==============================] - trainLoss: 3.2077  Val_loss: 12.9142 \n",
      "Epoch 46/80\n",
      "383/383 [==============================] - trainLoss: 3.1472  Val_loss: 13.3535 \n",
      "Epoch 47/80\n",
      "383/383 [==============================] - trainLoss: 3.1260  Val_loss: 13.1647 \n",
      "Epoch 48/80\n",
      "383/383 [==============================] - trainLoss: 3.0386  Val_loss: 12.7074 \n",
      "Epoch 49/80\n",
      "383/383 [==============================] - trainLoss: 3.0707  Val_loss: 13.1516 \n",
      "Epoch 50/80\n",
      "383/383 [==============================] - trainLoss: 2.9682  Val_loss: 12.4242 \n",
      "Epoch 51/80\n",
      "383/383 [==============================] - trainLoss: 2.9594  Val_loss: 13.0270 \n",
      "Epoch 52/80\n",
      "383/383 [==============================] - trainLoss: 2.7815  Val_loss: 12.9633 \n",
      "Epoch 53/80\n",
      "383/383 [==============================] - trainLoss: 2.8281  Val_loss: 12.6866 \n",
      "Epoch 54/80\n",
      "383/383 [==============================] - trainLoss: 2.7895  Val_loss: 12.7175 \n",
      "Epoch 55/80\n",
      "383/383 [==============================] - trainLoss: 2.8521  Val_loss: 12.8454 \n",
      "Epoch 56/80\n",
      "383/383 [==============================] - trainLoss: 2.8558  Val_loss: 12.5657 \n",
      "Epoch 57/80\n",
      "383/383 [==============================] - trainLoss: 2.8167  Val_loss: 12.8781 \n",
      "Epoch 58/80\n",
      "383/383 [==============================] - trainLoss: 2.6813  Val_loss: 12.7678 \n",
      "Epoch 59/80\n",
      "383/383 [==============================] - trainLoss: 2.6311  Val_loss: 12.6628 \n",
      "WARNING:tensorflow:No training configuration found in save file: the model was *not* compiled. Compile it manually.\n",
      "WARNING:tensorflow:No training configuration found in save file: the model was *not* compiled. Compile it manually.\n",
      "WARNING:tensorflow:No training configuration found in save file: the model was *not* compiled. Compile it manually.\n",
      "WARNING:tensorflow:No training configuration found in save file: the model was *not* compiled. Compile it manually.\n",
      "Elapsed/s:  41.704099893569946\n",
      "Final training loss:  tf.Tensor(2.6310847, shape=(), dtype=float32)\n",
      "best val loss:  tf.Tensor(12.424155, shape=(), dtype=float32)\n",
      "{0: {'parameters': {'codings_size': 169, 'beta': 1, 'n_neurons': 137, 'learning_rate': 0.0005, 'n_hidden': 2, 'n_hidden_classifier': 1, 'n_neurons_classifier': 72, 'N': 10.0}, 'val_loss': <tf.Tensor: shape=(), dtype=float32, numpy=13.004663>}, 1: {'parameters': {'codings_size': 177, 'beta': 10, 'n_neurons': 152, 'learning_rate': 0.001, 'n_hidden': 2, 'n_hidden_classifier': 2, 'n_neurons_classifier': 40, 'N': 1.0}, 'val_loss': <tf.Tensor: shape=(), dtype=float32, numpy=12.920776>}, 2: {'parameters': {'codings_size': 149, 'beta': 10, 'n_neurons': 380, 'learning_rate': 0.001, 'n_hidden': 1, 'n_hidden_classifier': 1, 'n_neurons_classifier': 21, 'N': 0.1}, 'val_loss': <tf.Tensor: shape=(), dtype=float32, numpy=12.603399>}, 3: {'parameters': {'codings_size': 149, 'beta': 15, 'n_neurons': 264, 'learning_rate': 0.0005, 'n_hidden': 1, 'n_hidden_classifier': 1, 'n_neurons_classifier': 34, 'N': 1.0}, 'val_loss': <tf.Tensor: shape=(), dtype=float32, numpy=13.14643>}, 4: {'parameters': {'codings_size': 34, 'beta': 1, 'n_neurons': 485, 'learning_rate': 0.001, 'n_hidden': 2, 'n_hidden_classifier': 1, 'n_neurons_classifier': 40, 'N': 0.1}, 'val_loss': <tf.Tensor: shape=(), dtype=float32, numpy=12.424155>}}\n",
      "WARNING:tensorflow:No training configuration found in save file: the model was *not* compiled. Compile it manually.\n",
      "WARNING:tensorflow:No training configuration found in save file: the model was *not* compiled. Compile it manually.\n",
      "WARNING:tensorflow:No training configuration found in save file: the model was *not* compiled. Compile it manually.\n",
      "WARNING:tensorflow:No training configuration found in save file: the model was *not* compiled. Compile it manually.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[(4,\n",
       "  {'parameters': {'N': 0.1,\n",
       "    'beta': 1,\n",
       "    'codings_size': 34,\n",
       "    'learning_rate': 0.001,\n",
       "    'n_hidden': 2,\n",
       "    'n_hidden_classifier': 1,\n",
       "    'n_neurons': 485,\n",
       "    'n_neurons_classifier': 40},\n",
       "   'val_loss': <tf.Tensor: shape=(), dtype=float32, numpy=12.424155>}),\n",
       " (2,\n",
       "  {'parameters': {'N': 0.1,\n",
       "    'beta': 10,\n",
       "    'codings_size': 149,\n",
       "    'learning_rate': 0.001,\n",
       "    'n_hidden': 1,\n",
       "    'n_hidden_classifier': 1,\n",
       "    'n_neurons': 380,\n",
       "    'n_neurons_classifier': 21},\n",
       "   'val_loss': <tf.Tensor: shape=(), dtype=float32, numpy=12.603399>}),\n",
       " (1,\n",
       "  {'parameters': {'N': 1.0,\n",
       "    'beta': 10,\n",
       "    'codings_size': 177,\n",
       "    'learning_rate': 0.001,\n",
       "    'n_hidden': 2,\n",
       "    'n_hidden_classifier': 2,\n",
       "    'n_neurons': 152,\n",
       "    'n_neurons_classifier': 40},\n",
       "   'val_loss': <tf.Tensor: shape=(), dtype=float32, numpy=12.920776>}),\n",
       " (0,\n",
       "  {'parameters': {'N': 10.0,\n",
       "    'beta': 1,\n",
       "    'codings_size': 169,\n",
       "    'learning_rate': 0.0005,\n",
       "    'n_hidden': 2,\n",
       "    'n_hidden_classifier': 1,\n",
       "    'n_neurons': 137,\n",
       "    'n_neurons_classifier': 72},\n",
       "   'val_loss': <tf.Tensor: shape=(), dtype=float32, numpy=13.004663>}),\n",
       " (3,\n",
       "  {'parameters': {'N': 1.0,\n",
       "    'beta': 15,\n",
       "    'codings_size': 149,\n",
       "    'learning_rate': 0.0005,\n",
       "    'n_hidden': 1,\n",
       "    'n_hidden_classifier': 1,\n",
       "    'n_neurons': 264,\n",
       "    'n_neurons_classifier': 34},\n",
       "   'val_loss': <tf.Tensor: shape=(), dtype=float32, numpy=13.14643>})]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result,variational_encoder,variational_decoder,classifier,y_distribution = hyperparameter_search(param_distribs=param_distribs,\n",
    "                                                                        epochs=80,patience=10,n_iter=5)\n",
    "result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Best KL-based model with only labelled samples has val nloglik of 12.424155. \n",
    "\n",
    " {'parameters': {'N': 0.1,\n",
    "    'beta': 1,\n",
    "    'codings_size': 34,\n",
    "    'learning_rate': 0.001,\n",
    "    'n_hidden': 2,\n",
    "    'n_hidden_classifier': 1,\n",
    "    'n_neurons': 485,\n",
    "    'n_neurons_classifier': 40},\n",
    "   'val_loss': <tf.Tensor: shape=(), dtype=float32, numpy=12.424155"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Single run # "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "variational_encoder,variational_decoder,classifier,y_distribution,model = build_model(n_hidden=2, n_neurons=485,input_shape=input_shape,beta=1,n_hidden_classifier=1,\n",
    "              n_neurons_classifier=40,N=0.1,codings_size=34)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0/100\n",
      "WARNING:tensorflow:Layer full_model is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because it's dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "320/383 [========================>.....] - Loss for batch: 41.8404WARNING:tensorflow:Layer concatenate_1 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because it's dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "383/383 [==============================] - trainLoss: 41.8404  Val_loss: 21.2401 \n",
      "Epoch 1/100\n",
      "383/383 [==============================] - trainLoss: 19.3902  Val_loss: 19.2463 \n",
      "Epoch 2/100\n",
      "383/383 [==============================] - trainLoss: 14.6143  Val_loss: 17.2830 \n",
      "Epoch 3/100\n",
      "383/383 [==============================] - trainLoss: 11.6654  Val_loss: 15.6559 \n",
      "Epoch 4/100\n",
      "383/383 [==============================] - trainLoss: 10.4054  Val_loss: 15.1692 \n",
      "Epoch 5/100\n",
      "383/383 [==============================] - trainLoss: 9.5562  Val_loss: 14.8097 \n",
      "Epoch 6/100\n",
      "383/383 [==============================] - trainLoss: 8.7385  Val_loss: 14.1093 \n",
      "Epoch 7/100\n",
      "383/383 [==============================] - trainLoss: 8.2016  Val_loss: 14.2167 \n",
      "Epoch 8/100\n",
      "383/383 [==============================] - trainLoss: 7.5271  Val_loss: 13.8937 \n",
      "Epoch 9/100\n",
      "383/383 [==============================] - trainLoss: 7.1215  Val_loss: 14.1218 \n",
      "Epoch 10/100\n",
      "383/383 [==============================] - trainLoss: 6.8274  Val_loss: 13.6603 \n",
      "Epoch 11/100\n",
      "383/383 [==============================] - trainLoss: 6.4796  Val_loss: 14.1159 \n",
      "Epoch 12/100\n",
      "383/383 [==============================] - trainLoss: 6.2715  Val_loss: 13.1857 \n",
      "Epoch 13/100\n",
      "383/383 [==============================] - trainLoss: 5.9635  Val_loss: 13.8156 \n",
      "Epoch 14/100\n",
      "383/383 [==============================] - trainLoss: 5.6978  Val_loss: 13.4856 \n",
      "Epoch 15/100\n",
      "383/383 [==============================] - trainLoss: 5.6779  Val_loss: 13.2441 \n",
      "Epoch 16/100\n",
      "383/383 [==============================] - trainLoss: 5.4323  Val_loss: 13.2489 \n",
      "Epoch 17/100\n",
      "383/383 [==============================] - trainLoss: 5.3574  Val_loss: 13.4721 \n",
      "Epoch 18/100\n",
      "383/383 [==============================] - trainLoss: 5.1160  Val_loss: 13.3581 \n",
      "Epoch 19/100\n",
      "383/383 [==============================] - trainLoss: 5.0365  Val_loss: 13.4404 \n",
      "Epoch 20/100\n",
      "383/383 [==============================] - trainLoss: 4.8791  Val_loss: 13.1007 \n",
      "Epoch 21/100\n",
      "383/383 [==============================] - trainLoss: 4.9329  Val_loss: 13.4570 \n",
      "Epoch 22/100\n",
      "383/383 [==============================] - trainLoss: 4.6724  Val_loss: 13.5106 \n",
      "Epoch 23/100\n",
      "383/383 [==============================] - trainLoss: 4.6318  Val_loss: 13.4486 \n",
      "Epoch 24/100\n",
      "383/383 [==============================] - trainLoss: 4.6671  Val_loss: 13.3086 \n",
      "Epoch 25/100\n",
      "383/383 [==============================] - trainLoss: 4.4346  Val_loss: 13.2299 \n",
      "Epoch 26/100\n",
      "383/383 [==============================] - trainLoss: 4.2742  Val_loss: 13.3516 \n",
      "Epoch 27/100\n",
      "383/383 [==============================] - trainLoss: 4.3890  Val_loss: 13.7432 \n",
      "Epoch 28/100\n",
      "383/383 [==============================] - trainLoss: 4.2178  Val_loss: 13.0260 \n",
      "Epoch 29/100\n",
      "383/383 [==============================] - trainLoss: 4.0965  Val_loss: 13.1989 \n",
      "Epoch 30/100\n",
      "383/383 [==============================] - trainLoss: 4.0941  Val_loss: 13.2335 \n",
      "Epoch 31/100\n",
      "383/383 [==============================] - trainLoss: 4.0250  Val_loss: 13.0266 \n",
      "Epoch 32/100\n",
      "383/383 [==============================] - trainLoss: 3.9305  Val_loss: 13.2547 \n",
      "Epoch 33/100\n",
      "383/383 [==============================] - trainLoss: 3.9612  Val_loss: 13.6801 \n",
      "Epoch 34/100\n",
      "383/383 [==============================] - trainLoss: 3.7054  Val_loss: 12.8758 \n",
      "Epoch 35/100\n",
      "383/383 [==============================] - trainLoss: 3.8928  Val_loss: 13.0686 \n",
      "Epoch 36/100\n",
      "383/383 [==============================] - trainLoss: 3.7398  Val_loss: 13.3439 \n",
      "Epoch 37/100\n",
      "383/383 [==============================] - trainLoss: 3.6650  Val_loss: 13.4584 \n",
      "Epoch 38/100\n",
      "383/383 [==============================] - trainLoss: 3.5418  Val_loss: 12.9629 \n",
      "Epoch 39/100\n",
      "383/383 [==============================] - trainLoss: 3.5307  Val_loss: 13.2393 \n",
      "Epoch 40/100\n",
      "383/383 [==============================] - trainLoss: 3.5480  Val_loss: 12.9777 \n",
      "Epoch 41/100\n",
      "383/383 [==============================] - trainLoss: 3.4804  Val_loss: 13.3576 \n",
      "Epoch 42/100\n",
      "383/383 [==============================] - trainLoss: 3.4291  Val_loss: 13.2830 \n",
      "Epoch 43/100\n",
      "383/383 [==============================] - trainLoss: 3.3274  Val_loss: 12.9025 \n",
      "WARNING:tensorflow:No training configuration found in save file: the model was *not* compiled. Compile it manually.\n",
      "WARNING:tensorflow:No training configuration found in save file: the model was *not* compiled. Compile it manually.\n",
      "WARNING:tensorflow:No training configuration found in save file: the model was *not* compiled. Compile it manually.\n",
      "WARNING:tensorflow:No training configuration found in save file: the model was *not* compiled. Compile it manually.\n",
      "Elapsed/s:  30.485880851745605\n",
      "Final training loss:  tf.Tensor(3.3274317, shape=(), dtype=float32)\n",
      "best val loss:  tf.Tensor(12.875778, shape=(), dtype=float32)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "([<tf.Tensor: shape=(), dtype=float32, numpy=41.840443>,\n",
       "  <tf.Tensor: shape=(), dtype=float32, numpy=19.390163>,\n",
       "  <tf.Tensor: shape=(), dtype=float32, numpy=14.61428>,\n",
       "  <tf.Tensor: shape=(), dtype=float32, numpy=11.665411>,\n",
       "  <tf.Tensor: shape=(), dtype=float32, numpy=10.405419>,\n",
       "  <tf.Tensor: shape=(), dtype=float32, numpy=9.5561695>,\n",
       "  <tf.Tensor: shape=(), dtype=float32, numpy=8.738458>,\n",
       "  <tf.Tensor: shape=(), dtype=float32, numpy=8.201592>,\n",
       "  <tf.Tensor: shape=(), dtype=float32, numpy=7.527142>,\n",
       "  <tf.Tensor: shape=(), dtype=float32, numpy=7.1215496>,\n",
       "  <tf.Tensor: shape=(), dtype=float32, numpy=6.82744>,\n",
       "  <tf.Tensor: shape=(), dtype=float32, numpy=6.4796176>,\n",
       "  <tf.Tensor: shape=(), dtype=float32, numpy=6.2714977>,\n",
       "  <tf.Tensor: shape=(), dtype=float32, numpy=5.9635024>,\n",
       "  <tf.Tensor: shape=(), dtype=float32, numpy=5.697788>,\n",
       "  <tf.Tensor: shape=(), dtype=float32, numpy=5.677863>,\n",
       "  <tf.Tensor: shape=(), dtype=float32, numpy=5.432315>,\n",
       "  <tf.Tensor: shape=(), dtype=float32, numpy=5.357395>,\n",
       "  <tf.Tensor: shape=(), dtype=float32, numpy=5.115958>,\n",
       "  <tf.Tensor: shape=(), dtype=float32, numpy=5.0365415>,\n",
       "  <tf.Tensor: shape=(), dtype=float32, numpy=4.879136>,\n",
       "  <tf.Tensor: shape=(), dtype=float32, numpy=4.9329457>,\n",
       "  <tf.Tensor: shape=(), dtype=float32, numpy=4.6724243>,\n",
       "  <tf.Tensor: shape=(), dtype=float32, numpy=4.6317964>,\n",
       "  <tf.Tensor: shape=(), dtype=float32, numpy=4.6671386>,\n",
       "  <tf.Tensor: shape=(), dtype=float32, numpy=4.4346056>,\n",
       "  <tf.Tensor: shape=(), dtype=float32, numpy=4.274215>,\n",
       "  <tf.Tensor: shape=(), dtype=float32, numpy=4.3890414>,\n",
       "  <tf.Tensor: shape=(), dtype=float32, numpy=4.2177854>,\n",
       "  <tf.Tensor: shape=(), dtype=float32, numpy=4.0965266>,\n",
       "  <tf.Tensor: shape=(), dtype=float32, numpy=4.0940924>,\n",
       "  <tf.Tensor: shape=(), dtype=float32, numpy=4.025016>,\n",
       "  <tf.Tensor: shape=(), dtype=float32, numpy=3.9305131>,\n",
       "  <tf.Tensor: shape=(), dtype=float32, numpy=3.9611712>,\n",
       "  <tf.Tensor: shape=(), dtype=float32, numpy=3.7054403>,\n",
       "  <tf.Tensor: shape=(), dtype=float32, numpy=3.8928423>,\n",
       "  <tf.Tensor: shape=(), dtype=float32, numpy=3.7398255>,\n",
       "  <tf.Tensor: shape=(), dtype=float32, numpy=3.6650019>,\n",
       "  <tf.Tensor: shape=(), dtype=float32, numpy=3.5418265>,\n",
       "  <tf.Tensor: shape=(), dtype=float32, numpy=3.5306838>,\n",
       "  <tf.Tensor: shape=(), dtype=float32, numpy=3.5479882>,\n",
       "  <tf.Tensor: shape=(), dtype=float32, numpy=3.4803972>,\n",
       "  <tf.Tensor: shape=(), dtype=float32, numpy=3.4290535>,\n",
       "  <tf.Tensor: shape=(), dtype=float32, numpy=3.3274317>],\n",
       " <tf.Tensor: shape=(), dtype=float32, numpy=12.875778>)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fit_model(X_train_omics_labelled, train_set_labelled_y,train_set_labelled_c,\n",
    "          100,X_valid_omics, valid_set_labelled_y,valid_set_labelled_c,\n",
    "              10,variational_encoder=variational_encoder,variational_decoder=variational_decoder,\n",
    "             classifier=classifier,y_distribution=y_distribution,model=model,\n",
    "          Sampling=Sampling,y_dist=y_dist,batch_size=32,learning_rate=0.001,valid_set=True,codings_size=34)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test_nlog_lik = tf.Tensor(13.229876, shape=(), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "test_nlog_lik = -validation_log_lik_sampling(test_set_labelled_y,X_test_omics.to_numpy(),test_set_labelled_c.to_numpy(),\n",
    "                                    variational_decoder=variational_decoder,codings_size=34,samples=2000)\n",
    "print(\"test_nlog_lik = \" + str(test_nlog_lik))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
