{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "assert sys.version_info >= (3, 5)\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "assert tf.__version__ >= \"2.0\"\n",
    "import numpy as np\n",
    "import time\n",
    "K = keras.backend\n",
    "import pandas as pd\n",
    "import math\n",
    "import os\n",
    "from sklearn.metrics import r2_score\n",
    "from scipy.stats import uniform,randint,norm\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import scipy.stats\n",
    "from sklearn.preprocessing import MinMaxScaler \n",
    "\n",
    "tf.random.set_seed(42)\n",
    "np.random.seed(42)\n",
    "random_state=42"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_path = 'save_path'\n",
    "os.chdir(save_path)\n",
    "\n",
    "X_train_omics_unlabelled = pd.read_csv(\"X_train_omics_unlabelled.csv\",index_col=0)\n",
    "X_train_omics_labelled = pd.read_csv(\"X_train_omics_labelled.csv\",index_col=0)\n",
    "X_test_omics= pd.read_csv(\"X_test_omics.csv\",index_col=0)\n",
    "X_valid_omics= pd.read_csv(\"X_valid_omics.csv\",index_col=0)\n",
    "features = np.load(\"feature_selection.npy\",allow_pickle=True)\n",
    "\n",
    "train_set_labelled_y= pd.read_csv(\"train_set_labelled_y.csv\",index_col=0)\n",
    "test_set_labelled_y= pd.read_csv(\"test_set_labelled_y.csv\",index_col=0)\n",
    "valid_set_labelled_y= pd.read_csv(\"valid_set_labelled_y.csv\",index_col=0)\n",
    "\n",
    "X_train_omics_unlabelled = X_train_omics_unlabelled[features]\n",
    "X_train_omics_labelled = X_train_omics_labelled[features]\n",
    "X_test_omics = X_test_omics[features]\n",
    "X_valid_omics = X_valid_omics[features]\n",
    "\n",
    "train_set_labelled_c= pd.read_csv(\"train_set_labelled_c.csv\",index_col=0)\n",
    "train_set_unlabelled_c= pd.read_csv(\"train_set_unlabelled_c.csv\",index_col=0)\n",
    "test_set_labelled_c= pd.read_csv(\"test_set_labelled_c.csv\",index_col=0)\n",
    "valid_set_labelled_c= pd.read_csv(\"valid_set_labelled_c.csv\",index_col=0)\n",
    "\n",
    "\n",
    "train_set_labelled_c = train_set_labelled_c[[\"age\",\"male\"]]\n",
    "train_set_unlabelled_c = train_set_unlabelled_c[[\"age\",\"male\"]]\n",
    "test_set_labelled_c = test_set_labelled_c[[\"age\",\"male\"]]\n",
    "valid_set_labelled_c = valid_set_labelled_c[[\"age\",\"male\"]]\n",
    "\n",
    "scaler = MinMaxScaler()\n",
    "train_set_labelled_y = scaler.fit_transform(train_set_labelled_y)\n",
    "valid_set_labelled_y = scaler.transform(valid_set_labelled_y)\n",
    "test_set_labelled_y = scaler.transform(test_set_labelled_y)\n",
    "\n",
    "valid_set_labelled_y[np.where(valid_set_labelled_y >1)] = 1\n",
    "test_set_labelled_y[np.where(test_set_labelled_y >1)] = 1\n",
    "\n",
    "\n",
    "train_set_labelled_c[\"age\"] = scaler.fit_transform(train_set_labelled_c[[\"age\"]])\n",
    "train_set_unlabelled_c[\"age\"] = scaler.transform(train_set_unlabelled_c[[\"age\"]])\n",
    "test_set_labelled_c[\"age\"] = scaler.transform(test_set_labelled_c[[\"age\"]])\n",
    "valid_set_labelled_c[\"age\"] = scaler.transform(valid_set_labelled_c[[\"age\"]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_model_path = 'save_model_path'\n",
    "os.chdir(save_model_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_shape = X_train_omics_labelled.shape[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Custom parts # "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Helpful functions ##"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def validation_log_lik_sampling(y_val,x_val,c_val,variational_decoder,codings_size,samples=200):\n",
    "\n",
    "    \"\"\"\n",
    "    Samples a value of z for the expectation, and calculates something proportional to loglikelihood.\n",
    "    \n",
    "    The more samples of z, the better the MC approximation to loglik, but the longer it takes to compute.\n",
    "    \n",
    "    This is how we do our evaluation on the validation and also test set. \n",
    "    \n",
    "    We look at the ability to generate x given y i.e. loglik(x|y,c)\"\"\"\n",
    "    \n",
    "    x_val_len = len(x_val)\n",
    "    expectation = 0\n",
    "    for i in range(samples):\n",
    "        z = np.random.normal(loc=0,scale=1,size=codings_size*x_val_len).reshape(x_val_len,codings_size)\n",
    "        x_pred = variational_decoder([z,y_val,c_val])\n",
    "        diff = (x_val-x_pred)**2\n",
    "        pdf = K.sum(diff,axis=-1)\n",
    "        pdf = K.exp(-pdf)\n",
    "        expectation += pdf \n",
    "    expectation = expectation / samples\n",
    "    lik = tf.math.log(expectation)\n",
    "    lik = K.mean(lik)    \n",
    "    return lik\n",
    "\n",
    "def create_batch(x_label, y_label, x_unlabel, c_label,c_unlabel, batch_s=32):\n",
    "    '''\n",
    "    Creates batches of labelled and unlabelled data. The total number of points in both batches is equal to batch_s. \n",
    "    Thanks to Omer Nivron for help with this.\n",
    "    \n",
    "    '''\n",
    "    proportion_labelled = x_label.shape[0]/(x_label.shape[0] + x_unlabel.shape[0])\n",
    "    \n",
    "    shape_label = x_label.shape[0]\n",
    "    label_per_batch = int(np.ceil(proportion_labelled*batch_s))\n",
    "    batch_idx_la = np.random.choice(list(range(shape_label)), label_per_batch)\n",
    "    batch_x_la = (x_label.iloc[batch_idx_la, :])\n",
    "    batch_y_la = (y_label[batch_idx_la,:])\n",
    "    batch_c_la = (c_label.iloc[batch_idx_la,:])\n",
    "\n",
    "    \n",
    "    shape_unlabel = x_unlabel.shape[0]\n",
    "    unlabel_per_batch = batch_s - label_per_batch\n",
    "    batch_idx_un = np.random.choice(list(range(shape_unlabel)), unlabel_per_batch)\n",
    "    batch_x_un = (x_unlabel.iloc[batch_idx_un, :])\n",
    "    batch_c_un = (c_unlabel.iloc[batch_idx_un,:])\n",
    "\n",
    "    \n",
    "    del batch_idx_la,batch_idx_un\n",
    "            \n",
    "    return batch_x_la, batch_y_la,batch_x_un,batch_c_la,batch_c_un\n",
    "\n",
    "\n",
    "def progress_bar(iteration, total, size=30):\n",
    "    \"\"\"Progress bar for training\"\"\"\n",
    "    running = iteration < total\n",
    "    c = \">\" if running else \"=\"\n",
    "    p = (size - 1) * iteration // total\n",
    "    fmt = \"{{:-{}d}}/{{}} [{{}}]\".format(len(str(total)))\n",
    "    params = [iteration, total, \"=\" * p + c + \".\" * (size - p - 1)]\n",
    "    return fmt.format(*params)\n",
    "\n",
    "def print_status_bar(iteration, total, loss, metrics=None, size=30):\n",
    "    \"\"\"Status bar for training\"\"\"\n",
    "    metrics = \" - \".join([\"Loss for batch: {:.4f}\".format(loss)])\n",
    "    end = \"\" if iteration < total else \"\\n\"\n",
    "    print(\"\\r{} - {}\".format(progress_bar(iteration, total), metrics), end=end)\n",
    "    \n",
    "def print_status_bar_epoch(iteration, total, training_loss_for_epoch,val_loss, metrics=None, size=30):\n",
    "    \"\"\"Status bar for training (end of epoch)\"\"\"\n",
    "    metrics = \" - \".join(\n",
    "        [\"trainLoss: {:.4f}  Val_loss: {:.4f} \".format(\n",
    "            training_loss_for_epoch,val_loss)]\n",
    "    )\n",
    "    end = \"\" if iteration < total else \"\\n\"\n",
    "    print(\"\\r{} - {}\".format(progress_bar(iteration, total), metrics), end=end)\n",
    "    \n",
    "    \n",
    "def list_average(list_of_loss):\n",
    "    return sum(list_of_loss)/len(list_of_loss)\n",
    " \n",
    "\n",
    "def gaussian_pdf(array,mean,sigma):\n",
    "    part1 = tf.math.divide(tf.constant(np.array(1.0).reshape(1,-1),dtype=\"float32\"),sigma*(2*math.pi)**0.5)\n",
    "    part2 = K.exp(-0.5*tf.math.divide((array-mean),sigma)**2)\n",
    "    return part1*part2\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model ##"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Custom components ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Sampling(keras.layers.Layer):\n",
    "    \"\"\"reparameterization trick\"\"\"\n",
    "    def call(self, inputs):\n",
    "        mean, log_var = inputs\n",
    "        return K.random_normal(tf.shape(log_var)) * K.exp(log_var/2) + mean  \n",
    "    \n",
    "    \n",
    "class y_dist(keras.layers.Layer):\n",
    "\n",
    "    \"\"\"\n",
    "    Custom layer that is used to learn the parameters of the distribution over y.\n",
    "    \n",
    "    Outputs a loss. The loss is used for training. The loss is a GMM loss.\n",
    "    The mean of this is then taken to provide a per batch loss. \n",
    "        \n",
    "    \"\"\"\n",
    "    def __init__(self,**kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        \n",
    "        \n",
    "    def build(self,batch_input_shape):\n",
    "        self.q1 = self.add_weight(name=\"q1\",shape=[1,1],initializer=\"random_normal\",trainable=True)\n",
    "        self.q2 = self.add_weight(name=\"q2\",shape=[1,1],initializer=\"random_normal\",trainable=True)\n",
    "        self.mu1 = self.add_weight(name=\"mu1\",shape=[1,1],initializer=\"random_normal\",trainable=True)\n",
    "        self.mu2 = self.add_weight(name=\"mu2\",shape=[1,1],initializer=\"random_normal\",trainable=True)\n",
    "        self.mu3 = self.add_weight(name=\"mu3\",shape=[1,1],initializer=\"random_normal\",trainable=True)\n",
    "        self.tau1 = self.add_weight(name=\"tau1\",shape=[1,1],initializer=\"random_normal\",trainable=True)\n",
    "        self.tau2 = self.add_weight(name=\"tau2\",shape=[1,1],initializer=\"random_normal\",trainable=True)\n",
    "        self.tau3 = self.add_weight(name=\"tau3\",shape=[1,1],initializer=\"random_normal\",trainable=True)\n",
    "\n",
    "        super().build(batch_input_shape)\n",
    "    \n",
    "    def call(self,X):\n",
    "        concatenated = tf.concat([self.q1,self.q2,tf.constant(np.array(0.0).reshape(1,-1),dtype=\"float32\")],axis=-1)\n",
    "        p = K.exp(concatenated)\n",
    "        p = tf.math.divide(p,K.sum(p))\n",
    "        sigma_concatenated = tf.concat([self.tau1,self.tau2,self.tau3],axis=-1)\n",
    "        sigma = K.exp(sigma_concatenated)\n",
    "        likelihood = p[0][0]*gaussian_pdf(X,mean=self.mu1,sigma=sigma[0][0])+p[0][1]*gaussian_pdf(X,mean=self.mu2,sigma=sigma[0][1])+p[0][2]*gaussian_pdf(X,mean=self.mu3,sigma=sigma[0][2]) \n",
    "        loglik = tf.math.log(likelihood)\n",
    "        loss = -loglik\n",
    "        loss = tf.reduce_mean(loss)\n",
    "        return loss\n",
    "    \n",
    "    def compute_output_shape(self,batch_input_shape):\n",
    "        return tf.TensorShape(1)    \n",
    "    \n",
    "    \n",
    "class FullModel_MMD(keras.models.Model):\n",
    "    \"\"\"\n",
    "    This is the full model. For MMD. This is used for training purposes.\n",
    "    \n",
    "    It requires an encoder, decoder, classifier and y_distribution model to be already defined (as can be done with \n",
    "    the build_model function).\n",
    "    \n",
    "    It returns the nloglik i.e. the loss. \n",
    "    \n",
    "    This loss can then be used in gradient descent and be minimised wrt parameters (of the four component models).\n",
    "    \n",
    "    At test time, you will call which of the component models you want to use (as opposed to trying to \"call\" this \n",
    "    FullModel which you won't want to do as its purpose is just to calculate the nloglik for training).\n",
    "    \n",
    "    \"\"\"\n",
    "    def __init__(self,N_parameter,beta,variational_encoder,variational_decoder,classifier,y_distribution,\n",
    "                 codings_size,**kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.encoder = variational_encoder\n",
    "        self.decoder = variational_decoder\n",
    "        self.classifier = classifier  \n",
    "        self.y_distribution = y_distribution\n",
    "        self.codings_size = codings_size\n",
    "        self.N = N_parameter\n",
    "        self.beta = beta\n",
    "    def call(self,inputs):\n",
    "        \"\"\"Inputs is a list, as such:\n",
    "            inputs[0] is labelled X \n",
    "            inputs[1] is labelled y \n",
    "            inputs[2] is unlabelled X\n",
    "            inputs[3] is labelled c\n",
    "            inputs[4] is unlabelled c\"\"\"\n",
    "        \n",
    "        X_labelled = inputs[0]\n",
    "        y_labelled = inputs[1]\n",
    "        X_unlabelled = inputs[2]\n",
    "        c_labelled = inputs[3]\n",
    "        c_unlabelled = inputs[4]\n",
    "        \n",
    "        ############### LABELLED CASE #################\n",
    "        \n",
    "        codings_mean,codings_log_var,codings = self.encoder([X_labelled,y_labelled,c_labelled])\n",
    "        y_pred_mean,y_pred_log_var,y_pred = self.classifier([X_labelled,c_labelled])\n",
    "        reconstructions = self.decoder([codings,y_labelled,c_labelled])\n",
    "\n",
    "        #LOSSES#\n",
    "        \n",
    "        recon_loss = labelled_loss_reconstruction_mmd(codings=codings,x=X_labelled,x_decoded_mean=reconstructions,\n",
    "                                                      batch_size=32,codings_size=self.codings_size,beta=self.beta)\n",
    "        cls_loss = labelled_cls_loss(y=y_labelled,y_pred_mean=y_pred_mean,y_pred_log_var=y_pred_log_var,N=self.N)\n",
    "        y_dist_loss1 = self.y_distribution(y_labelled)\n",
    "        labelled_loss = recon_loss + cls_loss + y_dist_loss1\n",
    "\n",
    "        ############### UNLABELLED CASE #################\n",
    "        y_pred_mean_unlabel,y_pred_log_var_unlabel,y_pred_unlabel = self.classifier([X_unlabelled,c_unlabelled])\n",
    "        codings_mean,codings_log_var,codings = self.encoder([X_unlabelled,y_pred_unlabel,c_unlabelled])\n",
    "        reconstructions_un = self.decoder([codings,y_pred_unlabel,c_unlabelled])\n",
    "        \n",
    "        #LOSSES#                                  \n",
    "        \n",
    "        unlabelled_recon_loss = unlabelled_loss_reconstruction_mmd(codings=codings,y_pred_log_var=y_pred_log_var_unlabel,\n",
    "                                    y_pred_mean= y_pred_mean_unlabel,y_upper_bound=2,y_lower_bound= -1,                         \n",
    "                                    x=X_unlabelled, x_decoded_mean=reconstructions_un,beta=self.beta,\n",
    "                                    codings_size=self.codings_size,batch_size=32)\n",
    "\n",
    "        y_dist_loss = self.y_distribution(y_pred_unlabel)\n",
    "        unlabelled_loss = unlabelled_recon_loss + y_dist_loss\n",
    "        \n",
    "        ############### ALL LOSSES #######################\n",
    "        \n",
    "        loss = labelled_loss + unlabelled_loss\n",
    "        return loss  \n",
    "\n",
    "    \n",
    "\n",
    "def build_model_mmd(n_hidden=1, n_neurons=723,input_shape=input_shape,beta=1,n_hidden_classifier=1,\n",
    "              n_neurons_classifier=300,N=30,codings_size=50):\n",
    "    \n",
    "    \"\"\"\n",
    "    Builds deep generative model.\n",
    "    \n",
    "    Parameters specify the architecture. Architecture is such that encoder and decoder have same number of nodes and hidden\n",
    "    layers. Done for simplicity. Classifier has its own architecture.\n",
    "    \n",
    "    Returns encoder,decoder,y_distribution, classifier and overall model. These can be used downstream.\n",
    "    \n",
    "    e.g. variational_encoder,variational_decoder,classifier,y_distribution,model = build_model_mmd(n_hidden=1, n_neurons=723,input_shape=input_shape,beta=1,n_hidden_classifier=1,\n",
    "              n_neurons_classifier=300,N=30,codings_size=50)\n",
    "    \"\"\"\n",
    "       \n",
    "    ########## ENCODER ###############\n",
    "    \n",
    "    x_in = keras.layers.Input(shape=[input_shape])\n",
    "    y_in = keras.layers.Input(shape=[1])\n",
    "    c_in = keras.layers.Input(shape=[2])\n",
    "    z = keras.layers.concatenate([x_in,y_in,c_in])\n",
    "    for layer in range(n_hidden):\n",
    "        z = keras.layers.Dense(n_neurons,activation=\"elu\",kernel_initializer=\"he_normal\")(z)\n",
    "        z = keras.layers.Dropout(0.3)(z)\n",
    "\n",
    "    codings_mean = keras.layers.Dense(codings_size)(z)\n",
    "    codings_log_var = keras.layers.Dense(codings_size)(z)\n",
    "    codings = Sampling()([codings_mean, codings_log_var])\n",
    "    variational_encoder = keras.models.Model(\n",
    "        inputs=[x_in,y_in,c_in], outputs=[codings_mean, codings_log_var, codings])\n",
    "    \n",
    "    \n",
    "    ########## DECODER ###############\n",
    "\n",
    "    latent = keras.layers.Input(shape=[codings_size])\n",
    "    l_merged = keras.layers.concatenate([latent,y_in,c_in])\n",
    "    x = l_merged\n",
    "    for layer in range(n_hidden):\n",
    "        x = keras.layers.Dense(n_neurons, activation=\"elu\",kernel_initializer=\"he_normal\")(x)\n",
    "        x = keras.layers.Dropout(0.3)(x)\n",
    "    x_out = keras.layers.Dense(input_shape,activation=\"sigmoid\")(x) \n",
    "    variational_decoder = keras.models.Model(inputs=[latent,y_in,c_in], outputs=[x_out])\n",
    "    \n",
    "    \n",
    "    ########### CLASSIFIER ############\n",
    "    \n",
    "    y_classifier = keras.layers.concatenate([x_in,c_in])\n",
    "    for layer in range(n_hidden_classifier):\n",
    "        y_classifier = keras.layers.Dense(n_neurons_classifier, activation=\"elu\",kernel_initializer=\"he_normal\")(y_classifier)\n",
    "        y_classifier = keras.layers.Dropout(rate=0.3)(y_classifier)\n",
    "        \n",
    "    y_pred_mean = keras.layers.Dense(1)(y_classifier) \n",
    "    y_pred_log_var = keras.layers.Dense(1)(y_classifier) \n",
    "    y_pred = Sampling()([y_pred_mean, y_pred_log_var])\n",
    "\n",
    "    classifier = keras.models.Model(inputs=[x_in,c_in], outputs=[y_pred_mean,y_pred_log_var,y_pred])\n",
    "    \n",
    "    \n",
    "    ############ Y DISTRIBUTION #############\n",
    "    \n",
    "    loss = y_dist()(y_in)\n",
    "    y_distribution = keras.models.Model(inputs=[y_in],outputs=[loss])\n",
    "    \n",
    "    \n",
    "    ########## FULL MODEL #############\n",
    "    \n",
    "    model = FullModel_MMD(N_parameter=N,beta=beta,variational_encoder=variational_encoder,\n",
    "                  variational_decoder=variational_decoder,classifier=classifier,y_distribution=y_distribution,\n",
    "                     codings_size=codings_size)\n",
    "    \n",
    "    return variational_encoder,variational_decoder,classifier,y_distribution,model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loss functions ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def custom_mse(x,x_decoded_mean):\n",
    "    \"\"\"returns column of squared errors. Length of column is number of samples.\"\"\"\n",
    "    diff = (x-x_decoded_mean)**2\n",
    "    return K.sum(diff,axis=-1) /2 \n",
    "\n",
    "def compute_kernel(x, y):\n",
    "    x_size = tf.shape(x)[0]\n",
    "    y_size = tf.shape(y)[0]\n",
    "    dim = tf.shape(x)[1]\n",
    "    tiled_x = tf.tile(tf.reshape(x, tf.stack([x_size, 1, dim])), tf.stack([1, y_size, 1]))\n",
    "    tiled_y = tf.tile(tf.reshape(y, tf.stack([1, y_size, dim])), tf.stack([x_size, 1, 1]))\n",
    "    return tf.exp(-tf.reduce_mean(tf.square(tiled_x - tiled_y), axis=2) / tf.cast(dim, tf.float32))\n",
    "\n",
    "def compute_mmd(x, y, sigma_sqr=1.0):\n",
    "    x_kernel = compute_kernel(x, x)\n",
    "    y_kernel = compute_kernel(y, y)\n",
    "    xy_kernel = compute_kernel(x, y)\n",
    "    return tf.reduce_mean(x_kernel) + tf.reduce_mean(y_kernel) - 2 * tf.reduce_mean(xy_kernel)\n",
    "    #read this for calculations: https://scikit-learn.org/stable/modules/generated/sklearn.gaussian_process.kernels.RBF.html\n",
    "    #https://stats.stackexchange.com/questions/239008/rbf-kernel-algorithm-python\n",
    "    #https://blogs.rstudio.com/ai/posts/2018-10-22-mmd-vae/\n",
    "    #https://ermongroup.github.io/blog/a-tutorial-on-mmd-variational-autoencoders/\n",
    "\n",
    "def labelled_loss_reconstruction_mmd(codings,x,x_decoded_mean,batch_size=32,codings_size=50,beta=1):\n",
    "    recon_loss = custom_mse(x,x_decoded_mean)\n",
    "        # Compare the generated z with true samples from a standard Gaussian, and compute their MMD distance\n",
    "    true_samples = tf.random.normal(tf.stack([batch_size, codings_size]))\n",
    "    loss_mmd = compute_mmd(true_samples, codings)\n",
    "    return K.mean(recon_loss) + beta*loss_mmd\n",
    "    \n",
    "\n",
    "def unlabelled_loss_reconstruction_mmd(x,x_decoded_mean,codings,\n",
    "                                   y_pred_log_var,y_pred_mean,y_upper_bound=2,y_lower_bound=-1,\n",
    "                                  beta=1,codings_size=50,batch_size=32):\n",
    "    \n",
    "    \"\"\"Unlabelled data. This is the reconstruction loss for the unlabelled portion. \n",
    "        The extra loss, dubbed the integral loss, is also specified here. The definition of this function follow below.\"\"\"\n",
    "\n",
    "    recon_loss = custom_mse(x, x_decoded_mean)\n",
    "    true_samples = tf.random.normal(tf.stack([batch_size, codings_size]))\n",
    "    loss_mmd = compute_mmd(true_samples, codings) \n",
    "    #now need to do the expectation with respect to y. Let's just use montecarlo with n=1 for now. Just like we do\n",
    "    #for the expecatation with respect to z\n",
    "    loss = K.mean(recon_loss) + beta*loss_mmd\n",
    "\n",
    "    integral_loss = unlabelled_integral_loss(y_pred_log_var,y_pred_mean,y_upper_bound,y_lower_bound) \n",
    "    #returns vector of one value per sample in batch\n",
    "\n",
    "    return K.mean(loss) + K.mean(integral_loss)\n",
    "\n",
    "def unlabelled_integral_loss(y_pred_log_var,y_pred_mean,y_upper_bound=2, y_lower_bound=-1):\n",
    "    \"\"\"Unlabelled data. This is the loss resulting from the integral that arises in the loss function. I have derived\n",
    "    it in my notes.\"\"\"\n",
    "    sigma = K.exp(y_pred_log_var/2)\n",
    "    term1 = 0.5* tf.math.log(1/(2**0.5 * (math.pi)**0.5*sigma)) * tf.math.erf((y_upper_bound-y_pred_mean)/(2**0.5 *sigma))\n",
    "    term2 = -0.25*tf.math.erf((y_upper_bound-y_pred_mean)/(2**0.5 *sigma))\n",
    "    term3 = (y_upper_bound-y_pred_mean)*K.exp(-((y_upper_bound-y_pred_mean)**2/(2*sigma**2)))/(2**1.5 *math.pi**0.5 *sigma)\n",
    "    part1 = term1 + term2 + term3\n",
    "    \n",
    "    term1 = 0.5* tf.math.log(1/(2**0.5 * (math.pi)**0.5*sigma)) * tf.math.erf((y_lower_bound-y_pred_mean)/(2**0.5 *sigma))\n",
    "    term2 = -0.25*tf.math.erf((y_lower_bound-y_pred_mean)/(2**0.5 *sigma))\n",
    "    term3 = (y_lower_bound-y_pred_mean)*K.exp(-((y_lower_bound-y_pred_mean)**2/(2*sigma**2)))/(2**1.5 *math.pi**0.5 *sigma)\n",
    "    part2 = term1 + term2 + term3\n",
    "    \n",
    "    integral = part1-part2\n",
    "    loss = integral\n",
    "    return loss\n",
    "\n",
    "def labelled_cls_loss(y,y_pred_mean,y_pred_log_var,N=383): \n",
    "    \"\"\"Labelled data only. This is the loss function for the part concerning learning the distibution q(y|x). \n",
    "        This loss function depends on the mean AND the sigma, hence both are included. This can be derived from\n",
    "        taking the log of the normal distribution pdf.\n",
    "        \n",
    "        N is the number of labelled data points in the training set. It is used with alpha to weight this loss term, so\n",
    "        that the model learns q(y|x) using the labelled training data. \n",
    "    \"\"\"\n",
    "    alpha=0.1*N\n",
    "    sigma = K.exp(y_pred_log_var/2)\n",
    "    diff = (y-y_pred_mean)**2\n",
    "    regression_loss = tf.math.divide(diff,(2*sigma**2))\n",
    "    loss = regression_loss + 0.5*y_pred_log_var\n",
    "    return alpha*K.mean(loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training functions ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "@tf.function\n",
    "def train_step(inputs):\n",
    "    \"\"\"Decorated train_step function which applies a gradient update to the parameters\"\"\"\n",
    "    with tf.GradientTape() as tape:\n",
    "        loss = model(inputs,training=True)\n",
    "        loss = tf.add_n([loss] + model.losses)\n",
    "    gradients = tape.gradient(loss, model.trainable_variables)\n",
    "    optimizer.apply_gradients(zip(gradients, model.trainable_variables))\n",
    "    return loss\n",
    "\n",
    "def fit_model(X_train_la, y_train_la, X_train_un,c_train_la,c_train_un,epochs,X_valid_la, y_valid_la,c_valid_la,\n",
    "              patience,variational_encoder,variational_decoder,\n",
    "             classifier,y_distribution,model,Sampling=Sampling,y_dist=y_dist,batch_size=32,learning_rate=0.001,codings_size=50,\n",
    "             valid_set=True):\n",
    "\n",
    "    \"\"\"\n",
    "    Fits a single model. Gets the validation loss too if valid set exists. \n",
    "    And includes a version of early stopping, given by the patience.\n",
    "    Progress bars are shown too.\n",
    "    Number of epochs are specified by the parameter epochs.\n",
    "    \n",
    "    Need to pass in all the custom components. Maybe could put them in a dictionary for cleanliness.\n",
    "    \n",
    "    Valid set is True or False depending if you have one. If you don't, the model at the end of training is saved.\n",
    "    You must still pass in dummy valid sets even if valid_set=False.\n",
    "    \n",
    "    Returns list of training loss, and the minimum validation loss. It also saves the best encoder, decoder and\n",
    "    regressor so they can be used. \n",
    "    \n",
    "    e.g. usage fit_model(X_train_omics_labelled, train_set_labelled_y, X_train_omics_unlabelled,50,X_valid_omics, valid_set_labelled_y,\n",
    "              10,variational_encoder=variational_encoder,variational_decoder=variational_decoder,\n",
    "             classifier=classifier,y_distribution=y_distribution,model=model,\n",
    "          Sampling=Sampling,y_dist=y_dist,batch_size=32,learning_rate=0.001,codings_size=50,valid_set=True)\n",
    "    \"\"\"\n",
    "    if valid_set is True:\n",
    "    \n",
    "        start = time.time()\n",
    "        history = []\n",
    "        K.clear_session()\n",
    "\n",
    "        @tf.function\n",
    "        def train_step(inputs):\n",
    "            with tf.GradientTape() as tape:\n",
    "                loss = model(inputs,training=True)\n",
    "                loss = tf.add_n([loss] + model.losses)\n",
    "            gradients = tape.gradient(loss, model.trainable_variables)\n",
    "            optimizer.apply_gradients(zip(gradients, model.trainable_variables))\n",
    "            return loss\n",
    "\n",
    "        validation_loss = []\n",
    "        optimizer=keras.optimizers.Adam(learning_rate=learning_rate)\n",
    "        batch_loss = []\n",
    "        batches_per_epoch = int(np.floor((X_train_la.shape[0] + X_train_un.shape[0])/batch_size))\n",
    "\n",
    "        for epoch in range(epochs):\n",
    "\n",
    "                print(\"Epoch {}/{}\".format(epoch,epochs))\n",
    "\n",
    "                for i in range(batches_per_epoch):\n",
    "\n",
    "                    batch_x_la, batch_y_la, batch_x_un,batch_c_la,batch_c_un= create_batch(\n",
    "                        X_train_la, y_train_la, X_train_un,c_train_la,c_train_un,batch_size)\n",
    "\n",
    "                    inputs = [batch_x_la.to_numpy(),batch_y_la,batch_x_un.to_numpy(),batch_c_la.to_numpy(),\n",
    "                             batch_c_un.to_numpy()]\n",
    "                    loss = train_step(inputs)\n",
    "                    batch_loss.append(loss)\n",
    "                    average_batch_loss = list_average(batch_loss)\n",
    "                    print_status_bar(i*batch_size,X_train_la.shape[0] + X_train_un.shape[0],average_batch_loss)\n",
    "\n",
    "                training_loss_for_epoch = list_average(batch_loss)\n",
    "                batch_loss = []\n",
    "                history.append(training_loss_for_epoch)\n",
    "                val_loss = -validation_log_lik_sampling(y_valid_la,X_valid_la.to_numpy(),c_valid_la.to_numpy(),\n",
    "                                                        variational_decoder=variational_decoder,codings_size=codings_size)\n",
    "\n",
    "                validation_loss.append(val_loss)\n",
    "                print_status_bar_epoch(X_train_la.shape[0] + X_train_un.shape[0]\n",
    "                                 ,(X_train_la.shape[0] + X_train_un.shape[0]),training_loss_for_epoch,val_loss )\n",
    "\n",
    "                #callback for early stopping\n",
    "                if epoch <= patience - 1:\n",
    "\n",
    "                    if epoch == 0:\n",
    "\n",
    "                        variational_encoder.save(\"variational_encoder.h5\")\n",
    "                        variational_decoder.save(\"variational_decoder.h5\")\n",
    "                        classifier.save(\"classifier.h5\")\n",
    "                        y_distribution.save(\"y_distribution.h5\")\n",
    "\n",
    "                    else:\n",
    "                        if all(val_loss<i for i in validation_loss[:-1]) is True:\n",
    "                            variational_encoder.save(\"variational_encoder.h5\")\n",
    "                            variational_decoder.save(\"variational_decoder.h5\")\n",
    "                            classifier.save(\"classifier.h5\")\n",
    "                            y_distribution.save(\"y_distribution.h5\")\n",
    "                #this statement means at least a model is saved. Because if the best model was before epoch > patience-1,\n",
    "                #then the statement below won't save any model, which is undesirable as we need to load a model. \n",
    "\n",
    "                if epoch > patience - 1:\n",
    "\n",
    "                    latest_val_loss = validation_loss[-patience:]\n",
    "                    if all(val_loss<i for i in latest_val_loss[:-2]) is True:\n",
    "                        variational_encoder.save(\"variational_encoder.h5\")\n",
    "                        variational_decoder.save(\"variational_decoder.h5\")\n",
    "                        classifier.save(\"classifier.h5\")\n",
    "                        y_distribution.save(\"y_distribution.h5\")\n",
    "                    if all(i>latest_val_loss[0] for i in latest_val_loss[1:]) is True:\n",
    "                        break     \n",
    "\n",
    "        #load best model#\n",
    "        variational_encoder = keras.models.load_model(\"variational_encoder.h5\", custom_objects={\n",
    "           \"Sampling\": Sampling\n",
    "        })\n",
    "        variational_decoder = keras.models.load_model(\"variational_decoder.h5\")\n",
    "        classifier = keras.models.load_model(\"classifier.h5\", custom_objects={\n",
    "           \"Sampling\": Sampling\n",
    "        })    \n",
    "        y_distribution = keras.models.load_model(\"y_distribution.h5\", custom_objects={\n",
    "           \"y_dist\": y_dist\n",
    "        })    \n",
    "\n",
    "        done = time.time()\n",
    "        elapsed = done-start\n",
    "        print(\"Elapsed/s: \",elapsed)\n",
    "        print(\"Final training loss: \",training_loss_for_epoch)\n",
    "        print(\"best val loss: \", min(validation_loss))\n",
    "        \n",
    "        return history, min(validation_loss)\n",
    "\n",
    "        \n",
    "    else:\n",
    "        \n",
    "        start = time.time()\n",
    "        history = []\n",
    "        K.clear_session()\n",
    "\n",
    "        @tf.function\n",
    "        def train_step(inputs):\n",
    "            with tf.GradientTape() as tape:\n",
    "                loss = model(inputs,training=True)\n",
    "                loss = tf.add_n([loss] + model.losses)\n",
    "            gradients = tape.gradient(loss, model.trainable_variables)\n",
    "            optimizer.apply_gradients(zip(gradients, model.trainable_variables))\n",
    "            return loss\n",
    "\n",
    "        optimizer=keras.optimizers.Adam(learning_rate=learning_rate)\n",
    "        batch_loss = []\n",
    "        batches_per_epoch = int(np.floor((X_train_la.shape[0] + X_train_un.shape[0])/batch_size))        \n",
    "        val_loss = 0\n",
    "\n",
    "        for epoch in range(epochs):\n",
    "\n",
    "                print(\"Epoch {}/{}\".format(epoch,epochs))\n",
    "                for i in range(batches_per_epoch):\n",
    "\n",
    "                    batch_x_la, batch_y_la, batch_x_un,batch_c_la,batch_c_un= create_batch(\n",
    "                        X_train_la, y_train_la, X_train_un,c_train_la,c_train_un,batch_size)\n",
    "\n",
    "                    inputs = [batch_x_la.to_numpy(),batch_y_la,batch_x_un.to_numpy(),batch_c_la.to_numpy(),\n",
    "                             batch_c_un.to_numpy()]\n",
    "                    loss = train_step(inputs)\n",
    "                    batch_loss.append(loss)\n",
    "                    average_batch_loss = list_average(batch_loss)\n",
    "                    print_status_bar(i*batch_size,X_train_la.shape[0] + X_train_un.shape[0],average_batch_loss)\n",
    "\n",
    "                training_loss_for_epoch = list_average(batch_loss)\n",
    "                batch_loss = []\n",
    "                history.append(training_loss_for_epoch)\n",
    "                print_status_bar_epoch(X_train_la.shape[0] + X_train_un.shape[0]\n",
    "                                 ,(X_train_la.shape[0] + X_train_un.shape[0]),training_loss_for_epoch,val_loss )\n",
    "        \n",
    "\n",
    "        variational_encoder.save(\"variational_encoder.h5\")\n",
    "        variational_decoder.save(\"variational_decoder.h5\")\n",
    "        classifier.save(\"classifier.h5\")\n",
    "        y_distribution.save(\"y_distribution.h5\")\n",
    "        \n",
    "        #load best model#\n",
    "        variational_encoder = keras.models.load_model(\"variational_encoder.h5\", custom_objects={\n",
    "           \"Sampling\": Sampling\n",
    "        })\n",
    "        variational_decoder = keras.models.load_model(\"variational_decoder.h5\")\n",
    "        classifier = keras.models.load_model(\"classifier.h5\", custom_objects={\n",
    "           \"Sampling\": Sampling\n",
    "        })     \n",
    "        y_distribution = keras.models.load_model(\"y_distribution.h5\", custom_objects={\n",
    "           \"y_dist\": y_dist\n",
    "        })    \n",
    "\n",
    "        done = time.time()\n",
    "        elapsed = done-start\n",
    "        print(\"Elapsed/s: \",elapsed)\n",
    "        print(\"Final training loss: \",training_loss_for_epoch)\n",
    "        \n",
    "    \n",
    "        return history\n",
    "\n",
    "\n",
    "def fit_model_search(X_train_la, y_train_la, X_train_un,c_train_la, c_train_un, epochs,X_valid_la, y_valid_la,c_valid_la,\n",
    "              patience,variational_encoder,variational_decoder,\n",
    "             classifier,y_distribution,model,Sampling=Sampling,y_dist=y_dist,batch_size=32,learning_rate=0.001,\n",
    "                    codings_size=50):\n",
    "\n",
    "    \"\"\"\n",
    "    Use for hyperparameter search. \n",
    "    \n",
    "    Fits the model. Gets the validation loss too. And includes a version of early stopping, given by the patience.\n",
    "    Progress bars are shown too.\n",
    "    Number of epochs are specified by the parameter epochs.\n",
    "    \n",
    "    Need to pass in all the custom components. Maybe could put them in a dictionary for cleanliness.\n",
    "    \n",
    "    Returns list of training loss, and the minimum validation loss. It also saves the best encoder, decoder and\n",
    "    regressor so they can be used. \n",
    "    \n",
    "    \"\"\"\n",
    "    \n",
    "    start = time.time()\n",
    "    history = []   \n",
    "       \n",
    "    @tf.function\n",
    "    def train_step(inputs):\n",
    "        with tf.GradientTape() as tape:\n",
    "            loss = model(inputs,training=True)\n",
    "            loss = tf.add_n([loss] + model.losses)\n",
    "        gradients = tape.gradient(loss, model.trainable_variables)\n",
    "        optimizer.apply_gradients(zip(gradients, model.trainable_variables))\n",
    "        return loss\n",
    "    \n",
    "    validation_loss = []\n",
    "    optimizer=keras.optimizers.Adam(learning_rate=learning_rate)\n",
    "    batch_loss = []    \n",
    "    batches_per_epoch = int(np.floor((X_train_la.shape[0] + X_train_un.shape[0])/batch_size))\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "            \n",
    "            print(\"Epoch {}/{}\".format(epoch,epochs))\n",
    "            \n",
    "            for i in range(batches_per_epoch):\n",
    "                \n",
    "                batch_x_la, batch_y_la, batch_x_un,batch_c_la,batch_c_un= create_batch(\n",
    "                    X_train_la, y_train_la, X_train_un,c_train_la,c_train_un,batch_size)\n",
    "\n",
    "                inputs = [batch_x_la.to_numpy(),batch_y_la,batch_x_un.to_numpy(),batch_c_la.to_numpy(),\n",
    "                         batch_c_un.to_numpy()]\n",
    "                loss = train_step(inputs)\n",
    "                batch_loss.append(loss)                \n",
    "                average_batch_loss = list_average(batch_loss)                \n",
    "                print_status_bar(i*batch_size,X_train_la.shape[0] + X_train_un.shape[0],average_batch_loss)\n",
    "            \n",
    "            training_loss_for_epoch = list_average(batch_loss)\n",
    "            batch_loss = []                \n",
    "            history.append(training_loss_for_epoch)            \n",
    "            val_loss = -validation_log_lik_sampling(y_valid_la,X_valid_la.to_numpy(),c_valid_la.to_numpy(),\n",
    "                                                    variational_decoder=variational_decoder,codings_size=codings_size)\n",
    "            \n",
    "            validation_loss.append(val_loss)            \n",
    "            print_status_bar_epoch(X_train_la.shape[0] + X_train_un.shape[0]\n",
    "                             ,(X_train_la.shape[0] + X_train_un.shape[0]),training_loss_for_epoch,val_loss )\n",
    "            \n",
    "            #callback for early stopping\n",
    "            \n",
    "            if epoch <= patience - 1:\n",
    "                \n",
    "                if epoch == 0:\n",
    "                \n",
    "                    variational_encoder.save(\"variational_encoder_intermediate.h5\")\n",
    "                    variational_decoder.save(\"variational_decoder_intermediate.h5\")\n",
    "                    classifier.save(\"classifier_intermediate.h5\")\n",
    "                    y_distribution.save(\"y_distribution_intermediate.h5\")\n",
    "                    \n",
    "                else:\n",
    "                    if all(val_loss<i for i in validation_loss[:-1]) is True:\n",
    "                        variational_encoder.save(\"variational_encoder_intermediate.h5\")\n",
    "                        variational_decoder.save(\"variational_decoder_intermediate.h5\")\n",
    "                        classifier.save(\"classifier_intermediate.h5\")\n",
    "                        y_distribution.save(\"y_distribution_intermediate.h5\")\n",
    "            #this statement means at least a model is saved. Because if the best model was before epoch > patience-1,\n",
    "            #then the statement below won't save any model, which is undesirable as we need to load a model. \n",
    "            \n",
    "            if epoch > patience - 1:\n",
    "                                \n",
    "                latest_val_loss = validation_loss[-patience:]\n",
    "                if all(val_loss<i for i in latest_val_loss[:-1]) is True:\n",
    "                    variational_encoder.save(\"variational_encoder_intermediate.h5\")\n",
    "                    variational_decoder.save(\"variational_decoder_intermediate.h5\")\n",
    "                    classifier.save(\"classifier_intermediate.h5\")\n",
    "                    y_distribution.save(\"y_distribution_intermediate.h5\")\n",
    "                if all(i>latest_val_loss[0] for i in latest_val_loss[1:]) is True:\n",
    "                    break     \n",
    "    \n",
    "    #load best model#\n",
    "    variational_encoder = keras.models.load_model(\"variational_encoder_intermediate.h5\", custom_objects={\n",
    "       \"Sampling\": Sampling\n",
    "    })\n",
    "    variational_decoder = keras.models.load_model(\"variational_decoder_intermediate.h5\")\n",
    "    classifier = keras.models.load_model(\"classifier_intermediate.h5\", custom_objects={\n",
    "           \"Sampling\": Sampling\n",
    "        })    \n",
    "    y_distribution = keras.models.load_model(\"y_distribution_intermediate.h5\", custom_objects={\n",
    "       \"y_dist\": y_dist\n",
    "    })    \n",
    "                \n",
    "    done = time.time()\n",
    "    elapsed = done-start\n",
    "    print(\"Elapsed/s: \",elapsed)\n",
    "    print(\"Final training loss: \",training_loss_for_epoch)\n",
    "    print(\"best val loss: \", min(validation_loss))\n",
    "    \n",
    "    return history, min(validation_loss)\n",
    "\n",
    "def hyperparameter_search_mmd(param_distribs,epochs,patience,n_iter,X_train_la=X_train_omics_labelled, \n",
    "                          y_train_la=train_set_labelled_y, X_train_un=X_train_omics_unlabelled,c_train_la=train_set_labelled_c,\n",
    "                              c_train_un=train_set_unlabelled_c,c_valid_la=valid_set_labelled_c,\n",
    "                          X_valid_la=X_valid_omics, y_valid_la=valid_set_labelled_y):\n",
    "    \n",
    "    \"\"\"\n",
    "    Performs hyperparameter, random search. Assesses performance by determining the score on the validation set. \n",
    "    \n",
    "    Saves best models (encoder, decoder and regressor) and returns these. These can then be used downstream.\n",
    "    \n",
    "    Also returns dictionary of the search results.\n",
    "    \n",
    "    Param_distribs of the form: \n",
    "            param_distribs = {\n",
    "            \"n_hidden\": [1],\n",
    "            \"n_hidden_classifier\": [1],\n",
    "            \"beta\": [1],\n",
    "            \"n_neurons\": randint.rvs(50,1000-49,size=20,random_state=random_state).tolist(),\n",
    "           \"n_neurons_classifier\": randint.rvs(49,1000-49,size=20,random_state=random_state).tolist(),\n",
    "            \"codings_size\": randint.rvs(50,290-50,size=30,random_state=random_state).tolist(),\n",
    "            \"N\" :randint.rvs().tolist(),\n",
    "            \"learning_rate\" : ....\n",
    "            #\"codings_size\": [50]}\n",
    "            \n",
    "    There must be a value for every parameter. If you know the value you want to use, set it in the param_distribs\n",
    "    dictionary.\n",
    "    \n",
    "    Patience must be less than the number of epochs.\n",
    "    \n",
    "    e.g. result,variational_encoder,variational_decoder,classifier,y_distribution =\n",
    "            hyperparameter_search_mmd(param_distribs,500,10,n_iter=10)\n",
    "\n",
    "            \n",
    "    \"\"\"\n",
    "\n",
    "    np.random.seed(42) #needs to be here so that everything that follows is consistent\n",
    "\n",
    "    min_val_loss = []\n",
    "    master = {}\n",
    "\n",
    "    for i in range(n_iter): \n",
    "        K.clear_session()\n",
    "        master[i] = {}\n",
    "        master[i][\"parameters\"] = {}\n",
    "        \n",
    "        N= np.random.choice(param_distribs[\"N\"])\n",
    "        learning_rate= np.random.choice(param_distribs[\"learning_rate\"])\n",
    "        beta= np.random.choice(param_distribs[\"beta\"])\n",
    "        n_neurons =np.random.choice(param_distribs[\"n_neurons\"]) \n",
    "        n_neurons_classifier =np.random.choice(param_distribs[\"n_neurons_classifier\"]) \n",
    "        n_hidden  =np.random.choice(param_distribs[\"n_hidden\"]) \n",
    "        n_hidden_classifier  =np.random.choice(param_distribs[\"n_hidden_classifier\"]) \n",
    "        codings_size =np.random.choice(param_distribs[\"codings_size\"]) \n",
    "       \n",
    "        master[i][\"parameters\"][\"N\"] = N\n",
    "        master[i][\"parameters\"][\"learning_rate\"] = learning_rate\n",
    "        master[i][\"parameters\"][\"beta\"] = beta\n",
    "        master[i][\"parameters\"][\"n_neurons\"] = n_neurons\n",
    "        master[i][\"parameters\"][\"n_neurons_classifier\"] = n_neurons_classifier\n",
    "        master[i][\"parameters\"][\"n_hidden\"] = n_hidden\n",
    "        master[i][\"parameters\"][\"n_hidden_classifier\"] = n_hidden_classifier\n",
    "        master[i][\"parameters\"][\"codings_size\"] = codings_size\n",
    "\n",
    "        \n",
    "        variational_encoder,variational_decoder,classifier,y_distribution,model = build_model_mmd(n_hidden=n_hidden,       \n",
    "                                       n_neurons=n_neurons,beta=beta,n_hidden_classifier=n_hidden_classifier,\n",
    "                                        n_neurons_classifier=n_neurons_classifier,N=N,codings_size=codings_size)\n",
    "        \n",
    "                \n",
    "        history,val_loss = fit_model_search(X_train_la=X_train_la, y_train_la=y_train_la, \n",
    "                                 X_train_un=X_train_un, epochs=epochs,X_valid_la=X_valid_la, \n",
    "                                 y_valid_la=y_valid_la,patience=patience,variational_encoder=variational_encoder,\n",
    "                                variational_decoder=variational_decoder, classifier=classifier,\n",
    "                                y_distribution=y_distribution,model=model,Sampling=Sampling,y_dist=y_dist,\n",
    "                                            batch_size=32,learning_rate=learning_rate,codings_size=codings_size,\n",
    "                                            \n",
    "                                c_train_la=c_train_la, c_train_un = c_train_un,c_valid_la=c_valid_la\n",
    "                                           )        \n",
    "\n",
    "        master[i][\"val_loss\"] = val_loss\n",
    "        min_val_loss.append(val_loss)\n",
    "\n",
    "        #If val loss is lowest, save this model. \n",
    "        if val_loss <=  min(min_val_loss):\n",
    "            os.rename(\"variational_encoder_intermediate.h5\",\"variational_encoder.h5\")\n",
    "            os.rename(\"variational_decoder_intermediate.h5\",\"variational_decoder.h5\")\n",
    "            os.rename(\"classifier_intermediate.h5\",\"classifier.h5\")\n",
    "            os.rename(\"y_distribution_intermediate.h5\",\"y_distribution.h5\")\n",
    "\n",
    "        print(master)\n",
    "            \n",
    "    #load best model#\n",
    "    variational_encoder = keras.models.load_model(\"variational_encoder.h5\", custom_objects={\n",
    "       \"Sampling\": Sampling\n",
    "    })\n",
    "    variational_decoder = keras.models.load_model(\"variational_decoder.h5\")\n",
    "    classifier = keras.models.load_model(\"classifier.h5\", custom_objects={\n",
    "           \"Sampling\": Sampling\n",
    "        }) \n",
    "    y_distribution = keras.models.load_model(\"y_distribution.h5\", custom_objects={\n",
    "       \"y_dist\": y_dist\n",
    "    })    \n",
    "\n",
    "    result = sorted(master.items(), key=lambda item: item[1][\"val_loss\"])\n",
    "    return result,variational_encoder,variational_decoder,classifier,y_distribution\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hyperparameter Search #"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 246,
   "metadata": {},
   "outputs": [],
   "source": [
    "param_distribs = {\n",
    "            \"n_hidden\": [1,2],\n",
    "            \"n_hidden_classifier\": [1,2],\n",
    "            \"beta\": [1,10,15],\n",
    "    #\"n_neurons\": [300,500],\n",
    "   # \"n_neurons_classifier\": [50,100,150],\n",
    "            \"n_neurons\": randint.rvs(50,600-49,size=20,random_state=random_state).tolist(),\n",
    "           \"n_neurons_classifier\": randint.rvs(20,120-20,size=20,random_state=random_state).tolist(),\n",
    "            \"codings_size\": randint.rvs(20,290-20,size=30,random_state=random_state).tolist(),\n",
    "   # \"codings_size\": [20,50,70],\n",
    "            \"N\" :[0.1,1,10\n",
    "                 \n",
    "                 \n",
    "            ],\n",
    "            \"learning_rate\" : [0.001,0.0005],\n",
    "            #\"codings_size\": [120,60]\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 247,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0/80\n",
      "WARNING:tensorflow:Layer full_model_mmd is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because it's dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "2240/2278 [============================>.] - Loss for batch: 22.7435WARNING:tensorflow:Layer concatenate_1 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because it's dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "2278/2278 [==============================] - trainLoss: 22.7435  Val_loss: 18.9364 \n",
      "Epoch 1/80\n",
      "2278/2278 [==============================] - trainLoss: 9.7974  Val_loss: 18.0681 \n",
      "Epoch 2/80\n",
      "2278/2278 [==============================] - trainLoss: 7.3631  Val_loss: 17.6534 \n",
      "Epoch 3/80\n",
      "2278/2278 [==============================] - trainLoss: 6.2799  Val_loss: 17.4134 \n",
      "Epoch 4/80\n",
      "2278/2278 [==============================] - trainLoss: 5.8135  Val_loss: 17.4054 \n",
      "Epoch 5/80\n",
      "2278/2278 [==============================] - trainLoss: 5.3067  Val_loss: 17.1440 \n",
      "Epoch 6/80\n",
      "2278/2278 [==============================] - trainLoss: 4.9144  Val_loss: 16.9464 \n",
      "Epoch 7/80\n",
      "2278/2278 [==============================] - trainLoss: 4.5202  Val_loss: 17.0573 \n",
      "Epoch 8/80\n",
      "2278/2278 [==============================] - trainLoss: 4.4162  Val_loss: 16.9021 \n",
      "Epoch 9/80\n",
      "2278/2278 [==============================] - trainLoss: 4.1477  Val_loss: 16.9570 \n",
      "Epoch 10/80\n",
      "2278/2278 [==============================] - trainLoss: 3.7179  Val_loss: 16.8970 \n",
      "Epoch 11/80\n",
      "2278/2278 [==============================] - trainLoss: 3.5578  Val_loss: 16.8757 \n",
      "Epoch 12/80\n",
      "2278/2278 [==============================] - trainLoss: 3.3992  Val_loss: 16.5972 \n",
      "Epoch 13/80\n",
      "2278/2278 [==============================] - trainLoss: 3.2084  Val_loss: 16.6488 \n",
      "Epoch 14/80\n",
      "2278/2278 [==============================] - trainLoss: 3.1219  Val_loss: 16.6691 \n",
      "Epoch 15/80\n",
      "2278/2278 [==============================] - trainLoss: 2.8500  Val_loss: 16.5633 \n",
      "Epoch 16/80\n",
      "2278/2278 [==============================] - trainLoss: 2.7828  Val_loss: 16.5150 \n",
      "Epoch 17/80\n",
      "2278/2278 [==============================] - trainLoss: 2.7707  Val_loss: 16.2986 \n",
      "Epoch 18/80\n",
      "2278/2278 [==============================] - trainLoss: 2.4738  Val_loss: 16.3028 \n",
      "Epoch 19/80\n",
      "2278/2278 [==============================] - trainLoss: 2.3990  Val_loss: 16.3066 \n",
      "Epoch 20/80\n",
      "2278/2278 [==============================] - trainLoss: 2.3265  Val_loss: 16.1719 \n",
      "Epoch 21/80\n",
      "2278/2278 [==============================] - trainLoss: 2.2057  Val_loss: 16.0251 \n",
      "Epoch 22/80\n",
      "2278/2278 [==============================] - trainLoss: 2.0792  Val_loss: 16.1830 \n",
      "Epoch 23/80\n",
      "2278/2278 [==============================] - trainLoss: 2.0526  Val_loss: 15.7134 \n",
      "Epoch 24/80\n",
      "2278/2278 [==============================] - trainLoss: 1.9064  Val_loss: 15.7751 \n",
      "Epoch 25/80\n",
      "2278/2278 [==============================] - trainLoss: 1.8392  Val_loss: 15.7599 \n",
      "Epoch 26/80\n",
      "2278/2278 [==============================] - trainLoss: 1.8631  Val_loss: 15.7605 \n",
      "Epoch 27/80\n",
      "2278/2278 [==============================] - trainLoss: 1.7602  Val_loss: 15.4235 \n",
      "Epoch 28/80\n",
      "2278/2278 [==============================] - trainLoss: 1.8222  Val_loss: 15.4734 \n",
      "Epoch 29/80\n",
      "2278/2278 [==============================] - trainLoss: 1.8132  Val_loss: 15.3477 \n",
      "Epoch 30/80\n",
      "2278/2278 [==============================] - trainLoss: 1.6697  Val_loss: 15.0919 \n",
      "Epoch 31/80\n",
      "2278/2278 [==============================] - trainLoss: 1.6592  Val_loss: 14.9144 \n",
      "Epoch 32/80\n",
      "2278/2278 [==============================] - trainLoss: 1.6584  Val_loss: 15.0837 \n",
      "Epoch 33/80\n",
      "2278/2278 [==============================] - trainLoss: 1.6382  Val_loss: 14.9879 \n",
      "Epoch 34/80\n",
      "2278/2278 [==============================] - trainLoss: 1.6241  Val_loss: 14.7860 \n",
      "Epoch 35/80\n",
      "2278/2278 [==============================] - trainLoss: 1.5544  Val_loss: 14.6378 \n",
      "Epoch 36/80\n",
      "2278/2278 [==============================] - trainLoss: 1.5079  Val_loss: 14.7066 \n",
      "Epoch 37/80\n",
      "2278/2278 [==============================] - trainLoss: 1.4016  Val_loss: 14.5360 \n",
      "Epoch 38/80\n",
      "2278/2278 [==============================] - trainLoss: 1.4392  Val_loss: 14.5636 \n",
      "Epoch 39/80\n",
      "2278/2278 [==============================] - trainLoss: 1.4204  Val_loss: 14.5096 \n",
      "Epoch 40/80\n",
      "2278/2278 [==============================] - trainLoss: 1.4082  Val_loss: 14.4561 \n",
      "Epoch 41/80\n",
      "2278/2278 [==============================] - trainLoss: 1.3356  Val_loss: 14.3118 \n",
      "Epoch 42/80\n",
      "2278/2278 [==============================] - trainLoss: 1.2687  Val_loss: 14.2638 \n",
      "Epoch 43/80\n",
      "2278/2278 [==============================] - trainLoss: 1.3764  Val_loss: 14.1984 \n",
      "Epoch 44/80\n",
      "2278/2278 [==============================] - trainLoss: 1.4001  Val_loss: 14.1484 \n",
      "Epoch 45/80\n",
      "2278/2278 [==============================] - trainLoss: 1.2721  Val_loss: 14.0453 \n",
      "Epoch 46/80\n",
      "2278/2278 [==============================] - trainLoss: 1.2924  Val_loss: 14.1078 \n",
      "Epoch 47/80\n",
      "2278/2278 [==============================] - trainLoss: 1.2593  Val_loss: 13.9596 \n",
      "Epoch 48/80\n",
      "2278/2278 [==============================] - trainLoss: 1.2095  Val_loss: 13.9330 \n",
      "Epoch 49/80\n",
      "2278/2278 [==============================] - trainLoss: 1.2131  Val_loss: 13.8915 \n",
      "Epoch 50/80\n",
      "2278/2278 [==============================] - trainLoss: 1.1782  Val_loss: 13.7942 \n",
      "Epoch 51/80\n",
      "2278/2278 [==============================] - trainLoss: 1.2797  Val_loss: 13.8664 \n",
      "Epoch 52/80\n",
      "2278/2278 [==============================] - trainLoss: 1.3074  Val_loss: 13.7642 \n",
      "Epoch 53/80\n",
      "2278/2278 [==============================] - trainLoss: 1.2830  Val_loss: 13.8161 \n",
      "Epoch 54/80\n",
      "2278/2278 [==============================] - trainLoss: 1.2697  Val_loss: 13.7433 \n",
      "Epoch 55/80\n",
      "2278/2278 [==============================] - trainLoss: 1.3071  Val_loss: 13.6517 \n",
      "Epoch 56/80\n",
      "2278/2278 [==============================] - trainLoss: 1.1730  Val_loss: 13.6070 \n",
      "Epoch 57/80\n",
      "2278/2278 [==============================] - trainLoss: 1.1663  Val_loss: 13.5588 \n",
      "Epoch 58/80\n",
      "2278/2278 [==============================] - trainLoss: 1.1394  Val_loss: 13.6030 \n",
      "Epoch 59/80\n",
      "2278/2278 [==============================] - trainLoss: 1.1377  Val_loss: 13.4835 \n",
      "Epoch 60/80\n",
      "2278/2278 [==============================] - trainLoss: 1.0806  Val_loss: 13.5067 \n",
      "Epoch 61/80\n",
      "2278/2278 [==============================] - trainLoss: 1.0921  Val_loss: 13.4616 \n",
      "Epoch 62/80\n",
      "2278/2278 [==============================] - trainLoss: 1.2332  Val_loss: 13.4155 \n",
      "Epoch 63/80\n",
      "2278/2278 [==============================] - trainLoss: 1.0968  Val_loss: 13.3062 \n",
      "Epoch 64/80\n",
      "2278/2278 [==============================] - trainLoss: 1.1947  Val_loss: 13.3039 \n",
      "Epoch 65/80\n",
      "2278/2278 [==============================] - trainLoss: 1.1045  Val_loss: 13.3177 \n",
      "Epoch 66/80\n",
      "2278/2278 [==============================] - trainLoss: 1.1169  Val_loss: 13.2396 \n",
      "Epoch 67/80\n",
      "2278/2278 [==============================] - trainLoss: 1.0681  Val_loss: 13.1826 \n",
      "Epoch 68/80\n",
      "2278/2278 [==============================] - trainLoss: 1.1069  Val_loss: 13.2806 \n",
      "Epoch 69/80\n",
      "2278/2278 [==============================] - trainLoss: 1.0840  Val_loss: 13.2042 \n",
      "Epoch 70/80\n",
      "2278/2278 [==============================] - trainLoss: 1.0627  Val_loss: 13.1672 \n",
      "Epoch 71/80\n",
      "2278/2278 [==============================] - trainLoss: 1.1332  Val_loss: 13.1906 \n",
      "Epoch 72/80\n",
      "2278/2278 [==============================] - trainLoss: 1.0662  Val_loss: 13.1151 \n",
      "Epoch 73/80\n",
      "2278/2278 [==============================] - trainLoss: 1.0008  Val_loss: 13.1312 \n",
      "Epoch 74/80\n",
      "2278/2278 [==============================] - trainLoss: 1.1333  Val_loss: 13.0790 \n",
      "Epoch 75/80\n",
      "2278/2278 [==============================] - trainLoss: 1.1111  Val_loss: 13.1062 \n",
      "Epoch 76/80\n",
      "2278/2278 [==============================] - trainLoss: 1.0518  Val_loss: 13.0315 \n",
      "Epoch 77/80\n",
      "2278/2278 [==============================] - trainLoss: 0.9917  Val_loss: 13.0595 \n",
      "Epoch 78/80\n",
      "2278/2278 [==============================] - trainLoss: 1.0716  Val_loss: 13.0703 \n",
      "Epoch 79/80\n",
      "2278/2278 [==============================] - trainLoss: 0.9421  Val_loss: 13.0602 \n",
      "WARNING:tensorflow:No training configuration found in save file: the model was *not* compiled. Compile it manually.\n",
      "WARNING:tensorflow:No training configuration found in save file: the model was *not* compiled. Compile it manually.\n",
      "WARNING:tensorflow:No training configuration found in save file: the model was *not* compiled. Compile it manually.\n",
      "WARNING:tensorflow:No training configuration found in save file: the model was *not* compiled. Compile it manually.\n",
      "Elapsed/s:  114.5376250743866\n",
      "Final training loss:  tf.Tensor(0.94205505, shape=(), dtype=float32)\n",
      "best val loss:  tf.Tensor(13.031519, shape=(), dtype=float32)\n",
      "{0: {'parameters': {'learning_rate': 0.0005, 'beta': 1, 'n_hidden': 2, 'n_neurons_classifier': 72, 'codings_size': 169, 'N': 10.0, 'n_hidden_classifier': 1, 'n_neurons': 137}, 'val_loss': <tf.Tensor: shape=(), dtype=float32, numpy=13.031519>}}\n",
      "Epoch 0/80\n",
      "WARNING:tensorflow:Layer full_model_mmd is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because it's dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "2240/2278 [============================>.] - Loss for batch: 17.9447WARNING:tensorflow:Layer concatenate_1 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because it's dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "2278/2278 [==============================] - trainLoss: 17.9447  Val_loss: 19.5116 \n",
      "Epoch 1/80\n",
      "2278/2278 [==============================] - trainLoss: 8.9445  Val_loss: 18.0788 \n",
      "Epoch 2/80\n",
      "2278/2278 [==============================] - trainLoss: 7.4456  Val_loss: 17.3453 \n",
      "Epoch 3/80\n",
      "2278/2278 [==============================] - trainLoss: 6.4728  Val_loss: 16.6380 \n",
      "Epoch 4/80\n",
      "2278/2278 [==============================] - trainLoss: 5.6171  Val_loss: 16.5363 \n",
      "Epoch 5/80\n",
      "2278/2278 [==============================] - trainLoss: 5.0315  Val_loss: 16.0281 \n",
      "Epoch 6/80\n",
      "2278/2278 [==============================] - trainLoss: 4.4855  Val_loss: 15.3795 \n",
      "Epoch 7/80\n",
      "2278/2278 [==============================] - trainLoss: 4.0349  Val_loss: 15.3852 \n",
      "Epoch 8/80\n",
      "2278/2278 [==============================] - trainLoss: 3.6435  Val_loss: 15.1219 \n",
      "Epoch 9/80\n",
      "2278/2278 [==============================] - trainLoss: 3.4052  Val_loss: 14.9371 \n",
      "Epoch 10/80\n",
      "2278/2278 [==============================] - trainLoss: 3.2053  Val_loss: 14.7187 \n",
      "Epoch 11/80\n",
      "2278/2278 [==============================] - trainLoss: 2.9947  Val_loss: 15.0029 \n",
      "Epoch 12/80\n",
      "2278/2278 [==============================] - trainLoss: 2.8408  Val_loss: 14.6358 \n",
      "Epoch 13/80\n",
      "2278/2278 [==============================] - trainLoss: 2.6169  Val_loss: 14.5797 \n",
      "Epoch 14/80\n",
      "2278/2278 [==============================] - trainLoss: 2.5552  Val_loss: 14.5954 \n",
      "Epoch 15/80\n",
      "2278/2278 [==============================] - trainLoss: 2.5394  Val_loss: 14.3549 \n",
      "Epoch 16/80\n",
      "2278/2278 [==============================] - trainLoss: 2.3845  Val_loss: 14.0869 \n",
      "Epoch 17/80\n",
      "2278/2278 [==============================] - trainLoss: 2.2161  Val_loss: 14.2902 \n",
      "Epoch 18/80\n",
      "2278/2278 [==============================] - trainLoss: 2.1989  Val_loss: 14.3184 \n",
      "Epoch 19/80\n",
      "2278/2278 [==============================] - trainLoss: 2.2234  Val_loss: 14.2451 \n",
      "Epoch 20/80\n",
      "2278/2278 [==============================] - trainLoss: 2.1459  Val_loss: 14.1892 \n",
      "Epoch 21/80\n",
      "2278/2278 [==============================] - trainLoss: 2.0515  Val_loss: 14.0290 \n",
      "Epoch 22/80\n",
      "2278/2278 [==============================] - trainLoss: 2.0129  Val_loss: 13.8328 \n",
      "Epoch 23/80\n",
      "2278/2278 [==============================] - trainLoss: 2.0276  Val_loss: 14.1312 \n",
      "Epoch 24/80\n",
      "2278/2278 [==============================] - trainLoss: 1.9445  Val_loss: 13.9082 \n",
      "Epoch 25/80\n",
      "2278/2278 [==============================] - trainLoss: 1.9925  Val_loss: 13.8974 \n",
      "Epoch 26/80\n",
      "2278/2278 [==============================] - trainLoss: 1.9631  Val_loss: 13.9969 \n",
      "Epoch 27/80\n",
      "2278/2278 [==============================] - trainLoss: 1.9364  Val_loss: 13.8920 \n",
      "Epoch 28/80\n",
      "2278/2278 [==============================] - trainLoss: 1.9267  Val_loss: 13.7596 \n",
      "Epoch 29/80\n",
      "2278/2278 [==============================] - trainLoss: 1.8990  Val_loss: 13.8124 \n",
      "Epoch 30/80\n",
      "2278/2278 [==============================] - trainLoss: 1.8426  Val_loss: 13.5580 \n",
      "Epoch 31/80\n",
      "2278/2278 [==============================] - trainLoss: 1.9091  Val_loss: 13.7336 \n",
      "Epoch 32/80\n",
      "2278/2278 [==============================] - trainLoss: 1.9268  Val_loss: 13.5722 \n",
      "Epoch 33/80\n",
      "2278/2278 [==============================] - trainLoss: 1.8590  Val_loss: 13.6081 \n",
      "Epoch 34/80\n",
      "2278/2278 [==============================] - trainLoss: 1.8634  Val_loss: 13.6026 \n",
      "Epoch 35/80\n",
      "2278/2278 [==============================] - trainLoss: 1.7910  Val_loss: 13.5512 \n",
      "Epoch 36/80\n",
      "2278/2278 [==============================] - trainLoss: 1.7957  Val_loss: 13.5621 \n",
      "Epoch 37/80\n",
      "2278/2278 [==============================] - trainLoss: 1.8263  Val_loss: 13.5610 \n",
      "Epoch 38/80\n",
      "2278/2278 [==============================] - trainLoss: 1.8202  Val_loss: 13.4633 \n",
      "Epoch 39/80\n",
      "2278/2278 [==============================] - trainLoss: 1.8692  Val_loss: 13.5111 \n",
      "Epoch 40/80\n",
      "2278/2278 [==============================] - trainLoss: 1.8948  Val_loss: 13.5198 \n",
      "Epoch 41/80\n",
      "2278/2278 [==============================] - trainLoss: 1.8411  Val_loss: 13.4631 \n",
      "Epoch 42/80\n",
      "2278/2278 [==============================] - trainLoss: 1.8284  Val_loss: 13.4680 \n",
      "Epoch 43/80\n",
      "2278/2278 [==============================] - trainLoss: 1.7269  Val_loss: 13.4136 \n",
      "Epoch 44/80\n",
      "2278/2278 [==============================] - trainLoss: 1.8026  Val_loss: 13.4288 \n",
      "Epoch 45/80\n",
      "2278/2278 [==============================] - trainLoss: 1.7976  Val_loss: 13.4280 \n",
      "Epoch 46/80\n",
      "2278/2278 [==============================] - trainLoss: 1.7707  Val_loss: 13.5332 \n",
      "Epoch 47/80\n",
      "2278/2278 [==============================] - trainLoss: 1.6788  Val_loss: 13.7233 \n",
      "Epoch 48/80\n",
      "2278/2278 [==============================] - trainLoss: 1.6930  Val_loss: 13.6292 \n",
      "Epoch 49/80\n",
      "2278/2278 [==============================] - trainLoss: 1.7898  Val_loss: 13.6832 \n",
      "Epoch 50/80\n",
      "2278/2278 [==============================] - trainLoss: 1.7992  Val_loss: 13.7233 \n",
      "Epoch 51/80\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2278/2278 [==============================] - trainLoss: 1.7804  Val_loss: 13.8789 \n",
      "Epoch 52/80\n",
      "2278/2278 [==============================] - trainLoss: 1.7665  Val_loss: 13.8356 \n",
      "WARNING:tensorflow:No training configuration found in save file: the model was *not* compiled. Compile it manually.\n",
      "WARNING:tensorflow:No training configuration found in save file: the model was *not* compiled. Compile it manually.\n",
      "WARNING:tensorflow:No training configuration found in save file: the model was *not* compiled. Compile it manually.\n",
      "WARNING:tensorflow:No training configuration found in save file: the model was *not* compiled. Compile it manually.\n",
      "Elapsed/s:  75.74787616729736\n",
      "Final training loss:  tf.Tensor(1.7665118, shape=(), dtype=float32)\n",
      "best val loss:  tf.Tensor(13.413608, shape=(), dtype=float32)\n",
      "{0: {'parameters': {'learning_rate': 0.0005, 'beta': 1, 'n_hidden': 2, 'n_neurons_classifier': 72, 'codings_size': 169, 'N': 10.0, 'n_hidden_classifier': 1, 'n_neurons': 137}, 'val_loss': <tf.Tensor: shape=(), dtype=float32, numpy=13.031519>}, 1: {'parameters': {'learning_rate': 0.001, 'beta': 10, 'n_hidden': 2, 'n_neurons_classifier': 79, 'codings_size': 122, 'N': 1.0, 'n_hidden_classifier': 2, 'n_neurons': 508}, 'val_loss': <tf.Tensor: shape=(), dtype=float32, numpy=13.413608>}}\n",
      "Epoch 0/80\n",
      "WARNING:tensorflow:Layer full_model_mmd is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because it's dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "2240/2278 [============================>.] - Loss for batch: 21.4185WARNING:tensorflow:Layer concatenate_1 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because it's dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "2278/2278 [==============================] - trainLoss: 21.4185  Val_loss: 19.5198 \n",
      "Epoch 1/80\n",
      "2278/2278 [==============================] - trainLoss: 9.7424  Val_loss: 18.7316 \n",
      "Epoch 2/80\n",
      "2278/2278 [==============================] - trainLoss: 7.3697  Val_loss: 17.9826 \n",
      "Epoch 3/80\n",
      "2278/2278 [==============================] - trainLoss: 6.4917  Val_loss: 17.7237 \n",
      "Epoch 4/80\n",
      "2278/2278 [==============================] - trainLoss: 5.8846  Val_loss: 17.5502 \n",
      "Epoch 5/80\n",
      "2278/2278 [==============================] - trainLoss: 5.6773  Val_loss: 17.3433 \n",
      "Epoch 6/80\n",
      "2278/2278 [==============================] - trainLoss: 5.1443  Val_loss: 17.1499 \n",
      "Epoch 7/80\n",
      "2278/2278 [==============================] - trainLoss: 4.7847  Val_loss: 16.9483 \n",
      "Epoch 8/80\n",
      "2278/2278 [==============================] - trainLoss: 4.5646  Val_loss: 16.8104 \n",
      "Epoch 9/80\n",
      "2278/2278 [==============================] - trainLoss: 4.2965  Val_loss: 16.6222 \n",
      "Epoch 10/80\n",
      "2278/2278 [==============================] - trainLoss: 4.1140  Val_loss: 16.4646 \n",
      "Epoch 11/80\n",
      "2278/2278 [==============================] - trainLoss: 3.7836  Val_loss: 16.4826 \n",
      "Epoch 12/80\n",
      "2278/2278 [==============================] - trainLoss: 3.4774  Val_loss: 16.0989 \n",
      "Epoch 13/80\n",
      "2278/2278 [==============================] - trainLoss: 3.4194  Val_loss: 16.2088 \n",
      "Epoch 14/80\n",
      "2278/2278 [==============================] - trainLoss: 3.2073  Val_loss: 16.1294 \n",
      "Epoch 15/80\n",
      "2278/2278 [==============================] - trainLoss: 2.9399  Val_loss: 15.9714 \n",
      "Epoch 16/80\n",
      "2278/2278 [==============================] - trainLoss: 2.7913  Val_loss: 15.9419 \n",
      "Epoch 17/80\n",
      "2278/2278 [==============================] - trainLoss: 2.7212  Val_loss: 15.9436 \n",
      "Epoch 18/80\n",
      "2278/2278 [==============================] - trainLoss: 2.5736  Val_loss: 15.6159 \n",
      "Epoch 19/80\n",
      "2278/2278 [==============================] - trainLoss: 2.3748  Val_loss: 15.6112 \n",
      "Epoch 20/80\n",
      "2278/2278 [==============================] - trainLoss: 2.2792  Val_loss: 15.5116 \n",
      "Epoch 21/80\n",
      "2278/2278 [==============================] - trainLoss: 2.2403  Val_loss: 15.4495 \n",
      "Epoch 22/80\n",
      "2278/2278 [==============================] - trainLoss: 2.3113  Val_loss: 15.2147 \n",
      "Epoch 23/80\n",
      "2278/2278 [==============================] - trainLoss: 2.0916  Val_loss: 15.0490 \n",
      "Epoch 24/80\n",
      "2278/2278 [==============================] - trainLoss: 2.0797  Val_loss: 15.0420 \n",
      "Epoch 25/80\n",
      "2278/2278 [==============================] - trainLoss: 1.8944  Val_loss: 15.0723 \n",
      "Epoch 26/80\n",
      "2278/2278 [==============================] - trainLoss: 1.9276  Val_loss: 14.8844 \n",
      "Epoch 27/80\n",
      "2278/2278 [==============================] - trainLoss: 1.8730  Val_loss: 14.7639 \n",
      "Epoch 28/80\n",
      "2278/2278 [==============================] - trainLoss: 1.7590  Val_loss: 14.5838 \n",
      "Epoch 29/80\n",
      "2278/2278 [==============================] - trainLoss: 1.7250  Val_loss: 14.4959 \n",
      "Epoch 30/80\n",
      "2278/2278 [==============================] - trainLoss: 1.6734  Val_loss: 14.3959 \n",
      "Epoch 31/80\n",
      "2278/2278 [==============================] - trainLoss: 1.6464  Val_loss: 14.4151 \n",
      "Epoch 32/80\n",
      "2278/2278 [==============================] - trainLoss: 1.6192  Val_loss: 14.3582 \n",
      "Epoch 33/80\n",
      "2278/2278 [==============================] - trainLoss: 1.4496  Val_loss: 14.2649 \n",
      "Epoch 34/80\n",
      "2278/2278 [==============================] - trainLoss: 1.4407  Val_loss: 14.2931 \n",
      "Epoch 35/80\n",
      "2278/2278 [==============================] - trainLoss: 1.4935  Val_loss: 14.1814 \n",
      "Epoch 36/80\n",
      "2278/2278 [==============================] - trainLoss: 1.4068  Val_loss: 14.1585 \n",
      "Epoch 37/80\n",
      "2278/2278 [==============================] - trainLoss: 1.3715  Val_loss: 14.1362 \n",
      "Epoch 38/80\n",
      "2278/2278 [==============================] - trainLoss: 1.3886  Val_loss: 14.0657 \n",
      "Epoch 39/80\n",
      "2278/2278 [==============================] - trainLoss: 1.3623  Val_loss: 13.9251 \n",
      "Epoch 40/80\n",
      "2278/2278 [==============================] - trainLoss: 1.4365  Val_loss: 13.9255 \n",
      "Epoch 41/80\n",
      "2278/2278 [==============================] - trainLoss: 1.2436  Val_loss: 13.8456 \n",
      "Epoch 42/80\n",
      "2278/2278 [==============================] - trainLoss: 1.2161  Val_loss: 13.6047 \n",
      "Epoch 43/80\n",
      "2278/2278 [==============================] - trainLoss: 1.3119  Val_loss: 13.6620 \n",
      "Epoch 44/80\n",
      "2278/2278 [==============================] - trainLoss: 1.2893  Val_loss: 13.5530 \n",
      "Epoch 45/80\n",
      "2278/2278 [==============================] - trainLoss: 1.2922  Val_loss: 13.5368 \n",
      "Epoch 46/80\n",
      "2278/2278 [==============================] - trainLoss: 1.1113  Val_loss: 13.5312 \n",
      "Epoch 47/80\n",
      "2278/2278 [==============================] - trainLoss: 1.2242  Val_loss: 13.4884 \n",
      "Epoch 48/80\n",
      "2278/2278 [==============================] - trainLoss: 1.2242  Val_loss: 13.4227 \n",
      "Epoch 49/80\n",
      "2278/2278 [==============================] - trainLoss: 1.2711  Val_loss: 13.3854 \n",
      "Epoch 50/80\n",
      "2278/2278 [==============================] - trainLoss: 1.2623  Val_loss: 13.3376 \n",
      "Epoch 51/80\n",
      "2278/2278 [==============================] - trainLoss: 1.2363  Val_loss: 13.2950 \n",
      "Epoch 52/80\n",
      "2278/2278 [==============================] - trainLoss: 1.1410  Val_loss: 13.3860 \n",
      "Epoch 53/80\n",
      "2278/2278 [==============================] - trainLoss: 1.2792  Val_loss: 13.2013 \n",
      "Epoch 54/80\n",
      "2278/2278 [==============================] - trainLoss: 1.2095  Val_loss: 13.2211 \n",
      "Epoch 55/80\n",
      "2278/2278 [==============================] - trainLoss: 1.0824  Val_loss: 13.1445 \n",
      "Epoch 56/80\n",
      "2278/2278 [==============================] - trainLoss: 1.1679  Val_loss: 13.2100 \n",
      "Epoch 57/80\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2278/2278 [==============================] - trainLoss: 1.2050  Val_loss: 13.2119 \n",
      "Epoch 58/80\n",
      "2278/2278 [==============================] - trainLoss: 1.1641  Val_loss: 13.0905 \n",
      "Epoch 59/80\n",
      "2278/2278 [==============================] - trainLoss: 1.1060  Val_loss: 13.2246 \n",
      "Epoch 60/80\n",
      "2278/2278 [==============================] - trainLoss: 1.0547  Val_loss: 13.1358 \n",
      "Epoch 61/80\n",
      "2278/2278 [==============================] - trainLoss: 1.1146  Val_loss: 13.0334 \n",
      "Epoch 62/80\n",
      "2278/2278 [==============================] - trainLoss: 1.0770  Val_loss: 13.1367 \n",
      "Epoch 63/80\n",
      "2278/2278 [==============================] - trainLoss: 1.0384  Val_loss: 13.0906 \n",
      "Epoch 64/80\n",
      "2278/2278 [==============================] - trainLoss: 1.0224  Val_loss: 13.0382 \n",
      "Epoch 65/80\n",
      "2278/2278 [==============================] - trainLoss: 1.0991  Val_loss: 13.0464 \n",
      "Epoch 66/80\n",
      "2278/2278 [==============================] - trainLoss: 1.0961  Val_loss: 12.9150 \n",
      "Epoch 67/80\n",
      "2278/2278 [==============================] - trainLoss: 0.9593  Val_loss: 12.9334 \n",
      "Epoch 68/80\n",
      "2278/2278 [==============================] - trainLoss: 0.9978  Val_loss: 12.8481 \n",
      "Epoch 69/80\n",
      "2278/2278 [==============================] - trainLoss: 1.0544  Val_loss: 12.8116 \n",
      "Epoch 70/80\n",
      "2278/2278 [==============================] - trainLoss: 1.0159  Val_loss: 12.9192 \n",
      "Epoch 71/80\n",
      "2278/2278 [==============================] - trainLoss: 1.0327  Val_loss: 12.8172 \n",
      "Epoch 72/80\n",
      "2278/2278 [==============================] - trainLoss: 0.9885  Val_loss: 12.7902 \n",
      "Epoch 73/80\n",
      "2278/2278 [==============================] - trainLoss: 1.0254  Val_loss: 12.7661 \n",
      "Epoch 74/80\n",
      "2278/2278 [==============================] - trainLoss: 0.9915  Val_loss: 12.7304 \n",
      "Epoch 75/80\n",
      "2278/2278 [==============================] - trainLoss: 0.9949  Val_loss: 12.8106 \n",
      "Epoch 76/80\n",
      "2278/2278 [==============================] - trainLoss: 1.0945  Val_loss: 12.8152 \n",
      "Epoch 77/80\n",
      "2278/2278 [==============================] - trainLoss: 1.0328  Val_loss: 12.7886 \n",
      "Epoch 78/80\n",
      "2278/2278 [==============================] - trainLoss: 0.8747  Val_loss: 12.7576 \n",
      "Epoch 79/80\n",
      "2278/2278 [==============================] - trainLoss: 0.9541  Val_loss: 12.7335 \n",
      "WARNING:tensorflow:No training configuration found in save file: the model was *not* compiled. Compile it manually.\n",
      "WARNING:tensorflow:No training configuration found in save file: the model was *not* compiled. Compile it manually.\n",
      "WARNING:tensorflow:No training configuration found in save file: the model was *not* compiled. Compile it manually.\n",
      "WARNING:tensorflow:No training configuration found in save file: the model was *not* compiled. Compile it manually.\n",
      "Elapsed/s:  118.73380780220032\n",
      "Final training loss:  tf.Tensor(0.9541069, shape=(), dtype=float32)\n",
      "best val loss:  tf.Tensor(12.7303915, shape=(), dtype=float32)\n",
      "{0: {'parameters': {'learning_rate': 0.0005, 'beta': 1, 'n_hidden': 2, 'n_neurons_classifier': 72, 'codings_size': 169, 'N': 10.0, 'n_hidden_classifier': 1, 'n_neurons': 137}, 'val_loss': <tf.Tensor: shape=(), dtype=float32, numpy=13.031519>}, 1: {'parameters': {'learning_rate': 0.001, 'beta': 10, 'n_hidden': 2, 'n_neurons_classifier': 79, 'codings_size': 122, 'N': 1.0, 'n_hidden_classifier': 2, 'n_neurons': 508}, 'val_loss': <tf.Tensor: shape=(), dtype=float32, numpy=13.413608>}, 2: {'parameters': {'learning_rate': 0.0005, 'beta': 10, 'n_hidden': 2, 'n_neurons_classifier': 94, 'codings_size': 234, 'N': 10.0, 'n_hidden_classifier': 1, 'n_neurons': 152}, 'val_loss': <tf.Tensor: shape=(), dtype=float32, numpy=12.7303915>}}\n",
      "Epoch 0/80\n",
      "WARNING:tensorflow:Layer full_model_mmd is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because it's dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "2240/2278 [============================>.] - Loss for batch: 12.1749WARNING:tensorflow:Layer concatenate_1 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because it's dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "2278/2278 [==============================] - trainLoss: 12.1749  Val_loss: 22.7623 \n",
      "Epoch 1/80\n",
      "2278/2278 [==============================] - trainLoss: 6.0149  Val_loss: 21.7590 \n",
      "Epoch 2/80\n",
      "2278/2278 [==============================] - trainLoss: 4.5079  Val_loss: 21.3839 \n",
      "Epoch 3/80\n",
      "2278/2278 [==============================] - trainLoss: 3.7935  Val_loss: 20.8516 \n",
      "Epoch 4/80\n",
      "2278/2278 [==============================] - trainLoss: 3.3314  Val_loss: 20.3195 \n",
      "Epoch 5/80\n",
      "2278/2278 [==============================] - trainLoss: 2.8786  Val_loss: 20.2189 \n",
      "Epoch 6/80\n",
      "2278/2278 [==============================] - trainLoss: 2.3952  Val_loss: 19.9010 \n",
      "Epoch 7/80\n",
      "2278/2278 [==============================] - trainLoss: 2.0689  Val_loss: 19.8894 \n",
      "Epoch 8/80\n",
      "2278/2278 [==============================] - trainLoss: 1.8727  Val_loss: 19.9713 \n",
      "Epoch 9/80\n",
      "2278/2278 [==============================] - trainLoss: 1.5969  Val_loss: 20.0268 \n",
      "Epoch 10/80\n",
      "2278/2278 [==============================] - trainLoss: 1.5414  Val_loss: 19.6269 \n",
      "Epoch 11/80\n",
      "2278/2278 [==============================] - trainLoss: 1.2584  Val_loss: 19.9997 \n",
      "Epoch 12/80\n",
      "2278/2278 [==============================] - trainLoss: 1.1907  Val_loss: 19.6079 \n",
      "Epoch 13/80\n",
      "2278/2278 [==============================] - trainLoss: 1.0697  Val_loss: 19.7925 \n",
      "Epoch 14/80\n",
      "2278/2278 [==============================] - trainLoss: 0.9044  Val_loss: 19.4322 \n",
      "Epoch 15/80\n",
      "2278/2278 [==============================] - trainLoss: 0.9851  Val_loss: 19.3779 \n",
      "Epoch 16/80\n",
      "2278/2278 [==============================] - trainLoss: 0.9190  Val_loss: 19.3821 \n",
      "Epoch 17/80\n",
      "2278/2278 [==============================] - trainLoss: 0.7702  Val_loss: 19.1514 \n",
      "Epoch 18/80\n",
      "2278/2278 [==============================] - trainLoss: 0.6531  Val_loss: 19.4072 \n",
      "Epoch 19/80\n",
      "2278/2278 [==============================] - trainLoss: 0.6296  Val_loss: 19.4279 \n",
      "Epoch 20/80\n",
      "2278/2278 [==============================] - trainLoss: 0.5742  Val_loss: 19.3603 \n",
      "Epoch 21/80\n",
      "2278/2278 [==============================] - trainLoss: 0.6190  Val_loss: 19.1864 \n",
      "Epoch 22/80\n",
      "2278/2278 [==============================] - trainLoss: 0.4767  Val_loss: 19.0976 \n",
      "Epoch 23/80\n",
      "2278/2278 [==============================] - trainLoss: 0.4928  Val_loss: 18.9975 \n",
      "Epoch 24/80\n",
      "2278/2278 [==============================] - trainLoss: 0.4699  Val_loss: 18.6853 \n",
      "Epoch 25/80\n",
      "2278/2278 [==============================] - trainLoss: 0.4632  Val_loss: 18.6697 \n",
      "Epoch 26/80\n",
      "2278/2278 [==============================] - trainLoss: 0.4950  Val_loss: 18.4784 \n",
      "Epoch 27/80\n",
      "2278/2278 [==============================] - trainLoss: 0.4076  Val_loss: 18.6092 \n",
      "Epoch 28/80\n",
      "2278/2278 [==============================] - trainLoss: 0.4257  Val_loss: 18.5836 \n",
      "Epoch 29/80\n",
      "2278/2278 [==============================] - trainLoss: 0.2830  Val_loss: 18.6535 \n",
      "Epoch 30/80\n",
      "2278/2278 [==============================] - trainLoss: 0.2588  Val_loss: 18.3932 \n",
      "Epoch 31/80\n",
      "2278/2278 [==============================] - trainLoss: 0.2433  Val_loss: 18.4672 \n",
      "Epoch 32/80\n",
      "2278/2278 [==============================] - trainLoss: 0.2803  Val_loss: 18.4160 \n",
      "Epoch 33/80\n",
      "2278/2278 [==============================] - trainLoss: 0.2881  Val_loss: 18.3445 \n",
      "Epoch 34/80\n",
      "2278/2278 [==============================] - trainLoss: 0.3597  Val_loss: 18.1173 \n",
      "Epoch 35/80\n",
      "2278/2278 [==============================] - trainLoss: 0.3315  Val_loss: 18.3307 \n",
      "Epoch 36/80\n",
      "2278/2278 [==============================] - trainLoss: 0.3052  Val_loss: 18.2159 \n",
      "Epoch 37/80\n",
      "2278/2278 [==============================] - trainLoss: 0.2565  Val_loss: 18.2327 \n",
      "Epoch 38/80\n",
      "2278/2278 [==============================] - trainLoss: 0.2678  Val_loss: 18.1418 \n",
      "Epoch 39/80\n",
      "2278/2278 [==============================] - trainLoss: 0.1563  Val_loss: 18.0796 \n",
      "Epoch 40/80\n",
      "2278/2278 [==============================] - trainLoss: 0.2317  Val_loss: 18.0601 \n",
      "Epoch 41/80\n",
      "2278/2278 [==============================] - trainLoss: 0.2207  Val_loss: 18.1688 \n",
      "Epoch 42/80\n",
      "2278/2278 [==============================] - trainLoss: 0.2011  Val_loss: 18.1194 \n",
      "Epoch 43/80\n",
      "2278/2278 [==============================] - trainLoss: 0.2782  Val_loss: 18.1645 \n",
      "Epoch 44/80\n",
      "2278/2278 [==============================] - trainLoss: 0.1664  Val_loss: 18.0687 \n",
      "Epoch 45/80\n",
      "2278/2278 [==============================] - trainLoss: 0.1008  Val_loss: 18.0228 \n",
      "Epoch 46/80\n",
      "2278/2278 [==============================] - trainLoss: 0.0942  Val_loss: 17.9193 \n",
      "Epoch 47/80\n",
      "2278/2278 [==============================] - trainLoss: 0.0780  Val_loss: 17.9739 \n",
      "Epoch 48/80\n",
      "2278/2278 [==============================] - trainLoss: 0.1312  Val_loss: 17.8203 \n",
      "Epoch 49/80\n",
      "2278/2278 [==============================] - trainLoss: 0.1468  Val_loss: 17.8514 \n",
      "Epoch 50/80\n",
      "2278/2278 [==============================] - trainLoss: 0.0731  Val_loss: 17.8744 \n",
      "Epoch 51/80\n",
      "2278/2278 [==============================] - trainLoss: -0.0110  Val_loss: 17.8287 \n",
      "Epoch 52/80\n",
      "2278/2278 [==============================] - trainLoss: 0.1693  Val_loss: 17.9132 \n",
      "Epoch 53/80\n",
      "2278/2278 [==============================] - trainLoss: 0.1532  Val_loss: 17.6702 \n",
      "Epoch 54/80\n",
      "2278/2278 [==============================] - trainLoss: 0.0789  Val_loss: 17.7482 \n",
      "Epoch 55/80\n",
      "2278/2278 [==============================] - trainLoss: 0.1073  Val_loss: 17.7873 \n",
      "Epoch 56/80\n",
      "2278/2278 [==============================] - trainLoss: 0.1154  Val_loss: 17.6141 \n",
      "Epoch 57/80\n",
      "2278/2278 [==============================] - trainLoss: 0.0214  Val_loss: 17.5432 \n",
      "Epoch 58/80\n",
      "2278/2278 [==============================] - trainLoss: 0.0122  Val_loss: 17.7788 \n",
      "Epoch 59/80\n",
      "2278/2278 [==============================] - trainLoss: 0.1590  Val_loss: 17.6706 \n",
      "Epoch 60/80\n",
      "2278/2278 [==============================] - trainLoss: 0.0694  Val_loss: 17.7180 \n",
      "Epoch 61/80\n",
      "2278/2278 [==============================] - trainLoss: 0.0860  Val_loss: 17.7775 \n",
      "Epoch 62/80\n",
      "2278/2278 [==============================] - trainLoss: 0.1386  Val_loss: 17.6171 \n",
      "Epoch 63/80\n",
      "2278/2278 [==============================] - trainLoss: 0.0605  Val_loss: 17.4718 \n",
      "Epoch 64/80\n",
      "2278/2278 [==============================] - trainLoss: -0.0315  Val_loss: 17.5286 \n",
      "Epoch 65/80\n",
      "2278/2278 [==============================] - trainLoss: 0.0463  Val_loss: 17.7426 \n",
      "Epoch 66/80\n",
      "2278/2278 [==============================] - trainLoss: 0.0396  Val_loss: 17.5997 \n",
      "Epoch 67/80\n",
      "2278/2278 [==============================] - trainLoss: -0.0398  Val_loss: 17.5866 \n",
      "Epoch 68/80\n",
      "2278/2278 [==============================] - trainLoss: 0.0616  Val_loss: 17.4673 \n",
      "Epoch 69/80\n",
      "2278/2278 [==============================] - trainLoss: 0.1255  Val_loss: 17.4834 \n",
      "Epoch 70/80\n",
      "2278/2278 [==============================] - trainLoss: -0.0336  Val_loss: 17.4837 \n",
      "Epoch 71/80\n",
      "2278/2278 [==============================] - trainLoss: -0.0673  Val_loss: 17.5395 \n",
      "Epoch 72/80\n",
      "2278/2278 [==============================] - trainLoss: 0.0338  Val_loss: 17.5319 \n",
      "Epoch 73/80\n",
      "2278/2278 [==============================] - trainLoss: -0.0277  Val_loss: 17.4687 \n",
      "Epoch 74/80\n",
      "2278/2278 [==============================] - trainLoss: -0.0676  Val_loss: 17.4160 \n",
      "Epoch 75/80\n",
      "2278/2278 [==============================] - trainLoss: -0.0403  Val_loss: 17.3930 \n",
      "Epoch 76/80\n",
      "2278/2278 [==============================] - trainLoss: -0.0375  Val_loss: 17.3890 \n",
      "Epoch 77/80\n",
      "2278/2278 [==============================] - trainLoss: 0.1681  Val_loss: 17.5169 \n",
      "Epoch 78/80\n",
      "2278/2278 [==============================] - trainLoss: -0.0034  Val_loss: 17.3653 \n",
      "Epoch 79/80\n",
      "2278/2278 [==============================] - trainLoss: -0.0781  Val_loss: 17.3762 \n",
      "WARNING:tensorflow:No training configuration found in save file: the model was *not* compiled. Compile it manually.\n",
      "WARNING:tensorflow:No training configuration found in save file: the model was *not* compiled. Compile it manually.\n",
      "WARNING:tensorflow:No training configuration found in save file: the model was *not* compiled. Compile it manually.\n",
      "WARNING:tensorflow:No training configuration found in save file: the model was *not* compiled. Compile it manually.\n",
      "Elapsed/s:  104.93493342399597\n",
      "Final training loss:  tf.Tensor(-0.07813839, shape=(), dtype=float32)\n",
      "best val loss:  tf.Tensor(17.365324, shape=(), dtype=float32)\n",
      "{0: {'parameters': {'learning_rate': 0.0005, 'beta': 1, 'n_hidden': 2, 'n_neurons_classifier': 72, 'codings_size': 169, 'N': 10.0, 'n_hidden_classifier': 1, 'n_neurons': 137}, 'val_loss': <tf.Tensor: shape=(), dtype=float32, numpy=13.031519>}, 1: {'parameters': {'learning_rate': 0.001, 'beta': 10, 'n_hidden': 2, 'n_neurons_classifier': 79, 'codings_size': 122, 'N': 1.0, 'n_hidden_classifier': 2, 'n_neurons': 508}, 'val_loss': <tf.Tensor: shape=(), dtype=float32, numpy=13.413608>}, 2: {'parameters': {'learning_rate': 0.0005, 'beta': 10, 'n_hidden': 2, 'n_neurons_classifier': 94, 'codings_size': 234, 'N': 10.0, 'n_hidden_classifier': 1, 'n_neurons': 152}, 'val_loss': <tf.Tensor: shape=(), dtype=float32, numpy=12.7303915>}, 3: {'parameters': {'learning_rate': 0.001, 'beta': 10, 'n_hidden': 1, 'n_neurons_classifier': 22, 'codings_size': 207, 'N': 10.0, 'n_hidden_classifier': 1, 'n_neurons': 380}, 'val_loss': <tf.Tensor: shape=(), dtype=float32, numpy=17.365324>}}\n",
      "Epoch 0/80\n",
      "WARNING:tensorflow:Layer full_model_mmd is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because it's dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "2240/2278 [============================>.] - Loss for batch: 18.5949WARNING:tensorflow:Layer concatenate_1 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because it's dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "2278/2278 [==============================] - trainLoss: 18.5949  Val_loss: 23.5931 \n",
      "Epoch 1/80\n",
      "2278/2278 [==============================] - trainLoss: 9.0164  Val_loss: 22.0501 \n",
      "Epoch 2/80\n",
      "2278/2278 [==============================] - trainLoss: 7.1800  Val_loss: 21.0589 \n",
      "Epoch 3/80\n",
      "2278/2278 [==============================] - trainLoss: 6.0808  Val_loss: 20.2526 \n",
      "Epoch 4/80\n",
      "2278/2278 [==============================] - trainLoss: 5.3202  Val_loss: 19.7236 \n",
      "Epoch 5/80\n",
      "2278/2278 [==============================] - trainLoss: 4.7971  Val_loss: 19.1940 \n",
      "Epoch 6/80\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2278/2278 [==============================] - trainLoss: 4.1408  Val_loss: 18.6233 \n",
      "Epoch 7/80\n",
      "2278/2278 [==============================] - trainLoss: 3.9778  Val_loss: 18.5868 \n",
      "Epoch 8/80\n",
      "2278/2278 [==============================] - trainLoss: 3.5854  Val_loss: 18.3307 \n",
      "Epoch 9/80\n",
      "2278/2278 [==============================] - trainLoss: 3.3928  Val_loss: 18.3910 \n",
      "Epoch 10/80\n",
      "2278/2278 [==============================] - trainLoss: 3.1453  Val_loss: 18.2849 \n",
      "Epoch 11/80\n",
      "2278/2278 [==============================] - trainLoss: 2.9395  Val_loss: 18.1672 \n",
      "Epoch 12/80\n",
      "2278/2278 [==============================] - trainLoss: 2.6681  Val_loss: 17.9881 \n",
      "Epoch 13/80\n",
      "2278/2278 [==============================] - trainLoss: 2.5207  Val_loss: 17.9276 \n",
      "Epoch 14/80\n",
      "2278/2278 [==============================] - trainLoss: 2.3713  Val_loss: 18.0301 \n",
      "Epoch 15/80\n",
      "2278/2278 [==============================] - trainLoss: 2.2861  Val_loss: 17.9689 \n",
      "Epoch 16/80\n",
      "2278/2278 [==============================] - trainLoss: 2.1613  Val_loss: 18.1007 \n",
      "Epoch 17/80\n",
      "2278/2278 [==============================] - trainLoss: 1.9788  Val_loss: 17.9249 \n",
      "Epoch 18/80\n",
      "2278/2278 [==============================] - trainLoss: 1.8547  Val_loss: 18.0752 \n",
      "Epoch 19/80\n",
      "2278/2278 [==============================] - trainLoss: 1.6930  Val_loss: 18.2101 \n",
      "Epoch 20/80\n",
      "2278/2278 [==============================] - trainLoss: 1.5628  Val_loss: 18.3123 \n",
      "Epoch 21/80\n",
      "2278/2278 [==============================] - trainLoss: 1.4996  Val_loss: 18.5965 \n",
      "Epoch 22/80\n",
      "2278/2278 [==============================] - trainLoss: 1.4814  Val_loss: 18.3200 \n",
      "Epoch 23/80\n",
      "2278/2278 [==============================] - trainLoss: 1.2926  Val_loss: 18.4562 \n",
      "Epoch 24/80\n",
      "2278/2278 [==============================] - trainLoss: 1.2919  Val_loss: 18.5435 \n",
      "Epoch 25/80\n",
      "2278/2278 [==============================] - trainLoss: 1.1615  Val_loss: 18.7948 \n",
      "Epoch 26/80\n",
      "2278/2278 [==============================] - trainLoss: 1.1806  Val_loss: 18.7084 \n",
      "WARNING:tensorflow:No training configuration found in save file: the model was *not* compiled. Compile it manually.\n",
      "WARNING:tensorflow:No training configuration found in save file: the model was *not* compiled. Compile it manually.\n",
      "WARNING:tensorflow:No training configuration found in save file: the model was *not* compiled. Compile it manually.\n",
      "WARNING:tensorflow:No training configuration found in save file: the model was *not* compiled. Compile it manually.\n",
      "Elapsed/s:  35.939977407455444\n",
      "Final training loss:  tf.Tensor(1.1806481, shape=(), dtype=float32)\n",
      "best val loss:  tf.Tensor(17.92488, shape=(), dtype=float32)\n",
      "{0: {'parameters': {'learning_rate': 0.0005, 'beta': 1, 'n_hidden': 2, 'n_neurons_classifier': 72, 'codings_size': 169, 'N': 10.0, 'n_hidden_classifier': 1, 'n_neurons': 137}, 'val_loss': <tf.Tensor: shape=(), dtype=float32, numpy=13.031519>}, 1: {'parameters': {'learning_rate': 0.001, 'beta': 10, 'n_hidden': 2, 'n_neurons_classifier': 79, 'codings_size': 122, 'N': 1.0, 'n_hidden_classifier': 2, 'n_neurons': 508}, 'val_loss': <tf.Tensor: shape=(), dtype=float32, numpy=13.413608>}, 2: {'parameters': {'learning_rate': 0.0005, 'beta': 10, 'n_hidden': 2, 'n_neurons_classifier': 94, 'codings_size': 234, 'N': 10.0, 'n_hidden_classifier': 1, 'n_neurons': 152}, 'val_loss': <tf.Tensor: shape=(), dtype=float32, numpy=12.7303915>}, 3: {'parameters': {'learning_rate': 0.001, 'beta': 10, 'n_hidden': 1, 'n_neurons_classifier': 22, 'codings_size': 207, 'N': 10.0, 'n_hidden_classifier': 1, 'n_neurons': 380}, 'val_loss': <tf.Tensor: shape=(), dtype=float32, numpy=17.365324>}, 4: {'parameters': {'learning_rate': 0.0005, 'beta': 15, 'n_hidden': 1, 'n_neurons_classifier': 95, 'codings_size': 112, 'N': 10.0, 'n_hidden_classifier': 2, 'n_neurons': 409}, 'val_loss': <tf.Tensor: shape=(), dtype=float32, numpy=17.92488>}}\n",
      "Epoch 0/80\n",
      "WARNING:tensorflow:Layer full_model_mmd is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because it's dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "2240/2278 [============================>.] - Loss for batch: 18.2953WARNING:tensorflow:Layer concatenate_1 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because it's dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "2278/2278 [==============================] - trainLoss: 18.2953  Val_loss: 20.6236 \n",
      "Epoch 1/80\n",
      "2278/2278 [==============================] - trainLoss: 9.6053  Val_loss: 18.3816 \n",
      "Epoch 2/80\n",
      "2278/2278 [==============================] - trainLoss: 7.8228  Val_loss: 17.1884 \n",
      "Epoch 3/80\n",
      "2278/2278 [==============================] - trainLoss: 6.6741  Val_loss: 16.2766 \n",
      "Epoch 4/80\n",
      "2278/2278 [==============================] - trainLoss: 5.9541  Val_loss: 15.7158 \n",
      "Epoch 5/80\n",
      "2278/2278 [==============================] - trainLoss: 5.3785  Val_loss: 15.2659 \n",
      "Epoch 6/80\n",
      "2278/2278 [==============================] - trainLoss: 4.9771  Val_loss: 15.2263 \n",
      "Epoch 7/80\n",
      "2278/2278 [==============================] - trainLoss: 4.7098  Val_loss: 15.0841 \n",
      "Epoch 8/80\n",
      "2278/2278 [==============================] - trainLoss: 4.4017  Val_loss: 15.0901 \n",
      "Epoch 9/80\n",
      "2278/2278 [==============================] - trainLoss: 4.2331  Val_loss: 15.0128 \n",
      "Epoch 10/80\n",
      "2278/2278 [==============================] - trainLoss: 4.0365  Val_loss: 14.9902 \n",
      "Epoch 11/80\n",
      "2278/2278 [==============================] - trainLoss: 3.8776  Val_loss: 15.2439 \n",
      "Epoch 12/80\n",
      "2278/2278 [==============================] - trainLoss: 3.7774  Val_loss: 15.2051 \n",
      "Epoch 13/80\n",
      "2278/2278 [==============================] - trainLoss: 3.5680  Val_loss: 15.3055 \n",
      "Epoch 14/80\n",
      "2278/2278 [==============================] - trainLoss: 3.4571  Val_loss: 15.3218 \n",
      "Epoch 15/80\n",
      "2278/2278 [==============================] - trainLoss: 3.2700  Val_loss: 15.4041 \n",
      "Epoch 16/80\n",
      "2278/2278 [==============================] - trainLoss: 3.2537  Val_loss: 15.5548 \n",
      "Epoch 17/80\n",
      "2278/2278 [==============================] - trainLoss: 3.1375  Val_loss: 15.5734 \n",
      "Epoch 18/80\n",
      "2278/2278 [==============================] - trainLoss: 3.0445  Val_loss: 15.7934 \n",
      "Epoch 19/80\n",
      "2278/2278 [==============================] - trainLoss: 2.9356  Val_loss: 15.9218 \n",
      "WARNING:tensorflow:No training configuration found in save file: the model was *not* compiled. Compile it manually.\n",
      "WARNING:tensorflow:No training configuration found in save file: the model was *not* compiled. Compile it manually.\n",
      "WARNING:tensorflow:No training configuration found in save file: the model was *not* compiled. Compile it manually.\n",
      "WARNING:tensorflow:No training configuration found in save file: the model was *not* compiled. Compile it manually.\n",
      "Elapsed/s:  26.09568738937378\n",
      "Final training loss:  tf.Tensor(2.9356198, shape=(), dtype=float32)\n",
      "best val loss:  tf.Tensor(14.990203, shape=(), dtype=float32)\n",
      "{0: {'parameters': {'learning_rate': 0.0005, 'beta': 1, 'n_hidden': 2, 'n_neurons_classifier': 72, 'codings_size': 169, 'N': 10.0, 'n_hidden_classifier': 1, 'n_neurons': 137}, 'val_loss': <tf.Tensor: shape=(), dtype=float32, numpy=13.031519>}, 1: {'parameters': {'learning_rate': 0.001, 'beta': 10, 'n_hidden': 2, 'n_neurons_classifier': 79, 'codings_size': 122, 'N': 1.0, 'n_hidden_classifier': 2, 'n_neurons': 508}, 'val_loss': <tf.Tensor: shape=(), dtype=float32, numpy=13.413608>}, 2: {'parameters': {'learning_rate': 0.0005, 'beta': 10, 'n_hidden': 2, 'n_neurons_classifier': 94, 'codings_size': 234, 'N': 10.0, 'n_hidden_classifier': 1, 'n_neurons': 152}, 'val_loss': <tf.Tensor: shape=(), dtype=float32, numpy=12.7303915>}, 3: {'parameters': {'learning_rate': 0.001, 'beta': 10, 'n_hidden': 1, 'n_neurons_classifier': 22, 'codings_size': 207, 'N': 10.0, 'n_hidden_classifier': 1, 'n_neurons': 380}, 'val_loss': <tf.Tensor: shape=(), dtype=float32, numpy=17.365324>}, 4: {'parameters': {'learning_rate': 0.0005, 'beta': 15, 'n_hidden': 1, 'n_neurons_classifier': 95, 'codings_size': 112, 'N': 10.0, 'n_hidden_classifier': 2, 'n_neurons': 409}, 'val_loss': <tf.Tensor: shape=(), dtype=float32, numpy=17.92488>}, 5: {'parameters': {'learning_rate': 0.0005, 'beta': 15, 'n_hidden': 1, 'n_neurons_classifier': 71, 'codings_size': 40, 'N': 0.1, 'n_hidden_classifier': 1, 'n_neurons': 398}, 'val_loss': <tf.Tensor: shape=(), dtype=float32, numpy=14.990203>}}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0/80\n",
      "WARNING:tensorflow:Layer full_model_mmd is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because it's dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "2240/2278 [============================>.] - Loss for batch: nanWARNING:tensorflow:Layer concatenate_1 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because it's dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "2278/2278 [==============================] - trainLoss: nan  Val_loss: nan \n",
      "Epoch 1/80\n",
      "2278/2278 [==============================] - trainLoss: nan  Val_loss: nan \n",
      "Epoch 2/80\n",
      "2278/2278 [==============================] - trainLoss: nan  Val_loss: nan \n",
      "Epoch 3/80\n",
      "2278/2278 [==============================] - trainLoss: nan  Val_loss: nan \n",
      "Epoch 4/80\n",
      "2278/2278 [==============================] - trainLoss: nan  Val_loss: nan \n",
      "Epoch 5/80\n",
      "2278/2278 [==============================] - trainLoss: nan  Val_loss: nan \n",
      "Epoch 6/80\n",
      "2278/2278 [==============================] - trainLoss: nan  Val_loss: nan \n",
      "Epoch 7/80\n",
      "2278/2278 [==============================] - trainLoss: nan  Val_loss: nan \n",
      "Epoch 8/80\n",
      "2278/2278 [==============================] - trainLoss: nan  Val_loss: nan \n",
      "Epoch 9/80\n",
      "2278/2278 [==============================] - trainLoss: nan  Val_loss: nan \n",
      "Epoch 10/80\n",
      "2278/2278 [==============================] - trainLoss: nan  Val_loss: nan \n",
      "Epoch 11/80\n",
      "2278/2278 [==============================] - trainLoss: nan  Val_loss: nan \n",
      "Epoch 12/80\n",
      "2278/2278 [==============================] - trainLoss: nan  Val_loss: nan \n",
      "Epoch 13/80\n",
      "2278/2278 [==============================] - trainLoss: nan  Val_loss: nan \n",
      "Epoch 14/80\n",
      "2278/2278 [==============================] - trainLoss: nan  Val_loss: nan \n",
      "Epoch 15/80\n",
      "2278/2278 [==============================] - trainLoss: nan  Val_loss: nan \n",
      "Epoch 16/80\n",
      "2278/2278 [==============================] - trainLoss: nan  Val_loss: nan \n",
      "Epoch 17/80\n",
      "2278/2278 [==============================] - trainLoss: nan  Val_loss: nan \n",
      "Epoch 18/80\n",
      "2278/2278 [==============================] - trainLoss: nan  Val_loss: nan \n",
      "Epoch 19/80\n",
      "2278/2278 [==============================] - trainLoss: nan  Val_loss: nan \n",
      "Epoch 20/80\n",
      "2278/2278 [==============================] - trainLoss: nan  Val_loss: nan \n",
      "Epoch 21/80\n",
      "2278/2278 [==============================] - trainLoss: nan  Val_loss: nan \n",
      "Epoch 22/80\n",
      "2278/2278 [==============================] - trainLoss: nan  Val_loss: nan \n",
      "Epoch 23/80\n",
      "2278/2278 [==============================] - trainLoss: nan  Val_loss: nan \n",
      "Epoch 24/80\n",
      "2278/2278 [==============================] - trainLoss: nan  Val_loss: nan \n",
      "Epoch 25/80\n",
      "2278/2278 [==============================] - trainLoss: nan  Val_loss: nan \n",
      "Epoch 26/80\n",
      "2278/2278 [==============================] - trainLoss: nan  Val_loss: nan \n",
      "Epoch 27/80\n",
      "2278/2278 [==============================] - trainLoss: nan  Val_loss: nan \n",
      "Epoch 28/80\n",
      "2278/2278 [==============================] - trainLoss: nan  Val_loss: nan \n",
      "Epoch 29/80\n",
      "2278/2278 [==============================] - trainLoss: nan  Val_loss: nan \n",
      "Epoch 30/80\n",
      "2278/2278 [==============================] - trainLoss: nan  Val_loss: nan \n",
      "Epoch 31/80\n",
      "2278/2278 [==============================] - trainLoss: nan  Val_loss: nan \n",
      "Epoch 32/80\n",
      "2278/2278 [==============================] - trainLoss: nan  Val_loss: nan \n",
      "Epoch 33/80\n",
      "2278/2278 [==============================] - trainLoss: nan  Val_loss: nan \n",
      "Epoch 34/80\n",
      "2278/2278 [==============================] - trainLoss: nan  Val_loss: nan \n",
      "Epoch 35/80\n",
      "2278/2278 [==============================] - trainLoss: nan  Val_loss: nan \n",
      "Epoch 36/80\n",
      "2278/2278 [==============================] - trainLoss: nan  Val_loss: nan \n",
      "Epoch 37/80\n",
      "2278/2278 [==============================] - trainLoss: nan  Val_loss: nan \n",
      "Epoch 38/80\n",
      "2278/2278 [==============================] - trainLoss: nan  Val_loss: nan \n",
      "Epoch 39/80\n",
      "2278/2278 [==============================] - trainLoss: nan  Val_loss: nan \n",
      "Epoch 40/80\n",
      "2278/2278 [==============================] - trainLoss: nan  Val_loss: nan \n",
      "Epoch 41/80\n",
      "2278/2278 [==============================] - trainLoss: nan  Val_loss: nan \n",
      "Epoch 42/80\n",
      "2278/2278 [==============================] - trainLoss: nan  Val_loss: nan \n",
      "Epoch 43/80\n",
      "2278/2278 [==============================] - trainLoss: nan  Val_loss: nan \n",
      "Epoch 44/80\n",
      "2278/2278 [==============================] - trainLoss: nan  Val_loss: nan \n",
      "Epoch 45/80\n",
      "2278/2278 [==============================] - trainLoss: nan  Val_loss: nan \n",
      "Epoch 46/80\n",
      "2278/2278 [==============================] - trainLoss: nan  Val_loss: nan \n",
      "Epoch 47/80\n",
      "2278/2278 [==============================] - trainLoss: nan  Val_loss: nan \n",
      "Epoch 48/80\n",
      "2278/2278 [==============================] - trainLoss: nan  Val_loss: nan \n",
      "Epoch 49/80\n",
      "2278/2278 [==============================] - trainLoss: nan  Val_loss: nan \n",
      "Epoch 50/80\n",
      "2278/2278 [==============================] - trainLoss: nan  Val_loss: nan \n",
      "Epoch 51/80\n",
      "2278/2278 [==============================] - trainLoss: nan  Val_loss: nan \n",
      "Epoch 52/80\n",
      "2278/2278 [==============================] - trainLoss: nan  Val_loss: nan \n",
      "Epoch 53/80\n",
      "2278/2278 [==============================] - trainLoss: nan  Val_loss: nan \n",
      "Epoch 54/80\n",
      "2278/2278 [==============================] - trainLoss: nan  Val_loss: nan \n",
      "Epoch 55/80\n",
      "2278/2278 [==============================] - trainLoss: nan  Val_loss: nan \n",
      "Epoch 56/80\n",
      "2278/2278 [==============================] - trainLoss: nan  Val_loss: nan \n",
      "Epoch 57/80\n",
      "2278/2278 [==============================] - trainLoss: nan  Val_loss: nan \n",
      "Epoch 58/80\n",
      "2278/2278 [==============================] - trainLoss: nan  Val_loss: nan \n",
      "Epoch 59/80\n",
      "2278/2278 [==============================] - trainLoss: nan  Val_loss: nan \n",
      "Epoch 60/80\n",
      "2278/2278 [==============================] - trainLoss: nan  Val_loss: nan \n",
      "Epoch 61/80\n",
      "2278/2278 [==============================] - trainLoss: nan  Val_loss: nan \n",
      "Epoch 62/80\n",
      "2278/2278 [==============================] - trainLoss: nan  Val_loss: nan \n",
      "Epoch 63/80\n",
      "2278/2278 [==============================] - trainLoss: nan  Val_loss: nan \n",
      "Epoch 64/80\n",
      "2278/2278 [==============================] - trainLoss: nan  Val_loss: nan \n",
      "Epoch 65/80\n",
      "2278/2278 [==============================] - trainLoss: nan  Val_loss: nan \n",
      "Epoch 66/80\n",
      "2278/2278 [==============================] - trainLoss: nan  Val_loss: nan \n",
      "Epoch 67/80\n",
      "2278/2278 [==============================] - trainLoss: nan  Val_loss: nan \n",
      "Epoch 68/80\n",
      "2278/2278 [==============================] - trainLoss: nan  Val_loss: nan \n",
      "Epoch 69/80\n",
      "2278/2278 [==============================] - trainLoss: nan  Val_loss: nan \n",
      "Epoch 70/80\n",
      "2278/2278 [==============================] - trainLoss: nan  Val_loss: nan \n",
      "Epoch 71/80\n",
      "2278/2278 [==============================] - trainLoss: nan  Val_loss: nan \n",
      "Epoch 72/80\n",
      "2278/2278 [==============================] - trainLoss: nan  Val_loss: nan \n",
      "Epoch 73/80\n",
      "2278/2278 [==============================] - trainLoss: nan  Val_loss: nan \n",
      "Epoch 74/80\n",
      "2278/2278 [==============================] - trainLoss: nan  Val_loss: nan \n",
      "Epoch 75/80\n",
      "2278/2278 [==============================] - trainLoss: nan  Val_loss: nan \n",
      "Epoch 76/80\n",
      "2278/2278 [==============================] - trainLoss: nan  Val_loss: nan \n",
      "Epoch 77/80\n",
      "2278/2278 [==============================] - trainLoss: nan  Val_loss: nan \n",
      "Epoch 78/80\n",
      "2278/2278 [==============================] - trainLoss: nan  Val_loss: nan \n",
      "Epoch 79/80\n",
      "2278/2278 [==============================] - trainLoss: nan  Val_loss: nan \n",
      "WARNING:tensorflow:No training configuration found in save file: the model was *not* compiled. Compile it manually.\n",
      "WARNING:tensorflow:No training configuration found in save file: the model was *not* compiled. Compile it manually.\n",
      "WARNING:tensorflow:No training configuration found in save file: the model was *not* compiled. Compile it manually.\n",
      "WARNING:tensorflow:No training configuration found in save file: the model was *not* compiled. Compile it manually.\n",
      "Elapsed/s:  92.99947929382324\n",
      "Final training loss:  tf.Tensor(nan, shape=(), dtype=float32)\n",
      "best val loss:  tf.Tensor(nan, shape=(), dtype=float32)\n",
      "{0: {'parameters': {'learning_rate': 0.0005, 'beta': 1, 'n_hidden': 2, 'n_neurons_classifier': 72, 'codings_size': 169, 'N': 10.0, 'n_hidden_classifier': 1, 'n_neurons': 137}, 'val_loss': <tf.Tensor: shape=(), dtype=float32, numpy=13.031519>}, 1: {'parameters': {'learning_rate': 0.001, 'beta': 10, 'n_hidden': 2, 'n_neurons_classifier': 79, 'codings_size': 122, 'N': 1.0, 'n_hidden_classifier': 2, 'n_neurons': 508}, 'val_loss': <tf.Tensor: shape=(), dtype=float32, numpy=13.413608>}, 2: {'parameters': {'learning_rate': 0.0005, 'beta': 10, 'n_hidden': 2, 'n_neurons_classifier': 94, 'codings_size': 234, 'N': 10.0, 'n_hidden_classifier': 1, 'n_neurons': 152}, 'val_loss': <tf.Tensor: shape=(), dtype=float32, numpy=12.7303915>}, 3: {'parameters': {'learning_rate': 0.001, 'beta': 10, 'n_hidden': 1, 'n_neurons_classifier': 22, 'codings_size': 207, 'N': 10.0, 'n_hidden_classifier': 1, 'n_neurons': 380}, 'val_loss': <tf.Tensor: shape=(), dtype=float32, numpy=17.365324>}, 4: {'parameters': {'learning_rate': 0.0005, 'beta': 15, 'n_hidden': 1, 'n_neurons_classifier': 95, 'codings_size': 112, 'N': 10.0, 'n_hidden_classifier': 2, 'n_neurons': 409}, 'val_loss': <tf.Tensor: shape=(), dtype=float32, numpy=17.92488>}, 5: {'parameters': {'learning_rate': 0.0005, 'beta': 15, 'n_hidden': 1, 'n_neurons_classifier': 71, 'codings_size': 40, 'N': 0.1, 'n_hidden_classifier': 1, 'n_neurons': 398}, 'val_loss': <tf.Tensor: shape=(), dtype=float32, numpy=14.990203>}, 6: {'parameters': {'learning_rate': 0.0005, 'beta': 15, 'n_hidden': 1, 'n_neurons_classifier': 80, 'codings_size': 34, 'N': 1.0, 'n_hidden_classifier': 1, 'n_neurons': 238}, 'val_loss': <tf.Tensor: shape=(), dtype=float32, numpy=nan>}}\n",
      "Epoch 0/80\n",
      "WARNING:tensorflow:Layer full_model_mmd is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because it's dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "2240/2278 [============================>.] - Loss for batch: 24.2293WARNING:tensorflow:Layer concatenate_1 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because it's dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "2278/2278 [==============================] - trainLoss: 24.2293  Val_loss: 19.5760 \n",
      "Epoch 1/80\n",
      "2278/2278 [==============================] - trainLoss: 10.9302  Val_loss: 18.0762 \n",
      "Epoch 2/80\n",
      "2278/2278 [==============================] - trainLoss: 8.2312  Val_loss: 17.4953 \n",
      "Epoch 3/80\n",
      "2278/2278 [==============================] - trainLoss: 7.0151  Val_loss: 17.1089 \n",
      "Epoch 4/80\n",
      "2278/2278 [==============================] - trainLoss: 6.3546  Val_loss: 16.6832 \n",
      "Epoch 5/80\n",
      "2278/2278 [==============================] - trainLoss: 5.8125  Val_loss: 16.3992 \n",
      "Epoch 6/80\n",
      "2278/2278 [==============================] - trainLoss: 5.3624  Val_loss: 16.2301 \n",
      "Epoch 7/80\n",
      "2278/2278 [==============================] - trainLoss: 5.0182  Val_loss: 15.9026 \n",
      "Epoch 8/80\n",
      "2278/2278 [==============================] - trainLoss: 4.6755  Val_loss: 15.8286 \n",
      "Epoch 9/80\n",
      "2278/2278 [==============================] - trainLoss: 4.5175  Val_loss: 15.5832 \n",
      "Epoch 10/80\n",
      "2278/2278 [==============================] - trainLoss: 4.1540  Val_loss: 15.5335 \n",
      "Epoch 11/80\n",
      "2278/2278 [==============================] - trainLoss: 3.8719  Val_loss: 15.1990 \n",
      "Epoch 12/80\n",
      "2278/2278 [==============================] - trainLoss: 3.6537  Val_loss: 15.0713 \n",
      "Epoch 13/80\n",
      "2278/2278 [==============================] - trainLoss: 3.5337  Val_loss: 14.9391 \n",
      "Epoch 14/80\n",
      "2278/2278 [==============================] - trainLoss: 3.1805  Val_loss: 14.9361 \n",
      "Epoch 15/80\n",
      "2278/2278 [==============================] - trainLoss: 2.9789  Val_loss: 14.8176 \n",
      "Epoch 16/80\n",
      "2278/2278 [==============================] - trainLoss: 2.8836  Val_loss: 14.6449 \n",
      "Epoch 17/80\n",
      "2278/2278 [==============================] - trainLoss: 2.6661  Val_loss: 14.5871 \n",
      "Epoch 18/80\n",
      "2278/2278 [==============================] - trainLoss: 2.8308  Val_loss: 14.3606 \n",
      "Epoch 19/80\n",
      "2278/2278 [==============================] - trainLoss: 2.3706  Val_loss: 14.2933 \n",
      "Epoch 20/80\n",
      "2278/2278 [==============================] - trainLoss: 2.3135  Val_loss: 14.2684 \n",
      "Epoch 21/80\n",
      "2278/2278 [==============================] - trainLoss: 2.1646  Val_loss: 14.0969 \n",
      "Epoch 22/80\n",
      "2278/2278 [==============================] - trainLoss: 2.1188  Val_loss: 13.9905 \n",
      "Epoch 23/80\n",
      "2278/2278 [==============================] - trainLoss: 2.1183  Val_loss: 13.9698 \n",
      "Epoch 24/80\n",
      "2278/2278 [==============================] - trainLoss: 1.9580  Val_loss: 14.0554 \n",
      "Epoch 25/80\n",
      "2278/2278 [==============================] - trainLoss: 1.9680  Val_loss: 13.8778 \n",
      "Epoch 26/80\n",
      "2278/2278 [==============================] - trainLoss: 1.8354  Val_loss: 13.9015 \n",
      "Epoch 27/80\n",
      "2278/2278 [==============================] - trainLoss: 1.7001  Val_loss: 13.8435 \n",
      "Epoch 28/80\n",
      "2278/2278 [==============================] - trainLoss: 1.7282  Val_loss: 13.7738 \n",
      "Epoch 29/80\n",
      "2278/2278 [==============================] - trainLoss: 1.7980  Val_loss: 13.7422 \n",
      "Epoch 30/80\n",
      "2278/2278 [==============================] - trainLoss: 1.6222  Val_loss: 13.6882 \n",
      "Epoch 31/80\n",
      "2278/2278 [==============================] - trainLoss: 1.6130  Val_loss: 13.5918 \n",
      "Epoch 32/80\n",
      "2278/2278 [==============================] - trainLoss: 1.6940  Val_loss: 13.4680 \n",
      "Epoch 33/80\n",
      "2278/2278 [==============================] - trainLoss: 1.5327  Val_loss: 13.5532 \n",
      "Epoch 34/80\n",
      "2278/2278 [==============================] - trainLoss: 1.4756  Val_loss: 13.4440 \n",
      "Epoch 35/80\n",
      "2278/2278 [==============================] - trainLoss: 1.3968  Val_loss: 13.3803 \n",
      "Epoch 36/80\n",
      "2278/2278 [==============================] - trainLoss: 1.4270  Val_loss: 13.3994 \n",
      "Epoch 37/80\n",
      "2278/2278 [==============================] - trainLoss: 1.2823  Val_loss: 13.2649 \n",
      "Epoch 38/80\n",
      "2278/2278 [==============================] - trainLoss: 1.3010  Val_loss: 13.4272 \n",
      "Epoch 39/80\n",
      "2278/2278 [==============================] - trainLoss: 1.2895  Val_loss: 13.3467 \n",
      "Epoch 40/80\n",
      "2278/2278 [==============================] - trainLoss: 1.2933  Val_loss: 13.3123 \n",
      "Epoch 41/80\n",
      "2278/2278 [==============================] - trainLoss: 1.3221  Val_loss: 13.2171 \n",
      "Epoch 42/80\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2278/2278 [==============================] - trainLoss: 1.1379  Val_loss: 13.1087 \n",
      "Epoch 43/80\n",
      "2278/2278 [==============================] - trainLoss: 1.2069  Val_loss: 13.2484 \n",
      "Epoch 44/80\n",
      "2278/2278 [==============================] - trainLoss: 1.1511  Val_loss: 13.1804 \n",
      "Epoch 45/80\n",
      "2278/2278 [==============================] - trainLoss: 1.1348  Val_loss: 13.1569 \n",
      "Epoch 46/80\n",
      "2278/2278 [==============================] - trainLoss: 1.1242  Val_loss: 13.1283 \n",
      "Epoch 47/80\n",
      "2278/2278 [==============================] - trainLoss: 1.1384  Val_loss: 13.0505 \n",
      "Epoch 48/80\n",
      "2278/2278 [==============================] - trainLoss: 1.1550  Val_loss: 13.0147 \n",
      "Epoch 49/80\n",
      "2278/2278 [==============================] - trainLoss: 1.0787  Val_loss: 13.0524 \n",
      "Epoch 50/80\n",
      "2278/2278 [==============================] - trainLoss: 1.1467  Val_loss: 13.0378 \n",
      "Epoch 51/80\n",
      "2278/2278 [==============================] - trainLoss: 0.9889  Val_loss: 13.0610 \n",
      "Epoch 52/80\n",
      "2278/2278 [==============================] - trainLoss: 1.1254  Val_loss: 13.0400 \n",
      "Epoch 53/80\n",
      "2278/2278 [==============================] - trainLoss: 1.0565  Val_loss: 13.0824 \n",
      "Epoch 54/80\n",
      "2278/2278 [==============================] - trainLoss: 1.0512  Val_loss: 12.9793 \n",
      "Epoch 55/80\n",
      "2278/2278 [==============================] - trainLoss: 0.9778  Val_loss: 12.9762 \n",
      "Epoch 56/80\n",
      "2278/2278 [==============================] - trainLoss: 1.0949  Val_loss: 12.9943 \n",
      "Epoch 57/80\n",
      "2278/2278 [==============================] - trainLoss: 1.0740  Val_loss: 13.0497 \n",
      "Epoch 58/80\n",
      "2278/2278 [==============================] - trainLoss: 0.9357  Val_loss: 13.0657 \n",
      "Epoch 59/80\n",
      "2278/2278 [==============================] - trainLoss: 0.9773  Val_loss: 12.9283 \n",
      "Epoch 60/80\n",
      "2278/2278 [==============================] - trainLoss: 0.9342  Val_loss: 12.9915 \n",
      "Epoch 61/80\n",
      "2278/2278 [==============================] - trainLoss: 1.0446  Val_loss: 13.0071 \n",
      "Epoch 62/80\n",
      "2278/2278 [==============================] - trainLoss: 0.8239  Val_loss: 12.8924 \n",
      "Epoch 63/80\n",
      "2278/2278 [==============================] - trainLoss: 0.9275  Val_loss: 12.8986 \n",
      "Epoch 64/80\n",
      "2278/2278 [==============================] - trainLoss: 1.0415  Val_loss: 12.8713 \n",
      "Epoch 65/80\n",
      "2278/2278 [==============================] - trainLoss: 0.8832  Val_loss: 12.8272 \n",
      "Epoch 66/80\n",
      "2278/2278 [==============================] - trainLoss: 0.9487  Val_loss: 12.8143 \n",
      "Epoch 67/80\n",
      "2278/2278 [==============================] - trainLoss: 0.8201  Val_loss: 12.8173 \n",
      "Epoch 68/80\n",
      "2278/2278 [==============================] - trainLoss: 0.7998  Val_loss: 12.7959 \n",
      "Epoch 69/80\n",
      "2278/2278 [==============================] - trainLoss: 0.9245  Val_loss: 12.8900 \n",
      "Epoch 70/80\n",
      "2278/2278 [==============================] - trainLoss: 0.9282  Val_loss: 12.7851 \n",
      "Epoch 71/80\n",
      "2278/2278 [==============================] - trainLoss: 0.8923  Val_loss: 12.8863 \n",
      "Epoch 72/80\n",
      "2278/2278 [==============================] - trainLoss: 0.8489  Val_loss: 12.9241 \n",
      "Epoch 73/80\n",
      "2278/2278 [==============================] - trainLoss: 0.8329  Val_loss: 12.8764 \n",
      "Epoch 74/80\n",
      "2278/2278 [==============================] - trainLoss: 0.8978  Val_loss: 12.8008 \n",
      "Epoch 75/80\n",
      "2278/2278 [==============================] - trainLoss: 0.7977  Val_loss: 12.7720 \n",
      "Epoch 76/80\n",
      "2278/2278 [==============================] - trainLoss: 0.7352  Val_loss: 12.7983 \n",
      "Epoch 77/80\n",
      "2278/2278 [==============================] - trainLoss: 0.7693  Val_loss: 12.7376 \n",
      "Epoch 78/80\n",
      "2278/2278 [==============================] - trainLoss: 0.8114  Val_loss: 12.7255 \n",
      "Epoch 79/80\n",
      "2278/2278 [==============================] - trainLoss: 0.7355  Val_loss: 12.8110 \n",
      "WARNING:tensorflow:No training configuration found in save file: the model was *not* compiled. Compile it manually.\n",
      "WARNING:tensorflow:No training configuration found in save file: the model was *not* compiled. Compile it manually.\n",
      "WARNING:tensorflow:No training configuration found in save file: the model was *not* compiled. Compile it manually.\n",
      "WARNING:tensorflow:No training configuration found in save file: the model was *not* compiled. Compile it manually.\n",
      "Elapsed/s:  110.0125470161438\n",
      "Final training loss:  tf.Tensor(0.7354695, shape=(), dtype=float32)\n",
      "best val loss:  tf.Tensor(12.725483, shape=(), dtype=float32)\n",
      "{0: {'parameters': {'learning_rate': 0.0005, 'beta': 1, 'n_hidden': 2, 'n_neurons_classifier': 72, 'codings_size': 169, 'N': 10.0, 'n_hidden_classifier': 1, 'n_neurons': 137}, 'val_loss': <tf.Tensor: shape=(), dtype=float32, numpy=13.031519>}, 1: {'parameters': {'learning_rate': 0.001, 'beta': 10, 'n_hidden': 2, 'n_neurons_classifier': 79, 'codings_size': 122, 'N': 1.0, 'n_hidden_classifier': 2, 'n_neurons': 508}, 'val_loss': <tf.Tensor: shape=(), dtype=float32, numpy=13.413608>}, 2: {'parameters': {'learning_rate': 0.0005, 'beta': 10, 'n_hidden': 2, 'n_neurons_classifier': 94, 'codings_size': 234, 'N': 10.0, 'n_hidden_classifier': 1, 'n_neurons': 152}, 'val_loss': <tf.Tensor: shape=(), dtype=float32, numpy=12.7303915>}, 3: {'parameters': {'learning_rate': 0.001, 'beta': 10, 'n_hidden': 1, 'n_neurons_classifier': 22, 'codings_size': 207, 'N': 10.0, 'n_hidden_classifier': 1, 'n_neurons': 380}, 'val_loss': <tf.Tensor: shape=(), dtype=float32, numpy=17.365324>}, 4: {'parameters': {'learning_rate': 0.0005, 'beta': 15, 'n_hidden': 1, 'n_neurons_classifier': 95, 'codings_size': 112, 'N': 10.0, 'n_hidden_classifier': 2, 'n_neurons': 409}, 'val_loss': <tf.Tensor: shape=(), dtype=float32, numpy=17.92488>}, 5: {'parameters': {'learning_rate': 0.0005, 'beta': 15, 'n_hidden': 1, 'n_neurons_classifier': 71, 'codings_size': 40, 'N': 0.1, 'n_hidden_classifier': 1, 'n_neurons': 398}, 'val_loss': <tf.Tensor: shape=(), dtype=float32, numpy=14.990203>}, 6: {'parameters': {'learning_rate': 0.0005, 'beta': 15, 'n_hidden': 1, 'n_neurons_classifier': 80, 'codings_size': 34, 'N': 1.0, 'n_hidden_classifier': 1, 'n_neurons': 238}, 'val_loss': <tf.Tensor: shape=(), dtype=float32, numpy=nan>}, 7: {'parameters': {'learning_rate': 0.0005, 'beta': 10, 'n_hidden': 2, 'n_neurons_classifier': 83, 'codings_size': 94, 'N': 10.0, 'n_hidden_classifier': 1, 'n_neurons': 201}, 'val_loss': <tf.Tensor: shape=(), dtype=float32, numpy=12.725483>}}\n",
      "Epoch 0/80\n",
      "WARNING:tensorflow:Layer full_model_mmd is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because it's dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "2240/2278 [============================>.] - Loss for batch: 22.8102WARNING:tensorflow:Layer concatenate_1 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because it's dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "2278/2278 [==============================] - trainLoss: 22.8102  Val_loss: 19.5624 \n",
      "Epoch 1/80\n",
      "2278/2278 [==============================] - trainLoss: 10.4045  Val_loss: 18.3400 \n",
      "Epoch 2/80\n",
      "2278/2278 [==============================] - trainLoss: 8.0791  Val_loss: 17.9017 \n",
      "Epoch 3/80\n",
      "2278/2278 [==============================] - trainLoss: 7.0654  Val_loss: 17.5969 \n",
      "Epoch 4/80\n",
      "2278/2278 [==============================] - trainLoss: 6.4128  Val_loss: 17.4702 \n",
      "Epoch 5/80\n",
      "2278/2278 [==============================] - trainLoss: 6.0199  Val_loss: 17.3579 \n",
      "Epoch 6/80\n",
      "2278/2278 [==============================] - trainLoss: 5.6495  Val_loss: 17.3449 \n",
      "Epoch 7/80\n",
      "2278/2278 [==============================] - trainLoss: 5.4394  Val_loss: 17.2357 \n",
      "Epoch 8/80\n",
      "2278/2278 [==============================] - trainLoss: 5.1746  Val_loss: 17.1723 \n",
      "Epoch 9/80\n",
      "2278/2278 [==============================] - trainLoss: 4.8810  Val_loss: 17.0998 \n",
      "Epoch 10/80\n",
      "2278/2278 [==============================] - trainLoss: 4.6496  Val_loss: 16.7646 \n",
      "Epoch 11/80\n",
      "2278/2278 [==============================] - trainLoss: 4.5170  Val_loss: 16.9769 \n",
      "Epoch 12/80\n",
      "2278/2278 [==============================] - trainLoss: 4.3480  Val_loss: 16.9215 \n",
      "Epoch 13/80\n",
      "2278/2278 [==============================] - trainLoss: 4.1755  Val_loss: 16.8549 \n",
      "Epoch 14/80\n",
      "2278/2278 [==============================] - trainLoss: 4.0499  Val_loss: 16.7795 \n",
      "Epoch 15/80\n",
      "2278/2278 [==============================] - trainLoss: 3.8405  Val_loss: 16.6811 \n",
      "Epoch 16/80\n",
      "2278/2278 [==============================] - trainLoss: 3.7196  Val_loss: 16.4225 \n",
      "Epoch 17/80\n",
      "2278/2278 [==============================] - trainLoss: 3.7088  Val_loss: 16.2751 \n",
      "Epoch 18/80\n",
      "2278/2278 [==============================] - trainLoss: 3.5254  Val_loss: 16.1383 \n",
      "Epoch 19/80\n",
      "2278/2278 [==============================] - trainLoss: 3.4053  Val_loss: 16.0594 \n",
      "Epoch 20/80\n",
      "2278/2278 [==============================] - trainLoss: 3.4474  Val_loss: 15.7918 \n",
      "Epoch 21/80\n",
      "2278/2278 [==============================] - trainLoss: 3.3151  Val_loss: 15.8471 \n",
      "Epoch 22/80\n",
      "2278/2278 [==============================] - trainLoss: 3.1596  Val_loss: 15.5611 \n",
      "Epoch 23/80\n",
      "2278/2278 [==============================] - trainLoss: 3.1629  Val_loss: 15.6303 \n",
      "Epoch 24/80\n",
      "2278/2278 [==============================] - trainLoss: 3.1255  Val_loss: 15.4637 \n",
      "Epoch 25/80\n",
      "2278/2278 [==============================] - trainLoss: 2.9834  Val_loss: 15.3347 \n",
      "Epoch 26/80\n",
      "2278/2278 [==============================] - trainLoss: 2.9360  Val_loss: 15.3758 \n",
      "Epoch 27/80\n",
      "2278/2278 [==============================] - trainLoss: 2.9106  Val_loss: 15.1296 \n",
      "Epoch 28/80\n",
      "2278/2278 [==============================] - trainLoss: 2.9040  Val_loss: 15.0953 \n",
      "Epoch 29/80\n",
      "2278/2278 [==============================] - trainLoss: 2.8371  Val_loss: 14.9339 \n",
      "Epoch 30/80\n",
      "2278/2278 [==============================] - trainLoss: 2.7823  Val_loss: 14.7923 \n",
      "Epoch 31/80\n",
      "2278/2278 [==============================] - trainLoss: 2.6993  Val_loss: 14.8423 \n",
      "Epoch 32/80\n",
      "2278/2278 [==============================] - trainLoss: 2.6993  Val_loss: 14.7201 \n",
      "Epoch 33/80\n",
      "2278/2278 [==============================] - trainLoss: 2.6254  Val_loss: 14.5077 \n",
      "Epoch 34/80\n",
      "2278/2278 [==============================] - trainLoss: 2.6378  Val_loss: 14.4988 \n",
      "Epoch 35/80\n",
      "2278/2278 [==============================] - trainLoss: 2.6370  Val_loss: 14.4769 \n",
      "Epoch 36/80\n",
      "2278/2278 [==============================] - trainLoss: 2.5814  Val_loss: 14.4089 \n",
      "Epoch 37/80\n",
      "2278/2278 [==============================] - trainLoss: 2.5996  Val_loss: 14.2799 \n",
      "Epoch 38/80\n",
      "2278/2278 [==============================] - trainLoss: 2.6171  Val_loss: 14.3766 \n",
      "Epoch 39/80\n",
      "2278/2278 [==============================] - trainLoss: 2.5791  Val_loss: 14.3709 \n",
      "Epoch 40/80\n",
      "2278/2278 [==============================] - trainLoss: 2.5343  Val_loss: 14.0932 \n",
      "Epoch 41/80\n",
      "2278/2278 [==============================] - trainLoss: 2.5124  Val_loss: 14.0135 \n",
      "Epoch 42/80\n",
      "2278/2278 [==============================] - trainLoss: 2.4065  Val_loss: 14.0313 \n",
      "Epoch 43/80\n",
      "2278/2278 [==============================] - trainLoss: 2.4554  Val_loss: 13.9290 \n",
      "Epoch 44/80\n",
      "2278/2278 [==============================] - trainLoss: 2.4358  Val_loss: 13.9639 \n",
      "Epoch 45/80\n",
      "2278/2278 [==============================] - trainLoss: 2.4140  Val_loss: 13.9259 \n",
      "Epoch 46/80\n",
      "2278/2278 [==============================] - trainLoss: 2.3663  Val_loss: 13.9506 \n",
      "Epoch 47/80\n",
      "2278/2278 [==============================] - trainLoss: 2.4115  Val_loss: 13.8619 \n",
      "Epoch 48/80\n",
      "2278/2278 [==============================] - trainLoss: 2.4726  Val_loss: 13.8234 \n",
      "Epoch 49/80\n",
      "2278/2278 [==============================] - trainLoss: 2.2729  Val_loss: 13.8260 \n",
      "Epoch 50/80\n",
      "2278/2278 [==============================] - trainLoss: 2.3207  Val_loss: 13.8324 \n",
      "Epoch 51/80\n",
      "2278/2278 [==============================] - trainLoss: 2.3622  Val_loss: 13.7364 \n",
      "Epoch 52/80\n",
      "2278/2278 [==============================] - trainLoss: 2.2715  Val_loss: 13.6800 \n",
      "Epoch 53/80\n",
      "2278/2278 [==============================] - trainLoss: 2.3324  Val_loss: 13.6655 \n",
      "Epoch 54/80\n",
      "2278/2278 [==============================] - trainLoss: 2.3152  Val_loss: 13.4574 \n",
      "Epoch 55/80\n",
      "2278/2278 [==============================] - trainLoss: 2.3924  Val_loss: 13.6368 \n",
      "Epoch 56/80\n",
      "2278/2278 [==============================] - trainLoss: 2.4271  Val_loss: 13.5724 \n",
      "Epoch 57/80\n",
      "2278/2278 [==============================] - trainLoss: 2.2590  Val_loss: 13.3731 \n",
      "Epoch 58/80\n",
      "2278/2278 [==============================] - trainLoss: 2.3406  Val_loss: 13.3748 \n",
      "Epoch 59/80\n",
      "2278/2278 [==============================] - trainLoss: 2.2811  Val_loss: 13.4124 \n",
      "Epoch 60/80\n",
      "2278/2278 [==============================] - trainLoss: 2.2574  Val_loss: 13.3828 \n",
      "Epoch 61/80\n",
      "2278/2278 [==============================] - trainLoss: 2.2858  Val_loss: 13.3716 \n",
      "Epoch 62/80\n",
      "2278/2278 [==============================] - trainLoss: 2.2440  Val_loss: 13.3641 \n",
      "Epoch 63/80\n",
      "2278/2278 [==============================] - trainLoss: 2.2592  Val_loss: 13.2527 \n",
      "Epoch 64/80\n",
      "2278/2278 [==============================] - trainLoss: 2.2228  Val_loss: 13.2390 \n",
      "Epoch 65/80\n",
      "2278/2278 [==============================] - trainLoss: 2.2583  Val_loss: 13.3216 \n",
      "Epoch 66/80\n",
      "2278/2278 [==============================] - trainLoss: 2.2520  Val_loss: 13.2503 \n",
      "Epoch 67/80\n",
      "2278/2278 [==============================] - trainLoss: 2.2166  Val_loss: 13.1621 \n",
      "Epoch 68/80\n",
      "2278/2278 [==============================] - trainLoss: 2.1411  Val_loss: 13.1607 \n",
      "Epoch 69/80\n",
      "2278/2278 [==============================] - trainLoss: 2.2164  Val_loss: 13.1238 \n",
      "Epoch 70/80\n",
      "2278/2278 [==============================] - trainLoss: 2.1932  Val_loss: 13.0873 \n",
      "Epoch 71/80\n",
      "2278/2278 [==============================] - trainLoss: 2.2162  Val_loss: 13.2117 \n",
      "Epoch 72/80\n",
      "2278/2278 [==============================] - trainLoss: 2.2004  Val_loss: 13.0874 \n",
      "Epoch 73/80\n",
      "2278/2278 [==============================] - trainLoss: 2.1518  Val_loss: 13.0945 \n",
      "Epoch 74/80\n",
      "2278/2278 [==============================] - trainLoss: 2.1358  Val_loss: 12.9904 \n",
      "Epoch 75/80\n",
      "2278/2278 [==============================] - trainLoss: 2.2116  Val_loss: 13.0319 \n",
      "Epoch 76/80\n",
      "2278/2278 [==============================] - trainLoss: 2.1817  Val_loss: 13.0060 \n",
      "Epoch 77/80\n",
      "2278/2278 [==============================] - trainLoss: 2.1612  Val_loss: 13.0170 \n",
      "Epoch 78/80\n",
      "2278/2278 [==============================] - trainLoss: 2.0894  Val_loss: 12.9981 \n",
      "Epoch 79/80\n",
      "2278/2278 [==============================] - trainLoss: 2.2251  Val_loss: 12.9185 \n",
      "WARNING:tensorflow:No training configuration found in save file: the model was *not* compiled. Compile it manually.\n",
      "WARNING:tensorflow:No training configuration found in save file: the model was *not* compiled. Compile it manually.\n",
      "WARNING:tensorflow:No training configuration found in save file: the model was *not* compiled. Compile it manually.\n",
      "WARNING:tensorflow:No training configuration found in save file: the model was *not* compiled. Compile it manually.\n",
      "Elapsed/s:  111.22882175445557\n",
      "Final training loss:  tf.Tensor(2.2251163, shape=(), dtype=float32)\n",
      "best val loss:  tf.Tensor(12.918543, shape=(), dtype=float32)\n",
      "{0: {'parameters': {'learning_rate': 0.0005, 'beta': 1, 'n_hidden': 2, 'n_neurons_classifier': 72, 'codings_size': 169, 'N': 10.0, 'n_hidden_classifier': 1, 'n_neurons': 137}, 'val_loss': <tf.Tensor: shape=(), dtype=float32, numpy=13.031519>}, 1: {'parameters': {'learning_rate': 0.001, 'beta': 10, 'n_hidden': 2, 'n_neurons_classifier': 79, 'codings_size': 122, 'N': 1.0, 'n_hidden_classifier': 2, 'n_neurons': 508}, 'val_loss': <tf.Tensor: shape=(), dtype=float32, numpy=13.413608>}, 2: {'parameters': {'learning_rate': 0.0005, 'beta': 10, 'n_hidden': 2, 'n_neurons_classifier': 94, 'codings_size': 234, 'N': 10.0, 'n_hidden_classifier': 1, 'n_neurons': 152}, 'val_loss': <tf.Tensor: shape=(), dtype=float32, numpy=12.7303915>}, 3: {'parameters': {'learning_rate': 0.001, 'beta': 10, 'n_hidden': 1, 'n_neurons_classifier': 22, 'codings_size': 207, 'N': 10.0, 'n_hidden_classifier': 1, 'n_neurons': 380}, 'val_loss': <tf.Tensor: shape=(), dtype=float32, numpy=17.365324>}, 4: {'parameters': {'learning_rate': 0.0005, 'beta': 15, 'n_hidden': 1, 'n_neurons_classifier': 95, 'codings_size': 112, 'N': 10.0, 'n_hidden_classifier': 2, 'n_neurons': 409}, 'val_loss': <tf.Tensor: shape=(), dtype=float32, numpy=17.92488>}, 5: {'parameters': {'learning_rate': 0.0005, 'beta': 15, 'n_hidden': 1, 'n_neurons_classifier': 71, 'codings_size': 40, 'N': 0.1, 'n_hidden_classifier': 1, 'n_neurons': 398}, 'val_loss': <tf.Tensor: shape=(), dtype=float32, numpy=14.990203>}, 6: {'parameters': {'learning_rate': 0.0005, 'beta': 15, 'n_hidden': 1, 'n_neurons_classifier': 80, 'codings_size': 34, 'N': 1.0, 'n_hidden_classifier': 1, 'n_neurons': 238}, 'val_loss': <tf.Tensor: shape=(), dtype=float32, numpy=nan>}, 7: {'parameters': {'learning_rate': 0.0005, 'beta': 10, 'n_hidden': 2, 'n_neurons_classifier': 83, 'codings_size': 94, 'N': 10.0, 'n_hidden_classifier': 1, 'n_neurons': 201}, 'val_loss': <tf.Tensor: shape=(), dtype=float32, numpy=12.725483>}, 8: {'parameters': {'learning_rate': 0.0005, 'beta': 1, 'n_hidden': 2, 'n_neurons_classifier': 21, 'codings_size': 107, 'N': 0.1, 'n_hidden_classifier': 1, 'n_neurons': 180}, 'val_loss': <tf.Tensor: shape=(), dtype=float32, numpy=12.918543>}}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0/80\n",
      "WARNING:tensorflow:Layer full_model_mmd is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because it's dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "2240/2278 [============================>.] - Loss for batch: 17.9735WARNING:tensorflow:Layer concatenate_1 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because it's dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "2278/2278 [==============================] - trainLoss: 17.9735  Val_loss: 19.9834 \n",
      "Epoch 1/80\n",
      "2278/2278 [==============================] - trainLoss: 8.8352  Val_loss: 18.3790 \n",
      "Epoch 2/80\n",
      "2278/2278 [==============================] - trainLoss: 7.3859  Val_loss: 17.2591 \n",
      "Epoch 3/80\n",
      "2278/2278 [==============================] - trainLoss: 6.1377  Val_loss: 16.6637 \n",
      "Epoch 4/80\n",
      "2278/2278 [==============================] - trainLoss: 5.2838  Val_loss: 16.2661 \n",
      "Epoch 5/80\n",
      "2278/2278 [==============================] - trainLoss: 4.3608  Val_loss: 15.8391 \n",
      "Epoch 6/80\n",
      "2278/2278 [==============================] - trainLoss: 3.8464  Val_loss: 15.4123 \n",
      "Epoch 7/80\n",
      "2278/2278 [==============================] - trainLoss: 3.3948  Val_loss: 15.1121 \n",
      "Epoch 8/80\n",
      "2278/2278 [==============================] - trainLoss: 2.9290  Val_loss: 14.9852 \n",
      "Epoch 9/80\n",
      "2278/2278 [==============================] - trainLoss: 2.7431  Val_loss: 14.7556 \n",
      "Epoch 10/80\n",
      "2278/2278 [==============================] - trainLoss: 2.2694  Val_loss: 14.6915 \n",
      "Epoch 11/80\n",
      "2278/2278 [==============================] - trainLoss: 2.1513  Val_loss: 14.6994 \n",
      "Epoch 12/80\n",
      "2278/2278 [==============================] - trainLoss: 1.8568  Val_loss: 14.4858 \n",
      "Epoch 13/80\n",
      "2278/2278 [==============================] - trainLoss: 1.8339  Val_loss: 14.4256 \n",
      "Epoch 14/80\n",
      "2278/2278 [==============================] - trainLoss: 1.6176  Val_loss: 14.5620 \n",
      "Epoch 15/80\n",
      "2278/2278 [==============================] - trainLoss: 1.5284  Val_loss: 14.3418 \n",
      "Epoch 16/80\n",
      "2278/2278 [==============================] - trainLoss: 1.5047  Val_loss: 14.1789 \n",
      "Epoch 17/80\n",
      "2278/2278 [==============================] - trainLoss: 1.2861  Val_loss: 14.2265 \n",
      "Epoch 18/80\n",
      "2278/2278 [==============================] - trainLoss: 1.2884  Val_loss: 14.1732 \n",
      "Epoch 19/80\n",
      "2278/2278 [==============================] - trainLoss: 1.2461  Val_loss: 14.0599 \n",
      "Epoch 20/80\n",
      "2278/2278 [==============================] - trainLoss: 1.1675  Val_loss: 14.0269 \n",
      "Epoch 21/80\n",
      "2278/2278 [==============================] - trainLoss: 1.0532  Val_loss: 14.0141 \n",
      "Epoch 22/80\n",
      "2278/2278 [==============================] - trainLoss: 1.1208  Val_loss: 13.9772 \n",
      "Epoch 23/80\n",
      "2278/2278 [==============================] - trainLoss: 1.2040  Val_loss: 13.5868 \n",
      "Epoch 24/80\n",
      "2278/2278 [==============================] - trainLoss: 1.0620  Val_loss: 13.8041 \n",
      "Epoch 25/80\n",
      "2278/2278 [==============================] - trainLoss: 1.0316  Val_loss: 13.7898 \n",
      "Epoch 26/80\n",
      "2278/2278 [==============================] - trainLoss: 0.8656  Val_loss: 13.7635 \n",
      "Epoch 27/80\n",
      "2278/2278 [==============================] - trainLoss: 1.0720  Val_loss: 13.6786 \n",
      "Epoch 28/80\n",
      "2278/2278 [==============================] - trainLoss: 1.0257  Val_loss: 13.8571 \n",
      "Epoch 29/80\n",
      "2278/2278 [==============================] - trainLoss: 0.9171  Val_loss: 13.7707 \n",
      "Epoch 30/80\n",
      "2278/2278 [==============================] - trainLoss: 0.8468  Val_loss: 13.8493 \n",
      "Epoch 31/80\n",
      "2278/2278 [==============================] - trainLoss: 0.9996  Val_loss: 13.7830 \n",
      "Epoch 32/80\n",
      "2278/2278 [==============================] - trainLoss: 0.8257  Val_loss: 13.7523 \n",
      "WARNING:tensorflow:No training configuration found in save file: the model was *not* compiled. Compile it manually.\n",
      "WARNING:tensorflow:No training configuration found in save file: the model was *not* compiled. Compile it manually.\n",
      "WARNING:tensorflow:No training configuration found in save file: the model was *not* compiled. Compile it manually.\n",
      "WARNING:tensorflow:No training configuration found in save file: the model was *not* compiled. Compile it manually.\n",
      "Elapsed/s:  50.16587567329407\n",
      "Final training loss:  tf.Tensor(0.8256577, shape=(), dtype=float32)\n",
      "best val loss:  tf.Tensor(13.586783, shape=(), dtype=float32)\n",
      "{0: {'parameters': {'learning_rate': 0.0005, 'beta': 1, 'n_hidden': 2, 'n_neurons_classifier': 72, 'codings_size': 169, 'N': 10.0, 'n_hidden_classifier': 1, 'n_neurons': 137}, 'val_loss': <tf.Tensor: shape=(), dtype=float32, numpy=13.031519>}, 1: {'parameters': {'learning_rate': 0.001, 'beta': 10, 'n_hidden': 2, 'n_neurons_classifier': 79, 'codings_size': 122, 'N': 1.0, 'n_hidden_classifier': 2, 'n_neurons': 508}, 'val_loss': <tf.Tensor: shape=(), dtype=float32, numpy=13.413608>}, 2: {'parameters': {'learning_rate': 0.0005, 'beta': 10, 'n_hidden': 2, 'n_neurons_classifier': 94, 'codings_size': 234, 'N': 10.0, 'n_hidden_classifier': 1, 'n_neurons': 152}, 'val_loss': <tf.Tensor: shape=(), dtype=float32, numpy=12.7303915>}, 3: {'parameters': {'learning_rate': 0.001, 'beta': 10, 'n_hidden': 1, 'n_neurons_classifier': 22, 'codings_size': 207, 'N': 10.0, 'n_hidden_classifier': 1, 'n_neurons': 380}, 'val_loss': <tf.Tensor: shape=(), dtype=float32, numpy=17.365324>}, 4: {'parameters': {'learning_rate': 0.0005, 'beta': 15, 'n_hidden': 1, 'n_neurons_classifier': 95, 'codings_size': 112, 'N': 10.0, 'n_hidden_classifier': 2, 'n_neurons': 409}, 'val_loss': <tf.Tensor: shape=(), dtype=float32, numpy=17.92488>}, 5: {'parameters': {'learning_rate': 0.0005, 'beta': 15, 'n_hidden': 1, 'n_neurons_classifier': 71, 'codings_size': 40, 'N': 0.1, 'n_hidden_classifier': 1, 'n_neurons': 398}, 'val_loss': <tf.Tensor: shape=(), dtype=float32, numpy=14.990203>}, 6: {'parameters': {'learning_rate': 0.0005, 'beta': 15, 'n_hidden': 1, 'n_neurons_classifier': 80, 'codings_size': 34, 'N': 1.0, 'n_hidden_classifier': 1, 'n_neurons': 238}, 'val_loss': <tf.Tensor: shape=(), dtype=float32, numpy=nan>}, 7: {'parameters': {'learning_rate': 0.0005, 'beta': 10, 'n_hidden': 2, 'n_neurons_classifier': 83, 'codings_size': 94, 'N': 10.0, 'n_hidden_classifier': 1, 'n_neurons': 201}, 'val_loss': <tf.Tensor: shape=(), dtype=float32, numpy=12.725483>}, 8: {'parameters': {'learning_rate': 0.0005, 'beta': 1, 'n_hidden': 2, 'n_neurons_classifier': 21, 'codings_size': 107, 'N': 0.1, 'n_hidden_classifier': 1, 'n_neurons': 180}, 'val_loss': <tf.Tensor: shape=(), dtype=float32, numpy=12.918543>}, 9: {'parameters': {'learning_rate': 0.001, 'beta': 15, 'n_hidden': 2, 'n_neurons_classifier': 94, 'codings_size': 136, 'N': 10.0, 'n_hidden_classifier': 2, 'n_neurons': 508}, 'val_loss': <tf.Tensor: shape=(), dtype=float32, numpy=13.586783>}}\n",
      "WARNING:tensorflow:No training configuration found in save file: the model was *not* compiled. Compile it manually.\n",
      "WARNING:tensorflow:No training configuration found in save file: the model was *not* compiled. Compile it manually.\n",
      "WARNING:tensorflow:No training configuration found in save file: the model was *not* compiled. Compile it manually.\n",
      "WARNING:tensorflow:No training configuration found in save file: the model was *not* compiled. Compile it manually.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[(7,\n",
       "  {'parameters': {'N': 10.0,\n",
       "    'beta': 10,\n",
       "    'codings_size': 94,\n",
       "    'learning_rate': 0.0005,\n",
       "    'n_hidden': 2,\n",
       "    'n_hidden_classifier': 1,\n",
       "    'n_neurons': 201,\n",
       "    'n_neurons_classifier': 83},\n",
       "   'val_loss': <tf.Tensor: shape=(), dtype=float32, numpy=12.725483>}),\n",
       " (2,\n",
       "  {'parameters': {'N': 10.0,\n",
       "    'beta': 10,\n",
       "    'codings_size': 234,\n",
       "    'learning_rate': 0.0005,\n",
       "    'n_hidden': 2,\n",
       "    'n_hidden_classifier': 1,\n",
       "    'n_neurons': 152,\n",
       "    'n_neurons_classifier': 94},\n",
       "   'val_loss': <tf.Tensor: shape=(), dtype=float32, numpy=12.7303915>}),\n",
       " (8,\n",
       "  {'parameters': {'N': 0.1,\n",
       "    'beta': 1,\n",
       "    'codings_size': 107,\n",
       "    'learning_rate': 0.0005,\n",
       "    'n_hidden': 2,\n",
       "    'n_hidden_classifier': 1,\n",
       "    'n_neurons': 180,\n",
       "    'n_neurons_classifier': 21},\n",
       "   'val_loss': <tf.Tensor: shape=(), dtype=float32, numpy=12.918543>}),\n",
       " (0,\n",
       "  {'parameters': {'N': 10.0,\n",
       "    'beta': 1,\n",
       "    'codings_size': 169,\n",
       "    'learning_rate': 0.0005,\n",
       "    'n_hidden': 2,\n",
       "    'n_hidden_classifier': 1,\n",
       "    'n_neurons': 137,\n",
       "    'n_neurons_classifier': 72},\n",
       "   'val_loss': <tf.Tensor: shape=(), dtype=float32, numpy=13.031519>}),\n",
       " (1,\n",
       "  {'parameters': {'N': 1.0,\n",
       "    'beta': 10,\n",
       "    'codings_size': 122,\n",
       "    'learning_rate': 0.001,\n",
       "    'n_hidden': 2,\n",
       "    'n_hidden_classifier': 2,\n",
       "    'n_neurons': 508,\n",
       "    'n_neurons_classifier': 79},\n",
       "   'val_loss': <tf.Tensor: shape=(), dtype=float32, numpy=13.413608>}),\n",
       " (9,\n",
       "  {'parameters': {'N': 10.0,\n",
       "    'beta': 15,\n",
       "    'codings_size': 136,\n",
       "    'learning_rate': 0.001,\n",
       "    'n_hidden': 2,\n",
       "    'n_hidden_classifier': 2,\n",
       "    'n_neurons': 508,\n",
       "    'n_neurons_classifier': 94},\n",
       "   'val_loss': <tf.Tensor: shape=(), dtype=float32, numpy=13.586783>}),\n",
       " (5,\n",
       "  {'parameters': {'N': 0.1,\n",
       "    'beta': 15,\n",
       "    'codings_size': 40,\n",
       "    'learning_rate': 0.0005,\n",
       "    'n_hidden': 1,\n",
       "    'n_hidden_classifier': 1,\n",
       "    'n_neurons': 398,\n",
       "    'n_neurons_classifier': 71},\n",
       "   'val_loss': <tf.Tensor: shape=(), dtype=float32, numpy=14.990203>}),\n",
       " (3,\n",
       "  {'parameters': {'N': 10.0,\n",
       "    'beta': 10,\n",
       "    'codings_size': 207,\n",
       "    'learning_rate': 0.001,\n",
       "    'n_hidden': 1,\n",
       "    'n_hidden_classifier': 1,\n",
       "    'n_neurons': 380,\n",
       "    'n_neurons_classifier': 22},\n",
       "   'val_loss': <tf.Tensor: shape=(), dtype=float32, numpy=17.365324>}),\n",
       " (4,\n",
       "  {'parameters': {'N': 10.0,\n",
       "    'beta': 15,\n",
       "    'codings_size': 112,\n",
       "    'learning_rate': 0.0005,\n",
       "    'n_hidden': 1,\n",
       "    'n_hidden_classifier': 2,\n",
       "    'n_neurons': 409,\n",
       "    'n_neurons_classifier': 95},\n",
       "   'val_loss': <tf.Tensor: shape=(), dtype=float32, numpy=17.92488>}),\n",
       " (6,\n",
       "  {'parameters': {'N': 1.0,\n",
       "    'beta': 15,\n",
       "    'codings_size': 34,\n",
       "    'learning_rate': 0.0005,\n",
       "    'n_hidden': 1,\n",
       "    'n_hidden_classifier': 1,\n",
       "    'n_neurons': 238,\n",
       "    'n_neurons_classifier': 80},\n",
       "   'val_loss': <tf.Tensor: shape=(), dtype=float32, numpy=nan>})]"
      ]
     },
     "execution_count": 247,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result,variational_encoder,variational_decoder,classifier,y_distribution = hyperparameter_search_mmd(param_distribs=param_distribs,\n",
    "                                                                        epochs=80,patience=10,n_iter=10)\n",
    "result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Best MMD-based model has val nloglik of 12.385776. \n",
    "\n",
    "{'parameters': {'learning_rate': 0.001, 'beta': 15, 'n_hidden': 2, 'n_neurons_classifier': 40, 'codings_size': 171, 'N': 10.0, 'n_hidden_classifier': 1, 'n_neurons': 171}, 'val_loss': <tf.Tensor: shape=(), dtype=float32"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Single run # "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "variational_encoder,variational_decoder,classifier,y_distribution,model = build_model_mmd(n_hidden=2, n_neurons=171,input_shape=input_shape,beta=15,n_hidden_classifier=1,\n",
    "              n_neurons_classifier=40,N=10,codings_size=171)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0/100\n",
      "WARNING:tensorflow:Layer full_model_mmd is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because it's dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "2240/2278 [============================>.] - Loss for batch: 15.3301WARNING:tensorflow:Layer concatenate_1 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because it's dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "2278/2278 [==============================] - trainLoss: 15.3301  Val_loss: 17.8070 \n",
      "Epoch 1/100\n",
      "2278/2278 [==============================] - trainLoss: 7.0534  Val_loss: 16.9998 \n",
      "Epoch 2/100\n",
      "2278/2278 [==============================] - trainLoss: 5.9513  Val_loss: 16.2125 \n",
      "Epoch 3/100\n",
      "2278/2278 [==============================] - trainLoss: 5.2185  Val_loss: 15.7867 \n",
      "Epoch 4/100\n",
      "2278/2278 [==============================] - trainLoss: 4.7441  Val_loss: 15.4283 \n",
      "Epoch 5/100\n",
      "2278/2278 [==============================] - trainLoss: 4.0797  Val_loss: 15.1698 \n",
      "Epoch 6/100\n",
      "2278/2278 [==============================] - trainLoss: 3.6541  Val_loss: 14.7569 \n",
      "Epoch 7/100\n",
      "2278/2278 [==============================] - trainLoss: 3.2607  Val_loss: 14.4983 \n",
      "Epoch 8/100\n",
      "2278/2278 [==============================] - trainLoss: 3.0316  Val_loss: 14.3393 \n",
      "Epoch 9/100\n",
      "2278/2278 [==============================] - trainLoss: 2.8793  Val_loss: 14.0506 \n",
      "Epoch 10/100\n",
      "2278/2278 [==============================] - trainLoss: 2.6827  Val_loss: 13.9452 \n",
      "Epoch 11/100\n",
      "2278/2278 [==============================] - trainLoss: 2.4193  Val_loss: 13.6649 \n",
      "Epoch 12/100\n",
      "2278/2278 [==============================] - trainLoss: 2.3825  Val_loss: 13.4964 \n",
      "Epoch 13/100\n",
      "2278/2278 [==============================] - trainLoss: 2.1200  Val_loss: 13.3760 \n",
      "Epoch 14/100\n",
      "2278/2278 [==============================] - trainLoss: 1.9906  Val_loss: 13.3811 \n",
      "Epoch 15/100\n",
      "2278/2278 [==============================] - trainLoss: 1.8243  Val_loss: 13.2323 \n",
      "Epoch 16/100\n",
      "2278/2278 [==============================] - trainLoss: 1.8008  Val_loss: 13.1215 \n",
      "Epoch 17/100\n",
      "2278/2278 [==============================] - trainLoss: 1.7278  Val_loss: 13.0906 \n",
      "Epoch 18/100\n",
      "2278/2278 [==============================] - trainLoss: 1.6723  Val_loss: 12.9883 \n",
      "Epoch 19/100\n",
      "2278/2278 [==============================] - trainLoss: 1.6878  Val_loss: 12.9715 \n",
      "Epoch 20/100\n",
      "2278/2278 [==============================] - trainLoss: 1.5701  Val_loss: 12.8051 \n",
      "Epoch 21/100\n",
      "2278/2278 [==============================] - trainLoss: 1.4347  Val_loss: 12.7613 \n",
      "Epoch 22/100\n",
      "2278/2278 [==============================] - trainLoss: 1.6876  Val_loss: 12.7733 \n",
      "Epoch 23/100\n",
      "2278/2278 [==============================] - trainLoss: 1.4329  Val_loss: 12.6820 \n",
      "Epoch 24/100\n",
      "2278/2278 [==============================] - trainLoss: 1.4306  Val_loss: 12.8235 \n",
      "Epoch 25/100\n",
      "2278/2278 [==============================] - trainLoss: 1.3904  Val_loss: 12.8040 \n",
      "Epoch 26/100\n",
      "2278/2278 [==============================] - trainLoss: 1.5655  Val_loss: 12.6927 \n",
      "Epoch 27/100\n",
      "2278/2278 [==============================] - trainLoss: 1.3240  Val_loss: 12.6585 \n",
      "Epoch 28/100\n",
      "2278/2278 [==============================] - trainLoss: 1.3135  Val_loss: 12.5395 \n",
      "Epoch 29/100\n",
      "2278/2278 [==============================] - trainLoss: 1.3335  Val_loss: 12.5479 \n",
      "Epoch 30/100\n",
      "2278/2278 [==============================] - trainLoss: 1.3589  Val_loss: 12.6147 \n",
      "Epoch 31/100\n",
      "2278/2278 [==============================] - trainLoss: 1.3370  Val_loss: 12.5573 \n",
      "Epoch 32/100\n",
      "2278/2278 [==============================] - trainLoss: 1.3024  Val_loss: 12.4989 \n",
      "Epoch 33/100\n",
      "2278/2278 [==============================] - trainLoss: 1.2691  Val_loss: 12.5388 \n",
      "Epoch 34/100\n",
      "2278/2278 [==============================] - trainLoss: 1.2075  Val_loss: 12.4978 \n",
      "Epoch 35/100\n",
      "2278/2278 [==============================] - trainLoss: 1.2656  Val_loss: 12.5503 \n",
      "Epoch 36/100\n",
      "2278/2278 [==============================] - trainLoss: 1.1963  Val_loss: 12.3937 \n",
      "Epoch 37/100\n",
      "2278/2278 [==============================] - trainLoss: 1.3680  Val_loss: 12.3529 \n",
      "Epoch 38/100\n",
      "2278/2278 [==============================] - trainLoss: 1.4189  Val_loss: 12.2820 \n",
      "Epoch 39/100\n",
      "2278/2278 [==============================] - trainLoss: 1.1579  Val_loss: 12.3647 \n",
      "Epoch 40/100\n",
      "2278/2278 [==============================] - trainLoss: 1.2408  Val_loss: 12.4388 \n",
      "Epoch 41/100\n",
      "2278/2278 [==============================] - trainLoss: 1.2447  Val_loss: 12.3228 \n",
      "Epoch 42/100\n",
      "2278/2278 [==============================] - trainLoss: 1.3551  Val_loss: 12.2006 \n",
      "Epoch 43/100\n",
      "2278/2278 [==============================] - trainLoss: 1.2080  Val_loss: 12.3480 \n",
      "Epoch 44/100\n",
      "2278/2278 [==============================] - trainLoss: 1.1227  Val_loss: 12.2925 \n",
      "Epoch 45/100\n",
      "2278/2278 [==============================] - trainLoss: 1.2032  Val_loss: 12.2153 \n",
      "Epoch 46/100\n",
      "2278/2278 [==============================] - trainLoss: 1.1274  Val_loss: 12.1787 \n",
      "Epoch 47/100\n",
      "2278/2278 [==============================] - trainLoss: 1.1020  Val_loss: 12.2406 \n",
      "Epoch 48/100\n",
      "2278/2278 [==============================] - trainLoss: 0.9545  Val_loss: 12.2095 \n",
      "Epoch 49/100\n",
      "2278/2278 [==============================] - trainLoss: 1.1321  Val_loss: 12.3394 \n",
      "Epoch 50/100\n",
      "2278/2278 [==============================] - trainLoss: 1.1640  Val_loss: 12.2786 \n",
      "Epoch 51/100\n",
      "2278/2278 [==============================] - trainLoss: 1.0836  Val_loss: 12.4217 \n",
      "Epoch 52/100\n",
      "2278/2278 [==============================] - trainLoss: 1.0436  Val_loss: 12.3972 \n",
      "Epoch 53/100\n",
      "2278/2278 [==============================] - trainLoss: 0.9996  Val_loss: 12.1867 \n",
      "Epoch 54/100\n",
      "2278/2278 [==============================] - trainLoss: 1.0079  Val_loss: 12.2347 \n",
      "Epoch 55/100\n",
      "2278/2278 [==============================] - trainLoss: 1.0179  Val_loss: 12.2946 \n",
      "WARNING:tensorflow:No training configuration found in save file: the model was *not* compiled. Compile it manually.\n",
      "WARNING:tensorflow:No training configuration found in save file: the model was *not* compiled. Compile it manually.\n",
      "WARNING:tensorflow:No training configuration found in save file: the model was *not* compiled. Compile it manually.\n",
      "WARNING:tensorflow:No training configuration found in save file: the model was *not* compiled. Compile it manually.\n",
      "Elapsed/s:  80.5705623626709\n",
      "Final training loss:  tf.Tensor(1.0178821, shape=(), dtype=float32)\n",
      "best val loss:  tf.Tensor(12.178725, shape=(), dtype=float32)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "([<tf.Tensor: shape=(), dtype=float32, numpy=15.330065>,\n",
       "  <tf.Tensor: shape=(), dtype=float32, numpy=7.053376>,\n",
       "  <tf.Tensor: shape=(), dtype=float32, numpy=5.9513254>,\n",
       "  <tf.Tensor: shape=(), dtype=float32, numpy=5.218501>,\n",
       "  <tf.Tensor: shape=(), dtype=float32, numpy=4.7441235>,\n",
       "  <tf.Tensor: shape=(), dtype=float32, numpy=4.079688>,\n",
       "  <tf.Tensor: shape=(), dtype=float32, numpy=3.6541464>,\n",
       "  <tf.Tensor: shape=(), dtype=float32, numpy=3.2606776>,\n",
       "  <tf.Tensor: shape=(), dtype=float32, numpy=3.0316384>,\n",
       "  <tf.Tensor: shape=(), dtype=float32, numpy=2.8792806>,\n",
       "  <tf.Tensor: shape=(), dtype=float32, numpy=2.682711>,\n",
       "  <tf.Tensor: shape=(), dtype=float32, numpy=2.4193249>,\n",
       "  <tf.Tensor: shape=(), dtype=float32, numpy=2.3825414>,\n",
       "  <tf.Tensor: shape=(), dtype=float32, numpy=2.1199887>,\n",
       "  <tf.Tensor: shape=(), dtype=float32, numpy=1.9906338>,\n",
       "  <tf.Tensor: shape=(), dtype=float32, numpy=1.8242937>,\n",
       "  <tf.Tensor: shape=(), dtype=float32, numpy=1.8007832>,\n",
       "  <tf.Tensor: shape=(), dtype=float32, numpy=1.7278115>,\n",
       "  <tf.Tensor: shape=(), dtype=float32, numpy=1.6722846>,\n",
       "  <tf.Tensor: shape=(), dtype=float32, numpy=1.6878257>,\n",
       "  <tf.Tensor: shape=(), dtype=float32, numpy=1.570092>,\n",
       "  <tf.Tensor: shape=(), dtype=float32, numpy=1.4347>,\n",
       "  <tf.Tensor: shape=(), dtype=float32, numpy=1.6875561>,\n",
       "  <tf.Tensor: shape=(), dtype=float32, numpy=1.4329491>,\n",
       "  <tf.Tensor: shape=(), dtype=float32, numpy=1.4306209>,\n",
       "  <tf.Tensor: shape=(), dtype=float32, numpy=1.3904212>,\n",
       "  <tf.Tensor: shape=(), dtype=float32, numpy=1.5655277>,\n",
       "  <tf.Tensor: shape=(), dtype=float32, numpy=1.3239939>,\n",
       "  <tf.Tensor: shape=(), dtype=float32, numpy=1.3134706>,\n",
       "  <tf.Tensor: shape=(), dtype=float32, numpy=1.3335267>,\n",
       "  <tf.Tensor: shape=(), dtype=float32, numpy=1.3588796>,\n",
       "  <tf.Tensor: shape=(), dtype=float32, numpy=1.3370252>,\n",
       "  <tf.Tensor: shape=(), dtype=float32, numpy=1.3024464>,\n",
       "  <tf.Tensor: shape=(), dtype=float32, numpy=1.2691352>,\n",
       "  <tf.Tensor: shape=(), dtype=float32, numpy=1.2074852>,\n",
       "  <tf.Tensor: shape=(), dtype=float32, numpy=1.2656147>,\n",
       "  <tf.Tensor: shape=(), dtype=float32, numpy=1.1963117>,\n",
       "  <tf.Tensor: shape=(), dtype=float32, numpy=1.3679705>,\n",
       "  <tf.Tensor: shape=(), dtype=float32, numpy=1.4188826>,\n",
       "  <tf.Tensor: shape=(), dtype=float32, numpy=1.1579355>,\n",
       "  <tf.Tensor: shape=(), dtype=float32, numpy=1.2408006>,\n",
       "  <tf.Tensor: shape=(), dtype=float32, numpy=1.2447356>,\n",
       "  <tf.Tensor: shape=(), dtype=float32, numpy=1.3550564>,\n",
       "  <tf.Tensor: shape=(), dtype=float32, numpy=1.2079866>,\n",
       "  <tf.Tensor: shape=(), dtype=float32, numpy=1.1226853>,\n",
       "  <tf.Tensor: shape=(), dtype=float32, numpy=1.2032157>,\n",
       "  <tf.Tensor: shape=(), dtype=float32, numpy=1.1274006>,\n",
       "  <tf.Tensor: shape=(), dtype=float32, numpy=1.102022>,\n",
       "  <tf.Tensor: shape=(), dtype=float32, numpy=0.95447433>,\n",
       "  <tf.Tensor: shape=(), dtype=float32, numpy=1.1321093>,\n",
       "  <tf.Tensor: shape=(), dtype=float32, numpy=1.1640259>,\n",
       "  <tf.Tensor: shape=(), dtype=float32, numpy=1.0835991>,\n",
       "  <tf.Tensor: shape=(), dtype=float32, numpy=1.0436491>,\n",
       "  <tf.Tensor: shape=(), dtype=float32, numpy=0.9996354>,\n",
       "  <tf.Tensor: shape=(), dtype=float32, numpy=1.0078564>,\n",
       "  <tf.Tensor: shape=(), dtype=float32, numpy=1.0178821>],\n",
       " <tf.Tensor: shape=(), dtype=float32, numpy=12.178725>)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fit_model(X_train_omics_labelled, train_set_labelled_y, X_train_omics_unlabelled,train_set_labelled_c,\n",
    "          train_set_unlabelled_c,100,X_valid_omics, valid_set_labelled_y,valid_set_labelled_c,\n",
    "              10,variational_encoder=variational_encoder,variational_decoder=variational_decoder,\n",
    "             classifier=classifier,y_distribution=y_distribution,model=model,\n",
    "          Sampling=Sampling,y_dist=y_dist,batch_size=32,learning_rate=0.001,valid_set=True,codings_size=171)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test_nlog_lik = tf.Tensor(12.561308, shape=(), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "test_nlog_lik = -validation_log_lik_sampling(test_set_labelled_y,X_test_omics.to_numpy(),test_set_labelled_c.to_numpy(),\n",
    "                                    variational_decoder=variational_decoder,codings_size=171,samples=2000)\n",
    "print(\"test_nlog_lik = \" + str(test_nlog_lik))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is the best score and therefore the model that will be used."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
